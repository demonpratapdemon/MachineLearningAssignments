{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3ab9a4",
   "metadata": {},
   "source": [
    "## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f59e432",
   "metadata": {},
   "source": [
    "**R-squared**, often denoted as **R2**, is a statistical measure used to evaluate the goodness of fit of a linear regression model. It quantifies how well the model explains the variance in the dependent variable based on the independent variables. R-squared is a key metric in regression analysis and provides insights into the model's performance.\n",
    "\n",
    "##### Calculation of R-squared:\n",
    "\n",
    "The formula to calculate R-squared is as follows:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SS_{R}}{SS_{T}} $$\n",
    "\n",
    "Where:\n",
    "- $R^2$ is the R-squared value, which ranges from 0 to 1.\n",
    "- $SS_{R}$ is the sum of the squared residuals (also known as the sum of squared errors), which measures the unexplained variance by the model.\n",
    "- $SS_{T}$ is the total sum of squares, which measures the total variance in the dependent variable.\n",
    "\n",
    "##### Interpretation of R-squared:\n",
    "\n",
    "- $R^2$ value ranges between 0 to 1, where:\n",
    "    - $R^2=0$: The model explains none of the variance in the dependent variable. It's essentially a poor fit to the data.\n",
    "    - $R^2=1$: The model explains all of the variance in the dependent variable. It's a perfect fit to the data which also means it is an overfit to the train data.\n",
    "    - $0<R^2<1$: The model explains a portion of the variance in the dependent variable, with higher values indicating a better fit.\n",
    "\n",
    "- R-squared represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model. For example, if $R^2=0.70$, it means that 70% of the variance in the dependent variable is explained by the model, and the remaining 30% is unexplained.\n",
    "\n",
    "- R-squared provides a relative measure of model fit. A higher R-squared suggests that the model does a better job of explaining the variability in the dependent variable compared to a model with a lower R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79be39",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0adfd5",
   "metadata": {},
   "source": [
    "**Adjusted R-squared**, also known as the **adjusted coefficient of determination**, is a modified version of the regular R-squared (R2) in the context of linear regression. Adjusted R-squared is designed to address some of the limitations of the standard R-squared and provides a more reliable measure of a model's goodness of fit.\n",
    "\n",
    "##### Calculation of Adjusted R2 score:\n",
    "\n",
    "$$ Adjusted R^2 = 1 - \\frac{(1-R^2).(n-1)}{n - k - 1} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $R^2$ is the R2 score of the model.\n",
    "- **n** is the number of variables in the model.\n",
    "- **k** is the number of independent variables in the model.\n",
    "\n",
    "##### Differences between R2 and Adjust R2 are as follows:\n",
    "\n",
    "\n",
    "|Points|R2|Adjusted R2|\n",
    "|---|---|---|\n",
    "|**Definition**|R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model.|Adjusted R-squared addresses the issue of overfitting by penalizing the inclusion of unnecessary independent variables. It considers the number of predictors in the model.|\n",
    "|**Overfitting Issue**|R2 increases as we add more independent variables to the model, even if they don't significantly improve the model's performance. This can lead to overfitting.|Adjusted R2 doesn't vary much when we add unnecessary independent variables to the training model.|\n",
    "|**Fit**|R2 score takes into account any features whether it is important or not.|Adjusted R-squared provides a more conservative measure of goodness of fit compared to regular R-squared. It takes into account the number of predictors, which means that adding uninformative predictors to the model will lead to a decrease in adjusted R-squared.|\n",
    "|**Prediction**|R2 does not account for the number of predictors in the model, which can make it difficult to compare models with a different number of independent variables.|Adjusted R-squared can be used to compare models with different numbers of predictors more accurately.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d95c71",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f34e4",
   "metadata": {},
   "source": [
    "**Adjusted R-squared** is more appropriate to use in the following situations:\n",
    "\n",
    "1. **Model Comparison:** When comparing different regression models with varying numbers of predictors (independent variables), adjusted R-squared is more informative. Regular R-squared tends to increase as we add more predictors, even if those predictors do not significantly improve the model's performance. Adjusted R-squared penalizes the inclusion of unnecessary or uninformative predictors, providing a more balanced comparison of models.\n",
    "\n",
    "2. **Feature Selection:** In the context of feature selection or variable selection, adjusted R-squared is valuable. When we are deciding which predictors to include in the model, we want to prioritize those that contribute meaningfully to explaining the variance in the dependent variable. Adjusted R-squared helps us identify whether adding a new predictor improves the model's fit, considering the model's complexity. It discourages including predictors that do not enhance the model's explanatory power.\n",
    "\n",
    "3. **Preventing Overfitting:** When dealing with models with a large number of predictors, it's essential to assess whether the inclusion of additional predictors leads to overfitting. Overfit models may perform well on the training data but generalize poorly to new data. Adjusted R-squared encourages the selection of a parsimonious model (one with fewer predictors) that is less likely to overfit the training data.\n",
    "\n",
    "4. **Robust Model Selection:**  When choosing the best model for predictive or explanatory purposes, adjusted R-squared is a more robust metric. It guides model selection by emphasizing models that maximize explanatory power while controlling for the trade-off between complexity and fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb4565",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17feb9",
   "metadata": {},
   "source": [
    "In the context of regression analysis, several metrics are commonly used to evaluate the performance of a regression model. Three of the most widely used metrics are **Root Mean Squared Error (RMSE)**, **Mean Squared Error (MSE)**, and **Mean Absolute Error (MAE)**. These metrics provide insights into the accuracy and error of the model's predictions.\n",
    "\n",
    "Here's an explanation of these metrics:\n",
    "\n",
    "1. **Mean Squared Error (MSE):**\n",
    "\n",
    "- MSE is a very common metric for regression analysis.\n",
    "- It measures the average of the squared differences between the actual and predicted values i.e. the residuals.\n",
    "- MSE is expressed in squared units, which may not be as interpretable as RMSE.\n",
    "- It is useful for mathematical calculations and optimization but less intuitive for explaining errors in real-world units.\n",
    "- MSE is calculated as follows:\n",
    "\n",
    "$$ MSE = \\frac{\\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2}{n} $$\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE):**\n",
    "\n",
    "- RMSE is a commonly used metric for regression analysis.\n",
    "- It measures the square root of the average of the squared differences between the actual and predicted values.\n",
    "- RMSE is expressed in the same units as the dependent variable (target), making it interpretable in the context of the problem.\n",
    "- It emphasizes larger errors more than smaller errors due to the squaring of differences.\n",
    "- RMSE is calculated as follows:\n",
    "\n",
    "$$ RMSE = \\sqrt(MSE) $$\n",
    "\n",
    "\n",
    "3. **Mean Absolute Error (MAE):**\n",
    "\n",
    "- MAE is a metric that measures the average of the absolute differences between the actual and predicted values.\n",
    "- Unlike RMSE and MSE, MAE does not square the errors, which makes it less sensitive to outliers.\n",
    "- MAE is expressed in the same units as the dependent variable and is easily interpretable.\n",
    "- MAE is calculated as follows:\n",
    "\n",
    "$$ MAE = \\frac{\\sum_{i=1}^{n} |y_{i} - \\hat{y_{i}}|}{n} $$\n",
    "\n",
    "##### Interpretation:\n",
    "\n",
    "**RMSE:** RMSE provides a measure of the typical size of the errors between the actual and predicted values. Smaller RMSE values indicate better model performance. It is more sensitive to large errors.\n",
    "\n",
    "**MSE:** MSE is closely related to RMSE but is expressed in squared units. It provides a measure of the average squared error. Smaller MSE values indicate better model performance.\n",
    "\n",
    "**MAE:** MAE provides a measure of the average absolute error between the actual and predicted values. It is less sensitive to outliers and emphasizes the magnitude of errors rather than their direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec414ac",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23e571",
   "metadata": {},
   "source": [
    "## <u>Advantages</u>\n",
    "\n",
    "##### RMSE:\n",
    "\n",
    "- **Sensitivity to Large Errors:** RMSE gives more weight to larger errors, making it useful when large errors are costly or significant. It emphasizes the importance of reducing large discrepancies between actual and predicted values.\n",
    "\n",
    "- **Differentiation:** RMSE can help differentiate between models with different levels of predictive accuracy. A lower RMSE indicates a better fit to the data.\n",
    "\n",
    "##### MSE:\n",
    "\n",
    "- **Mathematical Properties:** MSE has favorable mathematical properties, making it easier to work with in optimization and mathematical analysis. It is particularly useful in statistical inference and hypothesis testing.\n",
    "\n",
    "- **Emphasis on Squared Errors:** Squaring the errors in MSE emphasizes the magnitude of errors while maintaining a positive value for all errors. It is useful when negative and positive errors should be treated similarly.\n",
    "\n",
    "\n",
    "##### MAE:\n",
    "\n",
    "- **Robustness to Outliers:** MAE is less sensitive to outliers compared to RMSE and MSE. It provides a more robust measure of the typical error and is not heavily influenced by a few extreme values.\n",
    "\n",
    "- **Interpretability:** MAE is expressed in the same units as the dependent variable, making it easy to interpret. It provides a straightforward measure of the average magnitude of errors.\n",
    "\n",
    "- **Balance Between Underestimation and Overestimation:** MAE treats overestimation and underestimation equally, which can be appropriate in scenarios where both types of errors have similar consequences.\n",
    "\n",
    "\n",
    "## <u>Disadvantages</u>\n",
    "\n",
    "##### RMSE:\n",
    "\n",
    "- **Sensitivity to Outliers:** RMSE is highly sensitive to outliers, meaning that a few extreme values can have a significant impact on the metric. Outliers can distort the interpretation of the model's performance.\n",
    "\n",
    "- **Units of Measurement:** RMSE is expressed in the same units as the dependent variable, which can make it less interpretable in certain contexts.\n",
    "\n",
    "##### MSE:\n",
    "\n",
    "- **Units of Measurement:** Similar to RMSE, MSE is expressed in squared units, making it less interpretable in real-world terms. This can hinder communication of the model's performance to non-technical stakeholders.\n",
    "\n",
    "- **Sensitivity to Outliers:** MSE is also highly sensitive to outliers, and it may not accurately represent the model's overall performance when outliers are present.\n",
    "\n",
    "##### MAE:\n",
    "\n",
    "- **Lack of Sensitivity:** MAE does not differentiate between small and large errors. It treats all errors with equal weight, which may not be appropriate in situations where larger errors are more critical.\n",
    "\n",
    "- **Inferior Differentiation:** MAE may not differentiate between models with varying levels of predictive accuracy as effectively as RMSE, especially when small errors are common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e940b6a",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33484499",
   "metadata": {},
   "source": [
    "**Lasso regularization**, short for **Least Absolute Shrinkage and Selection Operator**, is a form of regularization used in linear regression and other regression models. It is designed to prevent overfitting by adding a penalty term to the linear regression cost function. Lasso regularization encourages the model to reduce the magnitude of less important features' coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "##### <u>Difference with Ridge Regression</u>\n",
    "\n",
    "|Point|Lasso Regression|Ridge Regression\n",
    "|---|---|---|\n",
    "|**Penalty**|Lasso uses an L1 penalty (absolute values of coefficients)|Ridge uses an L2 penalty (squared values of coefficients)|\n",
    "|**Effect on Coefficients**| Lasso encourages some coefficients to be exactly zero, effectively performing feature selection.|Ridge reduces the magnitude of coefficients but does not force them to be zero.|\n",
    "|**Solution Paths**|Lasso can lead to sparse solutions with a subset of non-zero coefficients, making it useful for feature selection.|Ridge tends to keep all features, but their importance is reduced.|\n",
    "|**Interpretability**| Lasso often results in a more interpretable model because it selects a subset of features, making it clear which features are important for prediction.|Since it tends to keep all the features, doesn't produce a clear interpretable model.|\n",
    "\n",
    "##### <u>When to Use Lasso Regularization:</u>\n",
    "\n",
    "- **Feature Selection:** Lasso is particularly useful when we want to perform feature selection and reduce the number of predictors in the model. It helps us identify the most important features and eliminate irrelevant ones.\n",
    "- **Sparse Models:** If we expect that only a few features are truly relevant for prediction, Lasso can lead to sparse models with a small number of non-zero coefficients.\n",
    "- **Interpretability:** Lasso may be preferred when model interpretability is crucial, as it simplifies the model and makes it clear which features contribute to predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95377f08",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc5b32",
   "metadata": {},
   "source": [
    "Regularized linear models are effective tools in preventing overfitting in machine learning. Overfitting occurs when a model learns to fit the training data too closely, capturing noise and minor fluctuations rather than the underlying patterns. This leads to poor generalization to new, unseen data. Regularized linear models add penalty terms to the loss function(cost function), discouraging complex models with large coefficients. Let's explore two common types of regularized linear models, Ridge and Lasso, and illustrate how they help prevent overfitting with an example.\n",
    "\n",
    "##### Ridge Regression (L2 Regularization):\n",
    "\n",
    "Ridge regression adds an L2 penalty to the linear regression cost function. It aims to minimize the sum of squared differences between actual and predicted values while penalizing large coefficient values.\n",
    "\n",
    "$$ J(\\theta) = MSE + \\alpha\\sum_{i=1}^{n} \\theta_{i}^2 $$\n",
    "\n",
    "Here, $\\alpha$ controls the strength of regularization. A higher $\\alpha$ leads to more regularization.\n",
    "\n",
    "##### Lasso Regression (L1 Regularization):\n",
    "\n",
    "Lasso regression adds an L1 penalty to the cost function. It encourages some coefficients to become exactly zero, effectively performing feature selection.\n",
    "\n",
    "The Lasso regression cost function is:\n",
    "\n",
    "$$ J(\\theta) = MSE + \\alpha\\sum_{i=1}^{n} |\\theta_{i}| $$\n",
    "\n",
    "Again, $\\alpha$ determines the regularization strength.\n",
    "\n",
    "## Illustrative Example:\n",
    "\n",
    "Suppose we are building a linear regression model to predict housing prices based on several features. We can collect a dataset with 100 samples and 20 features. Without regularization, out model fits the training data very closely, capturing noise in the data. The model's coefficients are large, indicating a high degree of complexity.\n",
    "\n",
    "Now, we apply Ridge and Lasso regularization to the model with different $\\alpha$ values. As $\\alpha$ increases, the penalty on coefficient magnitudes increases.\n",
    "\n",
    "##### Ridge Regression (L2 Regularization):\n",
    "\n",
    "- As we increase $\\alpha$ in Ridge regression, the coefficients decrease in magnitude. Some coefficients may become small but not exactly zero.\n",
    "- The model generalizes better to new data because it is less sensitive to minor fluctuations in the training data.\n",
    "- Ridge helps prevent overfitting by smoothing out the model.\n",
    "\n",
    "\n",
    "##### Lasso Regression (L1 Regularization):\n",
    "\n",
    "- When we increase $\\alpha$ in Lasso regression, the model encourages some coefficients to become exactly zero.\n",
    "- This leads to feature selection, as irrelevant features are assigned zero coefficients.\n",
    "- The model becomes more interpretable, and overfitting is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42588f34",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d9b4c",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Ridge and Lasso regression, offer valuable tools for regression analysis by preventing overfitting and promoting feature selection. However, they are not always the best choice and have some limitations. Here are the limitations of regularized linear models and reasons why they may not always be the best option:\n",
    "\n",
    "1. **Linearity Assumption:** Regularized linear models assume a linear relationship between the predictors and the target variable. If the underlying relationship is highly nonlinear, linear models may not capture the data's complexity effectively.\n",
    "\n",
    "2. **Feature Selection Bias:** While Lasso performs feature selection by driving some coefficients to zero, it may be biased in its selection, favoring certain features over others. It might overlook relevant but correlated features if they are competing for selection.\n",
    "\n",
    "3. **Complexity of Model Selection:** Choosing the appropriate regularization strength ($\\alpha$) can be challenging. It often requires **cross-validation**, which can be computationally expensive and may not always lead to a clear choice.\n",
    "\n",
    "4. **Loss of Information:** Lasso can eliminate features entirely, leading to a loss of information. This may not be desirable if we believe that all features contribute meaningfully to the target variable.\n",
    "\n",
    "5. **Interpretability:** Ridge and Lasso tend to reduce the magnitude of coefficients, making the model more interpretable. However, when it's essential to preserve the original feature scales for interpretation (e.g., in medical or financial applications), regularized models may not be the best choice.\n",
    "\n",
    "6. **Sparse Data:** Regularized linear models may not perform well with sparse data, where there are relatively few data points compared to the number of features. In such cases, the penalty terms can dominate the optimization process, making the model less reliable.\n",
    "\n",
    "7. **Robustness to Outliers:** Regularized models are not inherently robust to outliers. In the presence of extreme outliers, they may still be influenced by these data points. Robust regression techniques might be more appropriate in such cases.\n",
    "\n",
    "8. **Multicollinearity Handling:** While Ridge regression can handle multicollinearity by distributing the effects across correlated features, Lasso may select only one of the correlated features and assign it a non-zero coefficient. Handling multicollinearity might require alternative methods like principal component analysis (PCA).\n",
    "\n",
    "9. **Computation Complexity:** Regularized models can be computationally more demanding than standard linear regression, particularly when optimizing the regularization strength ($\\alpha$) through cross-validation.\n",
    "\n",
    "10. **Algorithm Choice:** The choice between Ridge and Lasso depends on the specific characteristics of the data. If it is unclear which form of regularization is more appropriate, it can be challenging to decide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f3415",
   "metadata": {},
   "source": [
    "## Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f88e29",
   "metadata": {},
   "source": [
    "Choosing the better performer between Model A with an RMSE of 10 and Model B with an MAE of 8 depends on the specific characteristics of the problem and the priorities of the analysis. Here's a comparison of the two models and their respective evaluation metrics, along with the limitations to consider:\n",
    "\n",
    "##### RMSE (Root Mean Squared Error):\n",
    "\n",
    "- RMSE is a metric that emphasizes the importance of larger errors. It penalizes models more for large prediction errors.\n",
    "- An RMSE of 10 indicates that, on average, the predictions of Model A have an error of about 10 units in the same units as the target variable.\n",
    "\n",
    "##### MAE (Mean Absolute Error):\n",
    "\n",
    "- MAE is a metric that treats all errors, regardless of their magnitude, with equal weight. It measures the average magnitude of errors.\n",
    "- An MAE of 8 indicates that, on average, the predictions of Model B have an error of about 8 units in the same units as the target variable.\n",
    "\n",
    "##### Considerations for Choosing the Better Model:\n",
    "\n",
    "- If our primary concern is to minimize the impact of large errors and we want to ensure that the model performs well for all observations, Model A (with RMSE) might be preferred. RMSE's sensitivity to large errors means that it gives more weight to those errors, which can be important in certain applications.\n",
    "\n",
    "- If our primary concern is to minimize the average magnitude of errors across all observations, Model B (with MAE) might be preferred. MAE treats all errors equally and does not heavily penalize large errors. It provides a measure of the typical error.\n",
    "\n",
    "- It's essential to consider the specific context and objectives of the analysis. For example, in applications where large prediction errors are costly or have significant consequences, RMSE might be more appropriate. In other cases, where we want a more interpretable and balanced measure of error, MAE might be the better choice.\n",
    "\n",
    "\n",
    "##### Limitations to the Choice of Metric:\n",
    "\n",
    "- The choice of evaluation metric should align with the problem's context and the goals of the analysis. The choice can be subjective and depend on the domain, stakeholders' preferences, and the nature of the data.\n",
    "\n",
    "- While RMSE and MAE provide important insights into model performance, they do not consider the impact of errors on decision-making. In some cases, other metrics, such as business-specific cost functions, might be more appropriate to evaluate models based on their impact on the organization's goals.\n",
    "\n",
    "- Model evaluation should not rely solely on a single metric. It's often beneficial to use a combination of metrics to gain a more comprehensive understanding of a model's strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177768a",
   "metadata": {},
   "source": [
    "## Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf95ee7",
   "metadata": {},
   "source": [
    "Choosing between Ridge regularization (Model A) with a regularization parameter of 0.1 and Lasso regularization (Model B) with a regularization parameter of 0.5 depends on the specific characteristics of the data and the goals of the analysis. Let's discuss the comparison and potential trade-offs or limitations of these two regularization methods:\n",
    "\n",
    "##### Ridge Regularization (L2 Regularization):\n",
    "\n",
    "- Ridge regularization adds an L2 penalty term to the linear regression cost function. It discourages large coefficient values without forcing any coefficients to become exactly zero.\n",
    "- The regularization parameter ($\\alpha$) controls the strength of regularization. Smaller $\\alpha$ values result in weaker regularization, while larger $\\alpha$ values lead to stronger regularization.\n",
    "\n",
    "##### Lasso Regularization (L1 Regularization):\n",
    "\n",
    "- Lasso regularization adds an L1 penalty term to the cost function. It encourages some coefficients to become exactly zero, effectively performing feature selection.\n",
    "- The regularization parameter ($\\alpha$) also controls the strength of regularization, with larger $\\alpha$ values leading to stronger regularization.\n",
    "\n",
    "## Factors to Consider:\n",
    "\n",
    "The choice between Ridge and Lasso depends on the nature of the data and the problem:\n",
    "\n",
    "- If we have a dataset with many features and suspect that some of them are irrelevant, Lasso (Model B) might be preferred. It can perform feature selection and set some coefficients to zero, simplifying the model.\n",
    "- If all features are relevant, or if we want to preserve all features, Ridge (Model A) can be a better choice. It reduces the magnitude of coefficients without eliminating them.\n",
    "\n",
    "## Trade-offs and Limitations:\n",
    "\n",
    "- Ridge regularization is less likely to produce models with exactly zero coefficients. If feature selection is not a priority, Ridge may be a more stable choice, as it can handle highly correlated features more effectively.\n",
    "\n",
    "- Lasso regularization is more likely to lead to sparse models with a subset of non-zero coefficients. It can be sensitive to the choice of $\\alpha$ and may select one feature over another in the presence of correlated features.\n",
    "\n",
    "- The choice of $\\alpha$ values for both Ridge and Lasso should be determined through cross-validation, which can be computationally expensive.\n",
    "\n",
    "- Ridge regularization tends to distribute the influence of correlated features, making it more robust to multicollinearity, while Lasso may select only one feature from the correlated group.\n",
    "\n",
    "- Both Ridge and Lasso introduce bias in the coefficients by shrinking them towards zero, and the choice of regularization strength ($\\alpha$) involves a trade-off between bias and variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
