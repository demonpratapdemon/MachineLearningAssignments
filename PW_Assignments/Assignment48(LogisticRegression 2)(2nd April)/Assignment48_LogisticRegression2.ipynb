{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d6aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66196f0",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b277707",
   "metadata": {},
   "source": [
    "Grid Search Cross-Validation (GridSearchCV) is a technique used in Machine Learning to tune hyperparameters for a model. The goal is to systematically search through a predefined hyperparameter grid and find the combination that yields the best performance for a given machine learning algorithm.\n",
    "\n",
    "##### Purpose of GridSearchCV:\n",
    "\n",
    "**1.Hyperparameter Tuning:** In machine learning algorithms, hyperparameters are parameters that are not learned from the training data but need to be set before training. Examples include the learning rate in gradient boosting or the regularization parameter in a support vector machine (SVM). GridSearchCV helps in finding the optimal values for these hyperparameters.\n",
    "\n",
    "**2.Optimization:** GridSearchCV aims to optimize the performance of a model by trying different combinations of hyperparameters. It automates the process of experimenting with various hyperparameter values, saving time and effort compared to manual tuning.\n",
    "\n",
    "**3.Cross-Validation:** It incorporates cross-validation during the hyperparameter search. Cross-validation helps in obtaining a more robust estimate of the model's performance by training and evaluating the model on multiple subsets of the data. GridSearchCV uses cross-validation to prevent overfitting to a specific subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34804118",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=10, n_classes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2af2bc",
   "metadata": {},
   "source": [
    "##### How GridSearchCV Works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9c10c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00774a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10], &#x27;kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d3dd4",
   "metadata": {},
   "source": [
    "##### Getting the best params learnt and the score obtained by the model from the above param_grid with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdbcb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:{'C': 0.1, 'kernel': 'linear'}\n",
      "Best score:0.9595959595959597\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best params:{grid_search.best_params_}\")\n",
    "print(f\"Best score:{grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20a533",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb1d5f2",
   "metadata": {},
   "source": [
    "|Points|Grid Search CV|Randomize Search CV|\n",
    "|---|---|---|\n",
    "|**Approach**|Grid Search CV performs an exhaustive search over a predefined hyperparameter grid. It evaluates the model for all possible combinations of hyperparameter values within the specified grid.|Randomized Search CV, on the other hand, explores the hyperparameter space by randomly sampling a specified number of combinations. Instead of trying all possible combinations, it randomly selects a subset.|\n",
    "|**Computational Cost**|The main drawback of Grid Search CV is its computational cost. As it considers every possible combination, the number of models to train and evaluate grows exponentially with the number of hyperparameters and their possible values.|Randomized Search CV is computationally less expensive than Grid Search CV because it doesn't evaluate every combination. The number of iterations is controlled by the ***n_iter parameter***, making it more scalable for large hyperparameter spaces.|\n",
    "|**Efficiency**|Grid Search CV provides a more precise search over the hyperparameter space, ensuring that no combination is missed. However, this precision comes at the cost of increased computational requirements.|While Randomized Search CV may not guarantee that all possible combinations are explored, it often finds good hyperparameter values more efficiently. This is particularly beneficial when the hyperparameter space is large, and a complete search would be too time-consuming.|\n",
    "\n",
    "##### When to Choose One Over the Other:\n",
    "\n",
    "**1. Grid Search CV:** Grid Search CV should be used when we have a relatively small hyperparameter space, and we want to perform an exhaustive search to find the best combination. It's suitable when computational resources are not a major constraint.\n",
    "\n",
    "**2.Randomized Search CV:** Randomized Search CV should be chosen when the hyperparameter space is large, and performing an exhaustive search would be computationally expensive or impractical. It's particularly useful when we have limited resources or when we want to quickly identify a good set of hyperparameters.\n",
    "\n",
    "**3.Hybrid Approach:** In some cases, a hybrid approach can be effective. We might start with a randomized search to explore a broad range of hyperparameter values, and then use the insights gained to define a narrower grid for a subsequent grid search in the promising regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa0ef3",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab69f07",
   "metadata": {},
   "source": [
    "**Data Leakage:** Data leakage in machine learning refers to the situation where information from the future or external to the training dataset is used to make predictions during model training. This can lead to overly optimistic performance estimates, as the model is unintentionally exposed to information it wouldn't have access to in a real-world scenario. Data leakage is a significant problem because it can result in models that perform well on the training data but fail to generalize to new, unseen data.\n",
    "\n",
    "##### Examples of Data Leakage:\n",
    "\n",
    "**1.Temporal Data Leakage:** This occurs when future information is included in the training dataset, leading to an unrealistic evaluation of model performance. For example, predicting stock prices based on historical data and including future stock prices as features would introduce temporal data leakage.\n",
    "\n",
    "**2. Data Preprocessing Leakage:** Leakage can also occur during data preprocessing steps. For instance, scaling features based on the entire dataset, including the test set, can introduce information from the test set into the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44749f40",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8789489",
   "metadata": {},
   "source": [
    "Preventing data leakage is crucial to building machine learning models that generalize well to new, unseen data. Here are some strategies to help prevent data leakage:\n",
    "\n",
    "**1. Strict Separation of Training and Testing Data:** Ensure a clear separation between training and testing datasets. Never use information from the testing set during model training, validation, or hyperparameter tuning. Use techniques like cross-validation to assess model performance without leaking information from the test set.\n",
    "\n",
    "**2. Temporal Validation Splits:** If the data involves time series, make sure to split the data chronologically. The training set should contain earlier time points, and the validation/test sets should contain later time points. This is essential to simulate real-world scenarios where future data is not known.\n",
    "\n",
    "**3. Feature Scaling and Preprocessing:** Apply feature scaling and preprocessing techniques separately for the training and validation/test sets. Scaling parameters (e.g., mean and standard deviation for normalization) should be computed only on the training set and then applied consistently to the validation/test sets.\n",
    "\n",
    "**4. Cross-Validation:** While using cross-validation, ensure that each fold is created independently. Data leakage can occur if the folds share information, such as when using k-fold cross-validation without shuffling the data properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bd078",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59263948",
   "metadata": {},
   "source": [
    "##### Confusion Matrix:\n",
    "\n",
    "A confusion matrix is a table that is often used to evaluate the performance of a **classification model**. It is particularly useful for binary classification problems, where the goal is to classify instances into one of two classes, however, it can be extended to multi-class classification as well.\n",
    "\n",
    "The confusion matrix consists of four main components:\n",
    "\n",
    "**1. True Positives (TP):** Instances that are actually positive and are correctly predicted as positive by the model.\n",
    "\n",
    "**2. True Negatives (TN):** Instances that are actually negative and are correctly predicted as negative by the model.\n",
    "\n",
    "**3. False Positives (FP):** Instances that are actually negative but are incorrectly predicted as positive by the model (Type I error).\n",
    "\n",
    "**4. False Negatives (FN):** Instances that are actually positive but are incorrectly predicted as negative by the model (Type II error).\n",
    "\n",
    "Using these components, various performance metrics can be derived:\n",
    "\n",
    "**1. Accuracy:** Accuracy measures the overall correctness of the model and is calculated as $(TP + TN) / (TP + TN + FP + FN)$.\n",
    "\n",
    "**2. Precision (Positive Predictive Value):** Precision measures the accuracy of positive predictions and is calculated as $TP / (TP + FP)$. It is the ratio of correctly predicted positive observations to the total predicted positives. **Example:** It is mostly used to determine spamming.\n",
    "\n",
    "**3. Recall (Sensitivity or True Positive Rate):** Recall measures the ability of the model to capture all the positive instances and is calculated as $TP / (TP + FN)$. It is the ratio of correctly predicted positive observations to the total actual positives. **Example:** It is used to in health care scenarios.\n",
    "\n",
    "**4. Specificity (True Negative Rate):** Specificity measures the ability of the model to correctly identify negative instances and is calculated as $TN / (TN + FP)$. It is the ratio of correctly predicted negative observations to the total actual negatives.\n",
    "\n",
    "**5. F1 Score:** The F1 score is the harmonic mean of precision and recall and is calculated as $2 * (Precision * Recall) / (Precision + Recall)$. It provides a balanced measure that considers both false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b45491",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5d7b8",
   "metadata": {},
   "source": [
    "|Point|Precision|Recall|\n",
    "|---|---|---|\n",
    "|**Definition**|Precision, also known as positive predictive value, measures the accuracy of positive predictions made by the model.|Recall, also known as sensitivity or true positive rate, measures the ability of the model to capture all the positive instances in the dataset.|\n",
    "|**Formula**|$$ \\frac{True Positive}{True Positive + False Positive} $$|$$ \\frac{True Positive}{True Positive + False Negative} $$|\n",
    "|**Focus**|Precision focuses on the accuracy of positive predictions, emphasizing the ability of the model to avoid false positives.|Recall emphasizes the ability of the model to capture all positive instances, regardless of how many false negatives occur.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d423ac",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9fac80",
   "metadata": {},
   "source": [
    "The following are the ways to interpret the confusion matrix:\n",
    "\n",
    "**1. Overall Model Accuracy:**\n",
    "\n",
    "    - Calculate overall accuracy: $\\frac{TP + TN}{ TP + FP + FN + TN}$ \n",
    "    - This gives you an overall measure of how well the model is performing.\n",
    "    \n",
    "**2. Error Types:**\n",
    "\n",
    "    - **Type I Error (False Positives)**\n",
    "        -- False positives (FP) occur when the model predicts a positive class when it is actually negative. \n",
    "    -- **Type II Error (False Negatives)**\n",
    "        -- False negatives (FN) occur when the model predicts a negative class when it is actually positive.\n",
    "\n",
    "**3. Precision and Recall:**\n",
    "    \n",
    "    - Precision helps us understand the accuracy of positive predictions. A high precision means that when the model predicts a positive, it is likely to be correct.\n",
    "    - Recall focuses on the model's ability to capture all positive instances. A high recall means the model is effective at finding most positive instances.\n",
    "    \n",
    "**4. F1 Score:**\n",
    "\n",
    "    - The F1 score is the harmonic mean of precision and recall, providing a balance between the two metrics. It is particularly useful when there is an imbalance between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3021fd",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe596d9",
   "metadata": {},
   "source": [
    "##### Confusion Matrix:\n",
    "\n",
    "A confusion matrix is a table that is often used to evaluate the performance of a **classification model**. It is particularly useful for binary classification problems, where the goal is to classify instances into one of two classes, however, it can be extended to multi-class classification as well.\n",
    "\n",
    "The confusion matrix consists of four main components:\n",
    "\n",
    "**1. True Positives (TP):** Instances that are actually positive and are correctly predicted as positive by the model.\n",
    "\n",
    "**2. True Negatives (TN):** Instances that are actually negative and are correctly predicted as negative by the model.\n",
    "\n",
    "**3. False Positives (FP):** Instances that are actually negative but are incorrectly predicted as positive by the model (Type I error).\n",
    "\n",
    "**4. False Negatives (FN):** Instances that are actually positive but are incorrectly predicted as negative by the model (Type II error).\n",
    "\n",
    "Using these components, various performance metrics can be derived:\n",
    "\n",
    "**1. Accuracy:** Accuracy measures the overall correctness of the model and is calculated as $(TP + TN) / (TP + TN + FP + FN)$.\n",
    "\n",
    "**2. Precision (Positive Predictive Value):** Precision measures the accuracy of positive predictions and is calculated as $TP / (TP + FP)$. It is the ratio of correctly predicted positive observations to the total predicted positives. **Example:** It is mostly used to determine spamming.\n",
    "\n",
    "**3. Recall (Sensitivity or True Positive Rate):** Recall measures the ability of the model to capture all the positive instances and is calculated as $TP / (TP + FN)$. It is the ratio of correctly predicted positive observations to the total actual positives. **Example:** It is used to in health care scenarios.\n",
    "\n",
    "**4. Specificity (True Negative Rate):** Specificity measures the ability of the model to correctly identify negative instances and is calculated as $TN / (TN + FP)$. It is the ratio of correctly predicted negative observations to the total actual negatives.\n",
    "\n",
    "**5. F1 Score:** The F1 score is the harmonic mean of precision and recall and is calculated as $2 * (Precision * Recall) / (Precision + Recall)$. It provides a balanced measure that considers both false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c4aa0",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d443b",
   "metadata": {},
   "source": [
    "Accuracy is a performance metric that measures the overall correctness of a classification model. It is calculated as the ratio of correctly predicted instances (both true positives and true negatives) to the total number of instances as follows:\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + FP + TN + FN} $$\n",
    "\n",
    "The accuracy formula shows that accuracy is determined by the sum of true positives and true negatives, divided by the total number of instances. In other words, accuracy represents the ratio of correct predictions to the total number of predictions.\n",
    "\n",
    "However, accuracy might not be the most suitable metric in all situations, especially when dealing with imbalanced datasets where one class significantly outnumbers the other. In such cases, a high accuracy value can be misleading, as the model might be biased towards the majority class. It's important to consider other metrics, such as precision, recall, F1 score, and others, to get a more comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c11b2",
   "metadata": {},
   "source": [
    "##  Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc3d6aa",
   "metadata": {},
   "source": [
    "A confusion matrix can be a valuable tool for identifying potential biases or limitations in your machine learning model, especially when dealing with classification tasks. Here are several ways to use a confusion matrix to uncover issues related to bias or limitations:\n",
    "\n",
    "1. **Class Imbalance:** The distribution of actual instances across different classes. If there's a significant class imbalance, where one class has many more instances than the other, the model may be biased towards the majority class. This can lead to high accuracy but poor performance on the minority class.\n",
    "\n",
    "2. **False Positive and False Negative Rates:** The false positive rate (FPR) and false negative rate (FNR) in the confusion matrix. A high FPR may indicate a tendency to incorrectly classify negative instances as positive, while a high FNR may suggest a bias towards misclassifying positive instances as negative.\n",
    "\n",
    "3. **Precision and Recall Disparities:** . A large disparity between precision and recall for different classes can highlight issues. For example, a high precision but low recall may indicate a model that is conservative in making positive predictions.\n",
    "\n",
    "4. **Threshold Effects:** Evaluate the impact of changing probability thresholds for class predictions. If the model's performance changes significantly with different thresholds, it may indicate sensitivity to the decision boundary and potential biases.\n",
    "\n",
    "5. **Reviewing Data Quality:** Examine the quality and representativeness of the training data. Biases in the training data, such as underrepresentation of certain groups, may lead to biased model predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
