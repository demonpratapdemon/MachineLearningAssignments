{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7480fa",
   "metadata": {
    "id": "5e7480fa"
   },
   "source": [
    "# Objective: Assess understanding of optimization algorithms in artificial neural networks. Evaluate the application and comparison of different optimizers. Enhance knowledge of optimizers' impact on model convergence and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a81a4",
   "metadata": {
    "id": "4c0a81a4"
   },
   "source": [
    "## Part 1: Understanding Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83137e",
   "metadata": {
    "id": "df83137e"
   },
   "source": [
    "### 1. What is the role of optimization algorithms in artificial neural networks? Why are they necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406c55d0",
   "metadata": {
    "id": "406c55d0"
   },
   "source": [
    "**Optimization** algorithms play a crucial role in the training of artificial neural networks by minimizing the loss function (also known as the **cost function** or **objective function**), which measures how well the network’s predictions align with the actual target values. These algorithms adjust the model’s parameters (weights and biases) iteratively to improve its performance on the training data.\n",
    "\n",
    "##### Role of Optimization Algorithms in Neural Networks:\n",
    "\n",
    "1. **Minimizing the Loss Function:** The main task of an optimization algorithm is to find the set of model parameters (weights and biases) that minimizes the loss function. This loss function measures the difference between the predicted outputs and the true outputs. Lowering the loss function corresponds to improving the model's predictive accuracy.\n",
    "\n",
    "2. **Efficient Parameter Updates:** Neural networks often have millions of parameters, making it infeasible to find the optimal set of parameters through brute force methods like grid search. Optimization algorithms efficiently update these parameters by computing the gradients (or slopes) of the loss function with respect to the parameters and using these gradients to make informed adjustments.\n",
    "\n",
    "3. **Gradient-Based Learning:** Optimization algorithms like **Gradient Descent** use the derivative of the loss function (calculated using backpropagation) to determine the direction and magnitude of changes to the weights and biases. The network learns by adjusting its parameters in the direction that reduces the loss.\n",
    "\n",
    "4. **Handling Non-Convex Problems:** The loss surface in neural networks, especially in deep networks, is often highly non-convex with many local minima and saddle points. Optimization algorithms help navigate this complex landscape to find good solutions, even if they are not globally optimal.\n",
    "\n",
    "\n",
    "##### Why Optimization Algorithms Are Necessary:\n",
    "\n",
    "1. **High Dimensionality:** Neural networks typically operate in a high-dimensional parameter space (hundreds, thousands, or even millions of parameters). Finding the optimal set of parameters in such a large space is computationally challenging. Optimization algorithms are designed to efficiently search for these parameters, avoiding brute-force methods.\n",
    "\n",
    "2. **Iterative Improvement:** Training a neural network requires an iterative process of parameter updates based on feedback from the loss function. Without an optimization algorithm, it would be difficult to know how to adjust the parameters to minimize error and improve performance. Optimization algorithms provide a systematic way to make these updates.\n",
    "\n",
    "3. **Preventing Overfitting and Underfitting:** A well-chosen optimization algorithm helps balance the model’s ability to generalize to new data (avoiding overfitting) while ensuring that the model captures the underlying patterns in the training data (avoiding underfitting). Regularization techniques, such as L2 regularization, can be incorporated into the optimization process to penalize overly complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce7c5a",
   "metadata": {
    "id": "dbce7c5a"
   },
   "source": [
    "### 2. Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30691716",
   "metadata": {
    "id": "30691716"
   },
   "source": [
    "**Gradient Descent** is an optimization algorithm used to minimize a function by iteratively moving towards the minimum of the function. In the context of neural networks, gradient descent aims to minimize the loss function by updating the **model’s parameters (weights and biases)** in the direction of the steepest descent, i.e., the negative gradient of the loss function with respect to those parameters.\n",
    "\n",
    "##### Key Variants of Gradient Descent:\n",
    "\n",
    "1. **Batch Gradient Descent (GD):** Uses the entire dataset to compute the gradient of the loss function for each parameter update.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD):** Uses one data point (example) at a time to update the parameters.\n",
    "\n",
    "3. **Mini-Batch Gradient Descent (MBGD):** Uses a small, random subset of the dataset (mini-batch) to compute the gradient at each update step.\n",
    "\n",
    "4. **Momentum:** Improves gradient descent by adding a fraction of the previous update to the current update, helping it converge faster and avoid oscillations.\n",
    "\n",
    "5. **Adagrad(Adaptive Gradient Descent):** Adjusts the learning rate individually for each parameter based on the historical sum of gradients, allowing it to adapt to sparse or dense gradients.\n",
    "\n",
    "6. **RMSprop:** Addresses Adagrad's diminishing learning rate problem by using a **moving average of the squared gradients** to scale learning rates.\n",
    "\n",
    "7. **Adam (Adaptive Moment Estimation):** Combines momentum and RMSprop by maintaining both first and second moment estimates of the gradient, providing an adaptive learning rate for each parameter.\n",
    "\n",
    "\n",
    "##### Differences and Trade-offs:\n",
    "\n",
    "|Point|Batch Gradient Descent|Stochastic Gradient Descent (SGD)|Mini-Batch Gradient Descent|Momentum|Adagrad|RMSprop|Adam|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|**Gradient Computation**|Full dataset|One data point per update|Small subset (mini-batch) of the dataset|Uses gradient of mini-batch with momentum factor|Sum of squared past gradients|Moving average of squared gradients|Momentum + RMSprop (first & second moment estimates)|\n",
    "|**Memory Requirements**|High (must load entire dataset)|Low|Medium|Low (only needs momentum term)|High (stores past squared gradients)|Medium (maintains moving average)|Medium (momentum + moving average)|\n",
    "|**Convergence Speed**|Slow, but stable|Faster (noisy)|Faster than Batch, less noisy than SGD|Faster than regular GD|Converges quickly at first, then slows|Fast convergence, smooth|Fast convergence, adaptive|\n",
    "|**Advantages**|Stable updates, globally optimal step|Fast updates, good for online learning|Balances between stability and speed|Reduces oscillations, speeds up training|Adaptive learning rate|Adaptive learning rate, resolves Adagrad issues|Combines benefits of Momentum and RMSprop|\n",
    "|**Disadvantages**|Computationally expensive, slow for large datasets|Highly noisy, can overshoot minima|Requires tuning of mini-batch size|Requires momentum hyperparameter|Learning rate decays too quickly|Requires tuning of decay rate|May not always generalize well, sensitive to hyperparameters|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7d63b",
   "metadata": {
    "id": "f7c7d63b"
   },
   "source": [
    "### 3. Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow convergence, local minima). How do modern optimizers address these challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85eecc0",
   "metadata": {
    "id": "d85eecc0"
   },
   "source": [
    "Traditional gradient descent optimization methods, such as Batch Gradient Descent (GD) and Stochastic Gradient Descent (SGD), are foundational techniques in training neural networks. However, they come with several challenges that can hinder performance and training efficiency. Modern optimizers, such as Momentum, RMSprop, and Adam, have been developed to address these challenges and improve the convergence of neural networks.\n",
    "\n",
    "##### Challenges of Traditional Gradient Descent Optimization:\n",
    "\n",
    "|Problem|Problem Description|Cause|\n",
    "|---|---|---|\n",
    "|**Slow Convergence**|In traditional gradient descent, convergence can be slow, especially when the loss function has flat regions, small gradients, or ill-conditioned curvature. It can take many iterations for the algorithm to reach the optimal minimum, especially if the learning rate is too low.|Gradients in regions of the loss surface may be small, leading to very small updates to the model parameters. The network struggles to make significant progress towards the global minimum.|\n",
    "|**Oscillations Near Minima**|In traditional SGD, the updates can oscillate around the minima, especially in cases where the loss function has steep sides and flat valleys (common in saddle points or ravines).|Using a fixed learning rate means that even when the algorithm gets close to a minimum, the gradient steps can be too large, causing overshooting and slow final convergence.|\n",
    "|**Sensitivity to Learning Rate**|Selecting an appropriate learning rate is crucial in traditional gradient descent. If the learning rate is too large, the model may fail to converge or diverge. If it’s too small, the model may take too long to converge or get stuck in local minima.|A fixed learning rate doesn’t adapt to the dynamics of the loss function’s landscape, causing inefficient or unstable learning.|\n",
    "|**Local Minima and Saddle Points**|Traditional gradient descent is prone to getting stuck in local minima or saddle points (flat regions where the gradient is zero in many directions but not necessarily a minimum).|The non-convex nature of neural network loss surfaces means there are many suboptimal solutions that can trap the optimization process. In traditional methods, small gradients in these regions can prevent further progress.|\n",
    "|**Vanishing/Exploding Gradients**|In deep neural networks, gradients can become extremely small (vanishing gradients) or excessively large (exploding gradients) as they propagate back through the layers, causing inefficient learning or instability.|This problem is particularly common with certain activation functions (e.g., sigmoid and tanh) and weight initialization methods. Traditional SGD struggles to cope with these phenomena.|\n",
    "|**Noisy Updates in SGD**|In stochastic gradient descent, the updates to parameters can be noisy because they are based on a single training example (or mini-batch), which might not represent the overall trend in the dataset. This can lead to erratic updates and slow convergence.|The randomness in selecting individual examples for gradient updates introduces variance in the gradient estimates.|\n",
    "\n",
    "\n",
    "##### How Modern Optimizers Address These Challenges:\n",
    "\n",
    "1. **Momentum:**\n",
    "    \n",
    "    - **Addresses Slow Convergence and Oscillations:**\n",
    "        \n",
    "        - Momentum accelerates gradient descent by adding a fraction of the previous update to the current one, effectively smoothing out oscillations and speeding up convergence, especially in steep regions or ravines.\n",
    "        - By considering past gradients, momentum helps the optimization algorithm continue moving in directions that have consistently reduced the loss.\n",
    "        \n",
    "        \n",
    "2. **Nesterov Accelerated Gradient (NAG):**\n",
    "    \n",
    "    - **Improves Momentum:**\n",
    "        \n",
    "        - NAG takes the momentum concept further by calculating the gradient at a future position (an anticipated update), which leads to better convergence and more precise steps.\n",
    "        - By \"looking ahead,\" NAG prevents overshooting and improves convergence speed near minima.\n",
    "        \n",
    "        \n",
    "3. **Adaptive Learning Rate Methods (Adagrad, RMSprop, Adam):**\n",
    "    \n",
    "    - **Address Sensitivity to Learning Rate:**\n",
    "        \n",
    "        - These methods adapt the learning rate for each parameter individually, based on how frequently the parameter is updated. This solves the issue of having to manually select a fixed learning rate.\n",
    "        - Parameters that receive larger updates over time have their learning rates reduced, while those with smaller updates have their learning rates increased.\n",
    "        \n",
    "    - **Adagrad** adjusts the learning rate by dividing it by the sum of past squared gradients, making it suitable for sparse data.\n",
    "    - **RMSprop** resolves the problem of Adagrad’s decaying learning rate by using a moving average of squared gradients.\n",
    "    - **Adam** combines the advantages of both momentum and RMSprop by maintaining a moving average of both the first (mean) and second moments (variance) of the gradients, ensuring smooth and efficient learning across diverse tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55d80b",
   "metadata": {
    "id": "1f55d80b"
   },
   "source": [
    "## Part 2: Optimizer Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb7fd8",
   "metadata": {
    "id": "dbeb7fd8"
   },
   "source": [
    "### 5. Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional gradient descent. Discuss its limitations and scenarios where it is most suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab841b4f",
   "metadata": {
    "id": "ab841b4f"
   },
   "source": [
    "##### Concept of Stochastic Gradient Descent (SGD):\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is an optimization algorithm used to minimize the loss function of a machine learning model by iteratively updating the model's parameters (weights and biases). The key difference between SGD and traditional Batch Gradient Descent (GD) lies in how the gradients are calculated.\n",
    "\n",
    "- **Traditional Batch Gradient Descent** computes the gradient of the loss function using the entire training dataset, making parameter updates only after processing the full dataset in each iteration.\n",
    "\n",
    "- **SGD**, on the other hand, computes the gradient using a single training example (or a mini-batch in some cases) and updates the parameters after each example. This means SGD performs many more updates in each epoch compared to batch GD, leading to faster updates but with more variance.\n",
    "\n",
    "Mathematically, the parameter update rule for SGD can be written as:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\eta \\nabla_{\\theta}J(\\theta; x^{(i)}, y^{(i)})  $$\n",
    "\n",
    "- $\\theta$ are the model parameters at time $t$\n",
    "- $\\eta$ is the learning rate\n",
    "- $\\nabla_{\\theta}J(\\theta; x^{(i)}, y^{(i)})$ is the gradient of the loss function $J$ with respect to the parameters $\\theta$ computed for a single training example $(x^{(i)}, y^{(i)})$.\n",
    "\n",
    "\n",
    "##### Advantages of Stochastic Gradient Descent (SGD) Compared to Traditional Gradient Descent:\n",
    "\n",
    "- SGD requires significantly less memory because it processes only one (or a small mini-batch of) training examples at a time, making it suitable for training on large datasets where loading the entire dataset into memory at once is impractical. In contrast, batch GD requires loading the entire dataset to compute the gradient, which can be prohibitive for very large datasets.\n",
    "\n",
    "- SGD is well-suited for online learning, where the model learns continuously from a stream of data. It can update the model parameters immediately after each data point is observed, making it ideal for applications where data arrives in real-time (e.g., recommendation systems, stock market prediction).\n",
    "\n",
    "- The noisy updates of SGD (caused by computing gradients using individual samples) can sometimes be an advantage. This noise allows the algorithm to escape from shallow local minima or saddle points, potentially leading to better global optima compared to traditional GD, which may get stuck in such points due to its deterministic nature.\n",
    "\n",
    "- SGD scales well to very large datasets, as it doesn't need to process the full dataset for each update. This makes it more efficient and faster to implement in distributed systems or when dealing with big data.\n",
    "\n",
    "\n",
    "##### Limitations of Stochastic Gradient Descent (SGD):\n",
    "\n",
    "1. **Noisy Updates and Convergence:** SGD is highly sensitive to the choice of learning rate. If the learning rate is too large, the model may overshoot the optimal solution and fail to converge. If the learning rate is too small, the model may take a long time to converge or get stuck in suboptimal solutions. Finding the right balance in the learning rate can be challenging and often requires experimenting with learning rate schedules or decaying learning rates over time.\n",
    "\n",
    "2. **Poor Handling of Complex Loss Landscapes:** While SGD can escape shallow local minima, it may still struggle with complex loss landscapes that have many local minima or saddle points. In such cases, SGD may not find the best solution, especially without additional techniques like momentum or adaptive learning rates.\n",
    "\n",
    "3. **Frequent Updates Increase Computational Cost:** Although SGD requires fewer computations per update, it performs many more updates than batch GD in each epoch, potentially leading to more computational overhead overall. This is particularly true when using small learning rates or training over many epochs.\n",
    "\n",
    "\n",
    "##### Scenarios Where SGD is Most Suitable:\n",
    "\n",
    "1. **Large Datasets:** SGD is highly effective for large-scale machine learning problems, such as training neural networks on massive datasets. Its ability to make updates using a single data point allows it to scale efficiently, avoiding the memory and computational bottlenecks associated with batch GD.\n",
    "\n",
    "2. **Online Learning:** In situations where data is arriving in a stream or where the model needs to continuously learn from incoming data (e.g., real-time recommendation systems or financial data), SGD is the best choice because it can make updates immediately as each new data point is observed.\n",
    "\n",
    "3. **Early Convergence:** In cases where fast, approximate solutions are preferred over precise convergence (e.g., in deep learning tasks where the goal is to minimize validation error, not necessarily reach the global minimum), SGD can be effective due to its rapid, frequent updates.\n",
    "\n",
    "4. **Non-Stationary Environments:** SGD is beneficial in environments where the data distribution is changing over time, such as in reinforcement learning or online learning scenarios. It can quickly adapt to new data patterns since it updates parameters frequently.\n",
    "\n",
    "5. **Exploratory Learning:** When training complex models like deep neural networks, SGD is useful because its noisy updates encourage exploration of the loss landscape, potentially avoiding poor local minima and leading to better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174c9e8",
   "metadata": {
    "id": "8174c9e8"
   },
   "source": [
    "### 6. Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates. Discuss its benefits and potential drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba09eacb",
   "metadata": {
    "id": "ba09eacb"
   },
   "source": [
    "##### Concept of the Adam Optimizer\n",
    "\n",
    "The **Adam (Adaptive Moment Estimation)** optimizer is one of the most widely used optimization algorithms in training deep learning models. It combines the benefits of momentum and adaptive learning rates to achieve faster convergence and more efficient updates. Adam is particularly effective in dealing with noisy gradients, sparse data, and non-stationary environments.\n",
    "\n",
    "Adam works by maintaining separate moving averages for the first moment (mean) and second moment (variance) of the gradients, which it uses to adaptively update the learning rate for each parameter.\n",
    "\n",
    "##### How Adam Combines Momentum and Adaptive Learning Rates\n",
    "\n",
    "1. **Momentum (First Moment Estimation)**\n",
    "\n",
    "    - Momentum helps accelerate SGD by adding a fraction of the previous gradient to the current one, allowing the optimizer to build up speed in directions that consistently reduce the loss.\n",
    "    \n",
    "    - In Adam, momentum is incorporated by calculating the exponentially weighted moving average of the gradient $m_{t}$ (first moment), which estimates the \"mean\" of the gradients. This allows Adam to smooth out noisy updates and accelerate the optimization in the right direction.\n",
    "    \n",
    "The update for the first moment is:\n",
    "\n",
    "$$ m_{t} = \\beta_{1} m_{t-1} + (1 - \\beta_{1})g_{t} $$\n",
    "\n",
    "where,\n",
    "- $m_{t}$ is the exponentially decayed moving average of past gradients.\n",
    "- $g_{t}$ is the current gradient at time step $t$.\n",
    "- $\\beta_{1}$ is the decay rate for the moving average (typically set to 0.9).\n",
    "\n",
    "2. **Adaptive Learning Rates (Second Moment Estimation)**\n",
    "\n",
    "    - Adaptive learning rates ensure that each parameter has its own learning rate, adjusting the step size based on the magnitude of the gradients. This helps reduce the learning rate for parameters with large gradients and increase it for those with small gradients.\n",
    "    \n",
    "    - Adam achieves this by computing the exponentially weighted moving average of the squared gradients $v_{t}$ (second moment), which estimates the \"variance\" of the gradients. The adaptive learning rate is then calculated using this variance.\n",
    "    \n",
    "The update for the second moment is:\n",
    "\n",
    "$$ v_{t} = \\beta_{2} v_{t-1} + (1 - \\beta_{2})g_{t}^2 $$\n",
    "\n",
    "where,\n",
    "- $v_{t}$ is the exponentially decayed moving average of the squared gradients.\n",
    "- $g_{t}^2$ is the element-wise square of the gradient.\n",
    "- $\\beta_{2}$ is the decay rate for the second moment (typically set to 0.999).\n",
    "\n",
    "\n",
    "3. **Bias-Correction**\n",
    "\n",
    "    - To counteract the fact that the moving averages $m_{t}$ and $v_{t}$ are initialized to zero (which would bias the estimates during the initial steps of training), Adam includes bias-correction terms:\n",
    "    \n",
    "    $$\\hat{m_{t}} = \\frac{m_{t}}{1 - \\beta_{1}^t}$$\n",
    "    \n",
    "    $$\\hat{v_{t}} = \\frac{v_{t}}{1 - \\beta_{2}^t}$$\n",
    "    \n",
    "    These bias-corrected estimates ensure that the early steps do not suffer from the initial zero bias.\n",
    "    \n",
    "    \n",
    "4. **Parameter Update**\n",
    "\n",
    "    - Finally, the parameters are updated using the combination of the first and second moments:\n",
    "    \n",
    "    $$ \\theta_{t + 1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v_{t}}} + \\epsilon} \\hat{m_{t}} $$\n",
    "    \n",
    "   where,\n",
    "   \n",
    "   - $\\theta_{t}$ is the parameter vector at time step $t$.\n",
    "   - $\\eta$ is the learning rate.\n",
    "   - $\\epsilon$ is a small constant (typically $10^{-8}$)  added for numerical stability to prevent division by zero.\n",
    "   \n",
    "   \n",
    "##### Benefits of Adam Optimizer\n",
    "\n",
    "1. **Faster Convergence:** Adam’s combination of momentum and adaptive learning rates enables it to converge more quickly than traditional gradient descent or stochastic gradient descent (SGD). Momentum smooths the gradient updates, while adaptive learning rates ensure that parameters receive appropriately scaled updates.\n",
    "\n",
    "2. **Adaptive Learning Rates:** Adam automatically adjusts the learning rate for each parameter, reducing the need for extensive tuning of the learning rate hyperparameter. This makes Adam more robust to different problems and easier to apply in practice, especially when the gradient magnitudes vary significantly across parameters.\n",
    "\n",
    "3. **Handles Noisy Gradients:** Adam is highly effective when training on noisy datasets or in non-stationary environments (e.g., reinforcement learning), where the loss function may fluctuate. The momentum term helps smooth out the noise, while the adaptive learning rates ensure efficient updates even with sparse or irregular gradients.\n",
    "\n",
    "4. **Bias Correction:** The bias correction terms ensure that Adam behaves well even in the early stages of training when gradients are small or unstable. This helps stabilize learning in the initial iterations.\n",
    "\n",
    "5. **Memory Efficiency:** Adam uses only first-order gradients (no need for second-order derivatives) and maintains two moving averages (first and second moments), making it memory-efficient compared to methods that require storing more information, such as the Hessian matrix in second-order methods.\n",
    "\n",
    "\n",
    "##### Potential Drawbacks of Adam Optimizer\n",
    "\n",
    "1. **Learning Rate Sensitivity:** Although Adam uses adaptive learning rates, its convergence can still be sensitive to the choice of the global learning rate $\\eta$. While it often requires less fine-tuning compared to other optimizers, improper selection of $\\eta$ can still lead to suboptimal convergence or even divergence.\n",
    "\n",
    "2. **Tendency to Overfit:** Adam’s rapid convergence can sometimes lead to overfitting, especially on small datasets. By aggressively minimizing the training loss, Adam may not generalize well to unseen data. To mitigate this, it may be necessary to use regularization techniques like dropout or early stopping.\n",
    "\n",
    "3. **Poor Performance in Some Scenarios:** Although Adam generally works well, some research (e.g., in reinforcement learning) has shown that SGD with momentum can outperform Adam in certain cases, particularly in tasks where generalization is crucial. This is because Adam’s fast convergence may lead to solutions that overfit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15bc8c",
   "metadata": {
    "id": "ad15bc8c"
   },
   "source": [
    "### 7. Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning rates. Compare it with Adam and discuss their relative strengths and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd613ee",
   "metadata": {
    "id": "7dd613ee"
   },
   "source": [
    "##### Concept of RMSprop Optimizer\n",
    "\n",
    "**RMSprop (Root Mean Square Propagation)** is an **adaptive learning rate optimization algorithm** designed to address the limitations of traditional Stochastic Gradient Descent (SGD), particularly in dealing with noisy and non-stationary objectives. The primary idea behind RMSprop is to maintain a moving average of the squared gradients to adjust the learning rate for each parameter. This allows the optimizer to scale the updates according to the magnitude of the gradients, preventing large, oscillating updates and promoting smoother convergence.\n",
    "\n",
    "##### How RMSprop Works:\n",
    "\n",
    "1. **Exponential Moving Average of Squared Gradients:**\n",
    "\n",
    "    - RMSprop keeps track of an exponentially decaying average of the squared gradients for each parameter. This average is then used to scale the learning rate, ensuring that parameters with large gradients receive smaller updates, while those with small gradients get larger updates.\n",
    "    \n",
    "    The squared gradient accumulation is updated as:\n",
    "    \n",
    "    $$ E[g^2]_{t} = \\beta E[g^2]_{t - 1} + (1 - \\beta) g_{t}^2 $$\n",
    "    \n",
    "    Where,\n",
    "    \n",
    "    - $E[g^2]_{t}$ is the exponentially weighted moving average of the squared gradient at time step $t$.\n",
    "    - $\\beta$ is the decay rate (typically set to 0.9).\n",
    "    - $g_{t}$ is the gradient at time step $t$.\n",
    "    \n",
    "    \n",
    "2. **Parameter Update Rule:**\n",
    "\n",
    "    - The learning rate for each parameter is adjusted by dividing the global learning rate $\\eta$ by the square root of the accumulated squared gradient (plus a small constant $\\epsilon$ for numerical stability):\n",
    "    \n",
    "    $$ \\theta_{t + 1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{E[g^2]_{t} + \\epsilon}} g_{t} $$\n",
    "    \n",
    "    where,\n",
    "    - $\\eta$ is the global learning parameter\n",
    "    - $\\epsilon$ is a small constant (typically $10^{-8}$)  added for numerical stability to prevent division by zero.\n",
    "    - $g_{t}$ is the gradient at time step $t$\n",
    "    - $\\theta_{t}$ represents the parameter at time $t$.\n",
    "    \n",
    "    \n",
    "##### Addressing the Challenge of Adaptive Learning Rates:\n",
    "\n",
    "- **RMSprop** addresses the issue of varying gradient magnitudes by adjusting the learning rate for each parameter individually based on the running average of squared gradients. Parameters with large gradients receive smaller updates, preventing unstable oscillations. Conversely, small gradients receive larger updates, ensuring that learning proceeds efficiently even in flat regions of the loss landscape.\n",
    "\n",
    "- This helps stabilize training in deep learning models, where gradients can vary dramatically across layers and parameters.\n",
    "\n",
    "\n",
    "##### Comparison Between RMSprop and Adam\n",
    "\n",
    "|Aspect|RMSprop|Adam|\n",
    "|---|---|---|\n",
    "|**Gradient Tracking**|RMSprop tracks the exponentially weighted moving average of the squared gradients.|Adam tracks both the first moment (mean) and second moment (variance) of the gradients (i.e., it combines RMSprop with momentum).|\n",
    "|**Bias Correction**|RMSprop does not include bias correction.|Adam includes bias correction terms for both first and second moments to prevent initialization bias in early stages of training.|\n",
    "|**Parameter Updates**|Parameters are updated using the adjusted learning rate based on the square root of the accumulated squared gradients.|Parameters are updated using a combination of the first moment (mean of gradients) and second moment (variance of gradients) with bias correction.|\n",
    "|**Learning Rate Adaptation**|RMSprop adapts the learning rate based on the magnitude of recent squared gradients, leading to smoother updates.|Adam also adapts the learning rate, but uses both mean and variance, leading to even more adaptive updates.|\n",
    "|**Momentum**|RMSprop does not inherently use momentum, though it can be manually combined with momentum in certain implementations.|Adam uses momentum automatically by tracking the exponentially decayed moving average of gradients (first moment), helping smooth updates and accelerate learning.|\n",
    "|**Convergence Speed**|RMSprop typically converges faster than traditional SGD and works well for deep networks and recurrent neural networks.|Adam often converges faster than RMSprop due to its combination of momentum and adaptive learning rates, particularly in early stages of training.|\n",
    "|**Memory Requirements**|RMSprop maintains only one moving average per parameter (for squared gradients), making it more memory-efficient than Adam.|Adam requires more memory since it tracks two moving averages per parameter (first and second moments).|\n",
    "|**Stability and Generalization**|RMSprop can be more stable in certain situations due to its simplicity, but may not generalize as well as Adam in complex models.|Adam tends to generalize better in many deep learning tasks, though it can sometimes overfit due to its aggressive minimization in early stages.|\n",
    "|**Best Use Cases**|Effective for training recurrent neural networks (RNNs), deep networks, and models with highly varying gradients across parameters.|Works well across a wide range of tasks, especially deep learning models, convolutional neural networks (CNNs), and reinforcement learning tasks.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3c95b",
   "metadata": {
    "id": "d9f3c95b"
   },
   "source": [
    "## Part 3: Applying Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67157eee",
   "metadata": {
    "id": "67157eee"
   },
   "source": [
    "### 8. Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a framework of your choice. Train the model on a suitable dataset and compare their impact on model convergence and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044395d8",
   "metadata": {
    "id": "044395d8"
   },
   "source": [
    "To compare the impact of SGD, Adam, and RMSprop on a deep learning model, we will implement a model using the PyTorch framework. We'll train the model on the MNIST dataset (handwritten digit classification), which is a widely-used benchmark dataset for deep learning tasks. We will define a simple Convolutional Neural Network (CNN) and train it using each optimizer, then compare the results in terms of convergence speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4a8fe0",
   "metadata": {
    "id": "9e4a8fe0"
   },
   "outputs": [],
   "source": [
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed4c2d5",
   "metadata": {
    "id": "3ed4c2d5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xglhU90-u3g3",
   "metadata": {
    "id": "xglhU90-u3g3"
   },
   "source": [
    "##### Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "oAtYA667vAG-",
   "metadata": {
    "id": "oAtYA667vAG-"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleCNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1,32, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    self.fc1 = nn.Linear(64*7*7, 128)\n",
    "    self.fc2 = nn.Linear(128, 10)\n",
    "    self.relu = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(self.relu(self.conv1(x)))\n",
    "    x = self.pool(self.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 64*7*7)\n",
    "    x = self.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obnnJFrsGhvP",
   "metadata": {
    "id": "obnnJFrsGhvP"
   },
   "source": [
    "##### Declaring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "MhypAIQYGgrY",
   "metadata": {
    "id": "MhypAIQYGgrY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ME-DvDoQ9a29",
   "metadata": {
    "id": "ME-DvDoQ9a29"
   },
   "source": [
    "##### Loading the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "GBsjcrbm9sFW",
   "metadata": {
    "id": "GBsjcrbm9sFW"
   },
   "outputs": [],
   "source": [
    "tranformers = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=tranformers, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=tranformers, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skHLG6-IEhXS",
   "metadata": {
    "id": "skHLG6-IEhXS"
   },
   "source": [
    "##### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "L4lSoLB_HFuL",
   "metadata": {
    "id": "L4lSoLB_HFuL"
   },
   "outputs": [],
   "source": [
    "def train_model(optimizer_name, model, train_loader, criterion, num_epochs=10):\n",
    "  optimizer = None\n",
    "  if optimizer_name == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "  elif optimizer_name == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  elif optimizer_name == 'RMSprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid optimizer name: {optimizer_name}\")\n",
    "\n",
    "  train_loss = []\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "      images = images.to(device)\n",
    "      labels = torch.device(device)\n",
    "      output = model(images)\n",
    "      loss = criterion(output, labels)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      if (i + 1) % 2 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_loss.append(avg_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "  print(f\"Training is finished!\")\n",
    "  return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P5xeaUzTTGYp",
   "metadata": {
    "id": "P5xeaUzTTGYp"
   },
   "source": [
    "##### Let's print the shape of the train loader images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "KQMbB7eFIyfu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQMbB7eFIyfu",
    "outputId": "1644ca61-3785-416e-acce-a2aede774af3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image is: torch.Size([64, 1, 28, 28])\n",
      "Sample labels is: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "example = iter(train_loader)\n",
    "samples, labels = next(example)\n",
    "print(f'Sample image is: {samples.shape}')\n",
    "print(f'Sample labels is: {samples.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X4LFHcUITX4-",
   "metadata": {
    "id": "X4LFHcUITX4-"
   },
   "source": [
    "##### Let's plot a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "SGiyehSxTaSd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "SGiyehSxTaSd",
    "outputId": "8e50dea0-c901-4680-ca1a-b1d8fa0c123c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(samples[i][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V9_AlOgVTgEF",
   "metadata": {
    "id": "V9_AlOgVTgEF"
   },
   "source": [
    "##### Evaluation Function for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "q0wEG-R0T2bn",
   "metadata": {
    "id": "q0wEG-R0T2bn"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XwlkQKh5UGN5",
   "metadata": {
    "id": "XwlkQKh5UGN5"
   },
   "source": [
    "#####  Train the Model with Different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0nP6__xiUOBR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nP6__xiUOBR",
    "outputId": "a21c64d3-e1c3-4848-f072-d90bf7aa41e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with SGD\n",
      "Epoch [1/10], Step [2/938], Loss: 2.3197\n",
      "Epoch [1/10], Step [4/938], Loss: 2.3075\n",
      "Epoch [1/10], Step [6/938], Loss: 2.3077\n",
      "Epoch [1/10], Step [8/938], Loss: 2.2934\n",
      "Epoch [1/10], Step [10/938], Loss: 2.3016\n",
      "Epoch [1/10], Step [12/938], Loss: 2.3072\n",
      "Epoch [1/10], Step [14/938], Loss: 2.3051\n",
      "Epoch [1/10], Step [16/938], Loss: 2.3005\n",
      "Epoch [1/10], Step [18/938], Loss: 2.2955\n",
      "Epoch [1/10], Step [20/938], Loss: 2.3064\n",
      "Epoch [1/10], Step [22/938], Loss: 2.2802\n",
      "Epoch [1/10], Step [24/938], Loss: 2.2882\n",
      "Epoch [1/10], Step [26/938], Loss: 2.3088\n",
      "Epoch [1/10], Step [28/938], Loss: 2.2999\n",
      "Epoch [1/10], Step [30/938], Loss: 2.3050\n",
      "Epoch [1/10], Step [32/938], Loss: 2.2941\n",
      "Epoch [1/10], Step [34/938], Loss: 2.3080\n",
      "Epoch [1/10], Step [36/938], Loss: 2.2919\n",
      "Epoch [1/10], Step [38/938], Loss: 2.2857\n",
      "Epoch [1/10], Step [40/938], Loss: 2.3066\n",
      "Epoch [1/10], Step [42/938], Loss: 2.2933\n",
      "Epoch [1/10], Step [44/938], Loss: 2.2968\n",
      "Epoch [1/10], Step [46/938], Loss: 2.2912\n",
      "Epoch [1/10], Step [48/938], Loss: 2.3155\n",
      "Epoch [1/10], Step [50/938], Loss: 2.3003\n",
      "Epoch [1/10], Step [52/938], Loss: 2.3046\n",
      "Epoch [1/10], Step [54/938], Loss: 2.2977\n",
      "Epoch [1/10], Step [56/938], Loss: 2.2958\n",
      "Epoch [1/10], Step [58/938], Loss: 2.3006\n",
      "Epoch [1/10], Step [60/938], Loss: 2.3100\n",
      "Epoch [1/10], Step [62/938], Loss: 2.2994\n",
      "Epoch [1/10], Step [64/938], Loss: 2.3098\n",
      "Epoch [1/10], Step [66/938], Loss: 2.3009\n",
      "Epoch [1/10], Step [68/938], Loss: 2.3022\n",
      "Epoch [1/10], Step [70/938], Loss: 2.2974\n",
      "Epoch [1/10], Step [72/938], Loss: 2.2922\n",
      "Epoch [1/10], Step [74/938], Loss: 2.2987\n",
      "Epoch [1/10], Step [76/938], Loss: 2.2917\n",
      "Epoch [1/10], Step [78/938], Loss: 2.2922\n",
      "Epoch [1/10], Step [80/938], Loss: 2.2914\n",
      "Epoch [1/10], Step [82/938], Loss: 2.2876\n",
      "Epoch [1/10], Step [84/938], Loss: 2.3027\n",
      "Epoch [1/10], Step [86/938], Loss: 2.3032\n",
      "Epoch [1/10], Step [88/938], Loss: 2.3000\n",
      "Epoch [1/10], Step [90/938], Loss: 2.2959\n",
      "Epoch [1/10], Step [92/938], Loss: 2.2906\n",
      "Epoch [1/10], Step [94/938], Loss: 2.2931\n",
      "Epoch [1/10], Step [96/938], Loss: 2.2837\n",
      "Epoch [1/10], Step [98/938], Loss: 2.3019\n",
      "Epoch [1/10], Step [100/938], Loss: 2.2877\n",
      "Epoch [1/10], Step [102/938], Loss: 2.2939\n",
      "Epoch [1/10], Step [104/938], Loss: 2.2938\n",
      "Epoch [1/10], Step [106/938], Loss: 2.2891\n",
      "Epoch [1/10], Step [108/938], Loss: 2.2926\n",
      "Epoch [1/10], Step [110/938], Loss: 2.2886\n",
      "Epoch [1/10], Step [112/938], Loss: 2.2765\n",
      "Epoch [1/10], Step [114/938], Loss: 2.2850\n",
      "Epoch [1/10], Step [116/938], Loss: 2.2970\n",
      "Epoch [1/10], Step [118/938], Loss: 2.2911\n",
      "Epoch [1/10], Step [120/938], Loss: 2.2952\n",
      "Epoch [1/10], Step [122/938], Loss: 2.2935\n",
      "Epoch [1/10], Step [124/938], Loss: 2.2856\n",
      "Epoch [1/10], Step [126/938], Loss: 2.2924\n",
      "Epoch [1/10], Step [128/938], Loss: 2.2936\n",
      "Epoch [1/10], Step [130/938], Loss: 2.2891\n",
      "Epoch [1/10], Step [132/938], Loss: 2.2879\n",
      "Epoch [1/10], Step [134/938], Loss: 2.2859\n",
      "Epoch [1/10], Step [136/938], Loss: 2.2905\n",
      "Epoch [1/10], Step [138/938], Loss: 2.2902\n",
      "Epoch [1/10], Step [140/938], Loss: 2.2939\n",
      "Epoch [1/10], Step [142/938], Loss: 2.2861\n",
      "Epoch [1/10], Step [144/938], Loss: 2.2944\n",
      "Epoch [1/10], Step [146/938], Loss: 2.2918\n",
      "Epoch [1/10], Step [148/938], Loss: 2.2897\n",
      "Epoch [1/10], Step [150/938], Loss: 2.2788\n",
      "Epoch [1/10], Step [152/938], Loss: 2.2908\n",
      "Epoch [1/10], Step [154/938], Loss: 2.2869\n",
      "Epoch [1/10], Step [156/938], Loss: 2.2908\n",
      "Epoch [1/10], Step [158/938], Loss: 2.2955\n",
      "Epoch [1/10], Step [160/938], Loss: 2.2946\n",
      "Epoch [1/10], Step [162/938], Loss: 2.2901\n",
      "Epoch [1/10], Step [164/938], Loss: 2.2923\n",
      "Epoch [1/10], Step [166/938], Loss: 2.2827\n",
      "Epoch [1/10], Step [168/938], Loss: 2.2892\n",
      "Epoch [1/10], Step [170/938], Loss: 2.2851\n",
      "Epoch [1/10], Step [172/938], Loss: 2.2879\n",
      "Epoch [1/10], Step [174/938], Loss: 2.2842\n",
      "Epoch [1/10], Step [176/938], Loss: 2.2861\n",
      "Epoch [1/10], Step [178/938], Loss: 2.2902\n",
      "Epoch [1/10], Step [180/938], Loss: 2.2878\n",
      "Epoch [1/10], Step [182/938], Loss: 2.2865\n",
      "Epoch [1/10], Step [184/938], Loss: 2.2835\n",
      "Epoch [1/10], Step [186/938], Loss: 2.2821\n",
      "Epoch [1/10], Step [188/938], Loss: 2.2901\n",
      "Epoch [1/10], Step [190/938], Loss: 2.2815\n",
      "Epoch [1/10], Step [192/938], Loss: 2.2859\n",
      "Epoch [1/10], Step [194/938], Loss: 2.2824\n",
      "Epoch [1/10], Step [196/938], Loss: 2.2832\n",
      "Epoch [1/10], Step [198/938], Loss: 2.2905\n",
      "Epoch [1/10], Step [200/938], Loss: 2.2827\n",
      "Epoch [1/10], Step [202/938], Loss: 2.2820\n",
      "Epoch [1/10], Step [204/938], Loss: 2.2847\n",
      "Epoch [1/10], Step [206/938], Loss: 2.2850\n",
      "Epoch [1/10], Step [208/938], Loss: 2.2795\n",
      "Epoch [1/10], Step [210/938], Loss: 2.2846\n",
      "Epoch [1/10], Step [212/938], Loss: 2.2814\n",
      "Epoch [1/10], Step [214/938], Loss: 2.2803\n",
      "Epoch [1/10], Step [216/938], Loss: 2.2885\n",
      "Epoch [1/10], Step [218/938], Loss: 2.2821\n",
      "Epoch [1/10], Step [220/938], Loss: 2.2793\n",
      "Epoch [1/10], Step [222/938], Loss: 2.2939\n",
      "Epoch [1/10], Step [224/938], Loss: 2.2837\n",
      "Epoch [1/10], Step [226/938], Loss: 2.2799\n",
      "Epoch [1/10], Step [228/938], Loss: 2.2826\n",
      "Epoch [1/10], Step [230/938], Loss: 2.2778\n",
      "Epoch [1/10], Step [232/938], Loss: 2.2891\n",
      "Epoch [1/10], Step [234/938], Loss: 2.2784\n",
      "Epoch [1/10], Step [236/938], Loss: 2.2864\n",
      "Epoch [1/10], Step [238/938], Loss: 2.2796\n",
      "Epoch [1/10], Step [240/938], Loss: 2.2809\n",
      "Epoch [1/10], Step [242/938], Loss: 2.2784\n",
      "Epoch [1/10], Step [244/938], Loss: 2.2821\n",
      "Epoch [1/10], Step [246/938], Loss: 2.2782\n",
      "Epoch [1/10], Step [248/938], Loss: 2.2729\n",
      "Epoch [1/10], Step [250/938], Loss: 2.2771\n",
      "Epoch [1/10], Step [252/938], Loss: 2.2773\n",
      "Epoch [1/10], Step [254/938], Loss: 2.2903\n",
      "Epoch [1/10], Step [256/938], Loss: 2.2810\n",
      "Epoch [1/10], Step [258/938], Loss: 2.2799\n",
      "Epoch [1/10], Step [260/938], Loss: 2.2779\n",
      "Epoch [1/10], Step [262/938], Loss: 2.2772\n",
      "Epoch [1/10], Step [264/938], Loss: 2.2808\n",
      "Epoch [1/10], Step [266/938], Loss: 2.2828\n",
      "Epoch [1/10], Step [268/938], Loss: 2.2791\n",
      "Epoch [1/10], Step [270/938], Loss: 2.2748\n",
      "Epoch [1/10], Step [272/938], Loss: 2.2786\n",
      "Epoch [1/10], Step [274/938], Loss: 2.2856\n",
      "Epoch [1/10], Step [276/938], Loss: 2.2810\n",
      "Epoch [1/10], Step [278/938], Loss: 2.2771\n",
      "Epoch [1/10], Step [280/938], Loss: 2.2810\n",
      "Epoch [1/10], Step [282/938], Loss: 2.2775\n",
      "Epoch [1/10], Step [284/938], Loss: 2.2752\n",
      "Epoch [1/10], Step [286/938], Loss: 2.2807\n",
      "Epoch [1/10], Step [288/938], Loss: 2.2824\n",
      "Epoch [1/10], Step [290/938], Loss: 2.2740\n",
      "Epoch [1/10], Step [292/938], Loss: 2.2739\n",
      "Epoch [1/10], Step [294/938], Loss: 2.2715\n",
      "Epoch [1/10], Step [296/938], Loss: 2.2845\n",
      "Epoch [1/10], Step [298/938], Loss: 2.2759\n",
      "Epoch [1/10], Step [300/938], Loss: 2.2783\n",
      "Epoch [1/10], Step [302/938], Loss: 2.2664\n",
      "Epoch [1/10], Step [304/938], Loss: 2.2794\n",
      "Epoch [1/10], Step [306/938], Loss: 2.2730\n",
      "Epoch [1/10], Step [308/938], Loss: 2.2768\n",
      "Epoch [1/10], Step [310/938], Loss: 2.2762\n",
      "Epoch [1/10], Step [312/938], Loss: 2.2712\n",
      "Epoch [1/10], Step [314/938], Loss: 2.2741\n",
      "Epoch [1/10], Step [316/938], Loss: 2.2718\n",
      "Epoch [1/10], Step [318/938], Loss: 2.2772\n",
      "Epoch [1/10], Step [320/938], Loss: 2.2681\n",
      "Epoch [1/10], Step [322/938], Loss: 2.2773\n",
      "Epoch [1/10], Step [324/938], Loss: 2.2723\n",
      "Epoch [1/10], Step [326/938], Loss: 2.2741\n",
      "Epoch [1/10], Step [328/938], Loss: 2.2771\n",
      "Epoch [1/10], Step [330/938], Loss: 2.2743\n",
      "Epoch [1/10], Step [332/938], Loss: 2.2707\n",
      "Epoch [1/10], Step [334/938], Loss: 2.2720\n",
      "Epoch [1/10], Step [336/938], Loss: 2.2781\n",
      "Epoch [1/10], Step [338/938], Loss: 2.2734\n",
      "Epoch [1/10], Step [340/938], Loss: 2.2751\n",
      "Epoch [1/10], Step [342/938], Loss: 2.2758\n",
      "Epoch [1/10], Step [344/938], Loss: 2.2611\n",
      "Epoch [1/10], Step [346/938], Loss: 2.2723\n",
      "Epoch [1/10], Step [348/938], Loss: 2.2768\n",
      "Epoch [1/10], Step [350/938], Loss: 2.2722\n",
      "Epoch [1/10], Step [352/938], Loss: 2.2704\n",
      "Epoch [1/10], Step [354/938], Loss: 2.2673\n",
      "Epoch [1/10], Step [356/938], Loss: 2.2725\n",
      "Epoch [1/10], Step [358/938], Loss: 2.2724\n",
      "Epoch [1/10], Step [360/938], Loss: 2.2687\n",
      "Epoch [1/10], Step [362/938], Loss: 2.2750\n",
      "Epoch [1/10], Step [364/938], Loss: 2.2732\n",
      "Epoch [1/10], Step [366/938], Loss: 2.2780\n",
      "Epoch [1/10], Step [368/938], Loss: 2.2736\n",
      "Epoch [1/10], Step [370/938], Loss: 2.2675\n",
      "Epoch [1/10], Step [372/938], Loss: 2.2705\n",
      "Epoch [1/10], Step [374/938], Loss: 2.2669\n",
      "Epoch [1/10], Step [376/938], Loss: 2.2682\n",
      "Epoch [1/10], Step [378/938], Loss: 2.2692\n",
      "Epoch [1/10], Step [380/938], Loss: 2.2667\n",
      "Epoch [1/10], Step [382/938], Loss: 2.2670\n",
      "Epoch [1/10], Step [384/938], Loss: 2.2626\n",
      "Epoch [1/10], Step [386/938], Loss: 2.2633\n",
      "Epoch [1/10], Step [388/938], Loss: 2.2710\n",
      "Epoch [1/10], Step [390/938], Loss: 2.2718\n",
      "Epoch [1/10], Step [392/938], Loss: 2.2702\n",
      "Epoch [1/10], Step [394/938], Loss: 2.2701\n",
      "Epoch [1/10], Step [396/938], Loss: 2.2665\n",
      "Epoch [1/10], Step [398/938], Loss: 2.2698\n",
      "Epoch [1/10], Step [400/938], Loss: 2.2646\n",
      "Epoch [1/10], Step [402/938], Loss: 2.2699\n",
      "Epoch [1/10], Step [404/938], Loss: 2.2725\n",
      "Epoch [1/10], Step [406/938], Loss: 2.2647\n",
      "Epoch [1/10], Step [408/938], Loss: 2.2718\n",
      "Epoch [1/10], Step [410/938], Loss: 2.2667\n",
      "Epoch [1/10], Step [412/938], Loss: 2.2695\n",
      "Epoch [1/10], Step [414/938], Loss: 2.2753\n",
      "Epoch [1/10], Step [416/938], Loss: 2.2677\n",
      "Epoch [1/10], Step [418/938], Loss: 2.2699\n",
      "Epoch [1/10], Step [420/938], Loss: 2.2597\n",
      "Epoch [1/10], Step [422/938], Loss: 2.2667\n",
      "Epoch [1/10], Step [424/938], Loss: 2.2621\n",
      "Epoch [1/10], Step [426/938], Loss: 2.2687\n",
      "Epoch [1/10], Step [428/938], Loss: 2.2671\n",
      "Epoch [1/10], Step [430/938], Loss: 2.2702\n",
      "Epoch [1/10], Step [432/938], Loss: 2.2604\n",
      "Epoch [1/10], Step [434/938], Loss: 2.2654\n",
      "Epoch [1/10], Step [436/938], Loss: 2.2662\n",
      "Epoch [1/10], Step [438/938], Loss: 2.2645\n",
      "Epoch [1/10], Step [440/938], Loss: 2.2603\n",
      "Epoch [1/10], Step [442/938], Loss: 2.2622\n",
      "Epoch [1/10], Step [444/938], Loss: 2.2663\n",
      "Epoch [1/10], Step [446/938], Loss: 2.2634\n",
      "Epoch [1/10], Step [448/938], Loss: 2.2585\n",
      "Epoch [1/10], Step [450/938], Loss: 2.2567\n",
      "Epoch [1/10], Step [452/938], Loss: 2.2699\n",
      "Epoch [1/10], Step [454/938], Loss: 2.2642\n",
      "Epoch [1/10], Step [456/938], Loss: 2.2631\n",
      "Epoch [1/10], Step [458/938], Loss: 2.2671\n",
      "Epoch [1/10], Step [460/938], Loss: 2.2648\n",
      "Epoch [1/10], Step [462/938], Loss: 2.2622\n",
      "Epoch [1/10], Step [464/938], Loss: 2.2606\n",
      "Epoch [1/10], Step [466/938], Loss: 2.2686\n",
      "Epoch [1/10], Step [468/938], Loss: 2.2667\n",
      "Epoch [1/10], Step [470/938], Loss: 2.2672\n",
      "Epoch [1/10], Step [472/938], Loss: 2.2606\n",
      "Epoch [1/10], Step [474/938], Loss: 2.2588\n",
      "Epoch [1/10], Step [476/938], Loss: 2.2570\n",
      "Epoch [1/10], Step [478/938], Loss: 2.2524\n",
      "Epoch [1/10], Step [480/938], Loss: 2.2677\n",
      "Epoch [1/10], Step [482/938], Loss: 2.2578\n",
      "Epoch [1/10], Step [484/938], Loss: 2.2570\n",
      "Epoch [1/10], Step [486/938], Loss: 2.2536\n",
      "Epoch [1/10], Step [488/938], Loss: 2.2628\n",
      "Epoch [1/10], Step [490/938], Loss: 2.2683\n",
      "Epoch [1/10], Step [492/938], Loss: 2.2550\n",
      "Epoch [1/10], Step [494/938], Loss: 2.2576\n",
      "Epoch [1/10], Step [496/938], Loss: 2.2622\n",
      "Epoch [1/10], Step [498/938], Loss: 2.2574\n",
      "Epoch [1/10], Step [500/938], Loss: 2.2567\n",
      "Epoch [1/10], Step [502/938], Loss: 2.2591\n",
      "Epoch [1/10], Step [504/938], Loss: 2.2595\n",
      "Epoch [1/10], Step [506/938], Loss: 2.2597\n",
      "Epoch [1/10], Step [508/938], Loss: 2.2526\n",
      "Epoch [1/10], Step [510/938], Loss: 2.2671\n",
      "Epoch [1/10], Step [512/938], Loss: 2.2557\n",
      "Epoch [1/10], Step [514/938], Loss: 2.2598\n",
      "Epoch [1/10], Step [516/938], Loss: 2.2618\n",
      "Epoch [1/10], Step [518/938], Loss: 2.2616\n",
      "Epoch [1/10], Step [520/938], Loss: 2.2650\n",
      "Epoch [1/10], Step [522/938], Loss: 2.2576\n",
      "Epoch [1/10], Step [524/938], Loss: 2.2536\n",
      "Epoch [1/10], Step [526/938], Loss: 2.2607\n",
      "Epoch [1/10], Step [528/938], Loss: 2.2498\n",
      "Epoch [1/10], Step [530/938], Loss: 2.2512\n",
      "Epoch [1/10], Step [532/938], Loss: 2.2561\n",
      "Epoch [1/10], Step [534/938], Loss: 2.2531\n",
      "Epoch [1/10], Step [536/938], Loss: 2.2519\n",
      "Epoch [1/10], Step [538/938], Loss: 2.2546\n",
      "Epoch [1/10], Step [540/938], Loss: 2.2523\n",
      "Epoch [1/10], Step [542/938], Loss: 2.2534\n",
      "Epoch [1/10], Step [544/938], Loss: 2.2572\n",
      "Epoch [1/10], Step [546/938], Loss: 2.2553\n",
      "Epoch [1/10], Step [548/938], Loss: 2.2560\n",
      "Epoch [1/10], Step [550/938], Loss: 2.2549\n",
      "Epoch [1/10], Step [552/938], Loss: 2.2557\n",
      "Epoch [1/10], Step [554/938], Loss: 2.2514\n",
      "Epoch [1/10], Step [556/938], Loss: 2.2627\n",
      "Epoch [1/10], Step [558/938], Loss: 2.2579\n",
      "Epoch [1/10], Step [560/938], Loss: 2.2497\n",
      "Epoch [1/10], Step [562/938], Loss: 2.2458\n",
      "Epoch [1/10], Step [564/938], Loss: 2.2592\n",
      "Epoch [1/10], Step [566/938], Loss: 2.2434\n",
      "Epoch [1/10], Step [568/938], Loss: 2.2555\n",
      "Epoch [1/10], Step [570/938], Loss: 2.2531\n",
      "Epoch [1/10], Step [572/938], Loss: 2.2357\n",
      "Epoch [1/10], Step [574/938], Loss: 2.2522\n",
      "Epoch [1/10], Step [576/938], Loss: 2.2571\n",
      "Epoch [1/10], Step [578/938], Loss: 2.2437\n",
      "Epoch [1/10], Step [580/938], Loss: 2.2575\n",
      "Epoch [1/10], Step [582/938], Loss: 2.2514\n",
      "Epoch [1/10], Step [584/938], Loss: 2.2533\n",
      "Epoch [1/10], Step [586/938], Loss: 2.2469\n",
      "Epoch [1/10], Step [588/938], Loss: 2.2540\n",
      "Epoch [1/10], Step [590/938], Loss: 2.2572\n",
      "Epoch [1/10], Step [592/938], Loss: 2.2438\n",
      "Epoch [1/10], Step [594/938], Loss: 2.2503\n",
      "Epoch [1/10], Step [596/938], Loss: 2.2471\n",
      "Epoch [1/10], Step [598/938], Loss: 2.2540\n",
      "Epoch [1/10], Step [600/938], Loss: 2.2497\n",
      "Epoch [1/10], Step [602/938], Loss: 2.2398\n",
      "Epoch [1/10], Step [604/938], Loss: 2.2452\n",
      "Epoch [1/10], Step [606/938], Loss: 2.2460\n",
      "Epoch [1/10], Step [608/938], Loss: 2.2426\n",
      "Epoch [1/10], Step [610/938], Loss: 2.2468\n",
      "Epoch [1/10], Step [612/938], Loss: 2.2508\n",
      "Epoch [1/10], Step [614/938], Loss: 2.2567\n",
      "Epoch [1/10], Step [616/938], Loss: 2.2296\n",
      "Epoch [1/10], Step [618/938], Loss: 2.2565\n",
      "Epoch [1/10], Step [620/938], Loss: 2.2484\n",
      "Epoch [1/10], Step [622/938], Loss: 2.2321\n",
      "Epoch [1/10], Step [624/938], Loss: 2.2427\n",
      "Epoch [1/10], Step [626/938], Loss: 2.2442\n",
      "Epoch [1/10], Step [628/938], Loss: 2.2462\n",
      "Epoch [1/10], Step [630/938], Loss: 2.2425\n",
      "Epoch [1/10], Step [632/938], Loss: 2.2446\n",
      "Epoch [1/10], Step [634/938], Loss: 2.2378\n",
      "Epoch [1/10], Step [636/938], Loss: 2.2423\n",
      "Epoch [1/10], Step [638/938], Loss: 2.2393\n",
      "Epoch [1/10], Step [640/938], Loss: 2.2488\n",
      "Epoch [1/10], Step [642/938], Loss: 2.2394\n",
      "Epoch [1/10], Step [644/938], Loss: 2.2472\n",
      "Epoch [1/10], Step [646/938], Loss: 2.2462\n",
      "Epoch [1/10], Step [648/938], Loss: 2.2432\n",
      "Epoch [1/10], Step [650/938], Loss: 2.2457\n",
      "Epoch [1/10], Step [652/938], Loss: 2.2352\n",
      "Epoch [1/10], Step [654/938], Loss: 2.2355\n",
      "Epoch [1/10], Step [656/938], Loss: 2.2287\n",
      "Epoch [1/10], Step [658/938], Loss: 2.2366\n",
      "Epoch [1/10], Step [660/938], Loss: 2.2495\n",
      "Epoch [1/10], Step [662/938], Loss: 2.2182\n",
      "Epoch [1/10], Step [664/938], Loss: 2.2451\n",
      "Epoch [1/10], Step [666/938], Loss: 2.2342\n",
      "Epoch [1/10], Step [668/938], Loss: 2.2369\n",
      "Epoch [1/10], Step [670/938], Loss: 2.2471\n",
      "Epoch [1/10], Step [672/938], Loss: 2.2345\n",
      "Epoch [1/10], Step [674/938], Loss: 2.2236\n",
      "Epoch [1/10], Step [676/938], Loss: 2.2245\n",
      "Epoch [1/10], Step [678/938], Loss: 2.2364\n",
      "Epoch [1/10], Step [680/938], Loss: 2.2347\n",
      "Epoch [1/10], Step [682/938], Loss: 2.2327\n",
      "Epoch [1/10], Step [684/938], Loss: 2.2351\n",
      "Epoch [1/10], Step [686/938], Loss: 2.2369\n",
      "Epoch [1/10], Step [688/938], Loss: 2.2301\n",
      "Epoch [1/10], Step [690/938], Loss: 2.2432\n",
      "Epoch [1/10], Step [692/938], Loss: 2.2329\n",
      "Epoch [1/10], Step [694/938], Loss: 2.2197\n",
      "Epoch [1/10], Step [696/938], Loss: 2.2419\n",
      "Epoch [1/10], Step [698/938], Loss: 2.2360\n",
      "Epoch [1/10], Step [700/938], Loss: 2.2397\n",
      "Epoch [1/10], Step [702/938], Loss: 2.2524\n",
      "Epoch [1/10], Step [704/938], Loss: 2.2270\n",
      "Epoch [1/10], Step [706/938], Loss: 2.2371\n",
      "Epoch [1/10], Step [708/938], Loss: 2.2390\n",
      "Epoch [1/10], Step [710/938], Loss: 2.2340\n",
      "Epoch [1/10], Step [712/938], Loss: 2.2306\n",
      "Epoch [1/10], Step [714/938], Loss: 2.2361\n",
      "Epoch [1/10], Step [716/938], Loss: 2.2324\n",
      "Epoch [1/10], Step [718/938], Loss: 2.2415\n",
      "Epoch [1/10], Step [720/938], Loss: 2.2405\n",
      "Epoch [1/10], Step [722/938], Loss: 2.2248\n",
      "Epoch [1/10], Step [724/938], Loss: 2.2227\n",
      "Epoch [1/10], Step [726/938], Loss: 2.2515\n",
      "Epoch [1/10], Step [728/938], Loss: 2.2278\n",
      "Epoch [1/10], Step [730/938], Loss: 2.2246\n",
      "Epoch [1/10], Step [732/938], Loss: 2.2258\n",
      "Epoch [1/10], Step [734/938], Loss: 2.2304\n",
      "Epoch [1/10], Step [736/938], Loss: 2.2291\n",
      "Epoch [1/10], Step [738/938], Loss: 2.2282\n",
      "Epoch [1/10], Step [740/938], Loss: 2.2385\n",
      "Epoch [1/10], Step [742/938], Loss: 2.2189\n",
      "Epoch [1/10], Step [744/938], Loss: 2.2350\n",
      "Epoch [1/10], Step [746/938], Loss: 2.2346\n",
      "Epoch [1/10], Step [748/938], Loss: 2.2177\n",
      "Epoch [1/10], Step [750/938], Loss: 2.2400\n",
      "Epoch [1/10], Step [752/938], Loss: 2.2098\n",
      "Epoch [1/10], Step [754/938], Loss: 2.2422\n",
      "Epoch [1/10], Step [756/938], Loss: 2.2163\n",
      "Epoch [1/10], Step [758/938], Loss: 2.2491\n",
      "Epoch [1/10], Step [760/938], Loss: 2.2268\n",
      "Epoch [1/10], Step [762/938], Loss: 2.2178\n",
      "Epoch [1/10], Step [764/938], Loss: 2.2311\n",
      "Epoch [1/10], Step [766/938], Loss: 2.2532\n",
      "Epoch [1/10], Step [768/938], Loss: 2.2376\n",
      "Epoch [1/10], Step [770/938], Loss: 2.2330\n",
      "Epoch [1/10], Step [772/938], Loss: 2.2159\n",
      "Epoch [1/10], Step [774/938], Loss: 2.2400\n",
      "Epoch [1/10], Step [776/938], Loss: 2.2231\n",
      "Epoch [1/10], Step [778/938], Loss: 2.2390\n",
      "Epoch [1/10], Step [780/938], Loss: 2.2221\n",
      "Epoch [1/10], Step [782/938], Loss: 2.2262\n",
      "Epoch [1/10], Step [784/938], Loss: 2.2234\n",
      "Epoch [1/10], Step [786/938], Loss: 2.2142\n",
      "Epoch [1/10], Step [788/938], Loss: 2.2146\n",
      "Epoch [1/10], Step [790/938], Loss: 2.2167\n",
      "Epoch [1/10], Step [792/938], Loss: 2.2256\n",
      "Epoch [1/10], Step [794/938], Loss: 2.2088\n",
      "Epoch [1/10], Step [796/938], Loss: 2.2243\n",
      "Epoch [1/10], Step [798/938], Loss: 2.2336\n",
      "Epoch [1/10], Step [800/938], Loss: 2.2268\n",
      "Epoch [1/10], Step [802/938], Loss: 2.2122\n",
      "Epoch [1/10], Step [804/938], Loss: 2.2333\n",
      "Epoch [1/10], Step [806/938], Loss: 2.2180\n",
      "Epoch [1/10], Step [808/938], Loss: 2.2166\n",
      "Epoch [1/10], Step [810/938], Loss: 2.2279\n",
      "Epoch [1/10], Step [812/938], Loss: 2.2316\n",
      "Epoch [1/10], Step [814/938], Loss: 2.2242\n",
      "Epoch [1/10], Step [816/938], Loss: 2.2138\n",
      "Epoch [1/10], Step [818/938], Loss: 2.2319\n",
      "Epoch [1/10], Step [820/938], Loss: 2.2215\n",
      "Epoch [1/10], Step [822/938], Loss: 2.2047\n",
      "Epoch [1/10], Step [824/938], Loss: 2.2289\n",
      "Epoch [1/10], Step [826/938], Loss: 2.2102\n",
      "Epoch [1/10], Step [828/938], Loss: 2.2120\n",
      "Epoch [1/10], Step [830/938], Loss: 2.2163\n",
      "Epoch [1/10], Step [832/938], Loss: 2.2193\n",
      "Epoch [1/10], Step [834/938], Loss: 2.2276\n",
      "Epoch [1/10], Step [836/938], Loss: 2.2063\n",
      "Epoch [1/10], Step [838/938], Loss: 2.2037\n",
      "Epoch [1/10], Step [840/938], Loss: 2.2152\n",
      "Epoch [1/10], Step [842/938], Loss: 2.2202\n",
      "Epoch [1/10], Step [844/938], Loss: 2.2127\n",
      "Epoch [1/10], Step [846/938], Loss: 2.2064\n",
      "Epoch [1/10], Step [848/938], Loss: 2.2236\n",
      "Epoch [1/10], Step [850/938], Loss: 2.2269\n",
      "Epoch [1/10], Step [852/938], Loss: 2.2285\n",
      "Epoch [1/10], Step [854/938], Loss: 2.2143\n",
      "Epoch [1/10], Step [856/938], Loss: 2.2117\n",
      "Epoch [1/10], Step [858/938], Loss: 2.2092\n",
      "Epoch [1/10], Step [860/938], Loss: 2.2301\n",
      "Epoch [1/10], Step [862/938], Loss: 2.2151\n",
      "Epoch [1/10], Step [864/938], Loss: 2.2093\n",
      "Epoch [1/10], Step [866/938], Loss: 2.1992\n",
      "Epoch [1/10], Step [868/938], Loss: 2.2172\n",
      "Epoch [1/10], Step [870/938], Loss: 2.2129\n",
      "Epoch [1/10], Step [872/938], Loss: 2.2089\n",
      "Epoch [1/10], Step [874/938], Loss: 2.1817\n",
      "Epoch [1/10], Step [876/938], Loss: 2.2058\n",
      "Epoch [1/10], Step [878/938], Loss: 2.2034\n",
      "Epoch [1/10], Step [880/938], Loss: 2.2174\n",
      "Epoch [1/10], Step [882/938], Loss: 2.2000\n",
      "Epoch [1/10], Step [884/938], Loss: 2.2092\n",
      "Epoch [1/10], Step [886/938], Loss: 2.2159\n",
      "Epoch [1/10], Step [888/938], Loss: 2.2112\n",
      "Epoch [1/10], Step [890/938], Loss: 2.2103\n",
      "Epoch [1/10], Step [892/938], Loss: 2.1984\n",
      "Epoch [1/10], Step [894/938], Loss: 2.2022\n",
      "Epoch [1/10], Step [896/938], Loss: 2.2214\n",
      "Epoch [1/10], Step [898/938], Loss: 2.2234\n",
      "Epoch [1/10], Step [900/938], Loss: 2.2050\n",
      "Epoch [1/10], Step [902/938], Loss: 2.2126\n",
      "Epoch [1/10], Step [904/938], Loss: 2.2087\n",
      "Epoch [1/10], Step [906/938], Loss: 2.2141\n",
      "Epoch [1/10], Step [908/938], Loss: 2.2117\n",
      "Epoch [1/10], Step [910/938], Loss: 2.2006\n",
      "Epoch [1/10], Step [912/938], Loss: 2.2161\n",
      "Epoch [1/10], Step [914/938], Loss: 2.2189\n",
      "Epoch [1/10], Step [916/938], Loss: 2.2010\n",
      "Epoch [1/10], Step [918/938], Loss: 2.2045\n",
      "Epoch [1/10], Step [920/938], Loss: 2.2105\n",
      "Epoch [1/10], Step [922/938], Loss: 2.2103\n",
      "Epoch [1/10], Step [924/938], Loss: 2.2108\n",
      "Epoch [1/10], Step [926/938], Loss: 2.2082\n",
      "Epoch [1/10], Step [928/938], Loss: 2.1949\n",
      "Epoch [1/10], Step [930/938], Loss: 2.1902\n",
      "Epoch [1/10], Step [932/938], Loss: 2.2137\n",
      "Epoch [1/10], Step [934/938], Loss: 2.1797\n",
      "Epoch [1/10], Step [936/938], Loss: 2.2116\n",
      "Epoch [1/10], Step [938/938], Loss: 2.2386\n",
      "Epoch [1/10], Loss: 2.2579\n",
      "Epoch [2/10], Step [2/938], Loss: 2.1889\n",
      "Epoch [2/10], Step [4/938], Loss: 2.2040\n",
      "Epoch [2/10], Step [6/938], Loss: 2.2038\n",
      "Epoch [2/10], Step [8/938], Loss: 2.2041\n",
      "Epoch [2/10], Step [10/938], Loss: 2.2092\n",
      "Epoch [2/10], Step [12/938], Loss: 2.1847\n",
      "Epoch [2/10], Step [14/938], Loss: 2.1818\n",
      "Epoch [2/10], Step [16/938], Loss: 2.1903\n",
      "Epoch [2/10], Step [18/938], Loss: 2.2071\n",
      "Epoch [2/10], Step [20/938], Loss: 2.2007\n",
      "Epoch [2/10], Step [22/938], Loss: 2.1943\n",
      "Epoch [2/10], Step [24/938], Loss: 2.1978\n",
      "Epoch [2/10], Step [26/938], Loss: 2.1956\n",
      "Epoch [2/10], Step [28/938], Loss: 2.1985\n",
      "Epoch [2/10], Step [30/938], Loss: 2.1854\n",
      "Epoch [2/10], Step [32/938], Loss: 2.1959\n",
      "Epoch [2/10], Step [34/938], Loss: 2.1960\n",
      "Epoch [2/10], Step [36/938], Loss: 2.1871\n",
      "Epoch [2/10], Step [38/938], Loss: 2.2026\n",
      "Epoch [2/10], Step [40/938], Loss: 2.1953\n",
      "Epoch [2/10], Step [42/938], Loss: 2.1970\n",
      "Epoch [2/10], Step [44/938], Loss: 2.1990\n",
      "Epoch [2/10], Step [46/938], Loss: 2.1975\n",
      "Epoch [2/10], Step [48/938], Loss: 2.1869\n",
      "Epoch [2/10], Step [50/938], Loss: 2.2073\n",
      "Epoch [2/10], Step [52/938], Loss: 2.1881\n",
      "Epoch [2/10], Step [54/938], Loss: 2.2054\n",
      "Epoch [2/10], Step [56/938], Loss: 2.1811\n",
      "Epoch [2/10], Step [58/938], Loss: 2.2025\n",
      "Epoch [2/10], Step [60/938], Loss: 2.1888\n",
      "Epoch [2/10], Step [62/938], Loss: 2.2099\n",
      "Epoch [2/10], Step [64/938], Loss: 2.1830\n",
      "Epoch [2/10], Step [66/938], Loss: 2.1824\n",
      "Epoch [2/10], Step [68/938], Loss: 2.1753\n",
      "Epoch [2/10], Step [70/938], Loss: 2.1925\n",
      "Epoch [2/10], Step [72/938], Loss: 2.1992\n",
      "Epoch [2/10], Step [74/938], Loss: 2.1773\n",
      "Epoch [2/10], Step [76/938], Loss: 2.1763\n",
      "Epoch [2/10], Step [78/938], Loss: 2.2170\n",
      "Epoch [2/10], Step [80/938], Loss: 2.1781\n",
      "Epoch [2/10], Step [82/938], Loss: 2.1979\n",
      "Epoch [2/10], Step [84/938], Loss: 2.1651\n",
      "Epoch [2/10], Step [86/938], Loss: 2.1798\n",
      "Epoch [2/10], Step [88/938], Loss: 2.1852\n",
      "Epoch [2/10], Step [90/938], Loss: 2.2010\n",
      "Epoch [2/10], Step [92/938], Loss: 2.1811\n",
      "Epoch [2/10], Step [94/938], Loss: 2.1843\n",
      "Epoch [2/10], Step [96/938], Loss: 2.1686\n",
      "Epoch [2/10], Step [98/938], Loss: 2.1963\n",
      "Epoch [2/10], Step [100/938], Loss: 2.1783\n",
      "Epoch [2/10], Step [102/938], Loss: 2.1795\n",
      "Epoch [2/10], Step [104/938], Loss: 2.1835\n",
      "Epoch [2/10], Step [106/938], Loss: 2.1915\n",
      "Epoch [2/10], Step [108/938], Loss: 2.1814\n",
      "Epoch [2/10], Step [110/938], Loss: 2.1706\n",
      "Epoch [2/10], Step [112/938], Loss: 2.1799\n",
      "Epoch [2/10], Step [114/938], Loss: 2.1958\n",
      "Epoch [2/10], Step [116/938], Loss: 2.1866\n",
      "Epoch [2/10], Step [118/938], Loss: 2.1844\n",
      "Epoch [2/10], Step [120/938], Loss: 2.1812\n",
      "Epoch [2/10], Step [122/938], Loss: 2.1636\n",
      "Epoch [2/10], Step [124/938], Loss: 2.1798\n",
      "Epoch [2/10], Step [126/938], Loss: 2.1758\n",
      "Epoch [2/10], Step [128/938], Loss: 2.1511\n",
      "Epoch [2/10], Step [130/938], Loss: 2.1681\n",
      "Epoch [2/10], Step [132/938], Loss: 2.1712\n",
      "Epoch [2/10], Step [134/938], Loss: 2.1718\n",
      "Epoch [2/10], Step [136/938], Loss: 2.1809\n",
      "Epoch [2/10], Step [138/938], Loss: 2.1623\n",
      "Epoch [2/10], Step [140/938], Loss: 2.1749\n",
      "Epoch [2/10], Step [142/938], Loss: 2.1792\n",
      "Epoch [2/10], Step [144/938], Loss: 2.1778\n",
      "Epoch [2/10], Step [146/938], Loss: 2.1760\n",
      "Epoch [2/10], Step [148/938], Loss: 2.1752\n",
      "Epoch [2/10], Step [150/938], Loss: 2.1463\n",
      "Epoch [2/10], Step [152/938], Loss: 2.1864\n",
      "Epoch [2/10], Step [154/938], Loss: 2.1786\n",
      "Epoch [2/10], Step [156/938], Loss: 2.1858\n",
      "Epoch [2/10], Step [158/938], Loss: 2.1670\n",
      "Epoch [2/10], Step [160/938], Loss: 2.1745\n",
      "Epoch [2/10], Step [162/938], Loss: 2.1689\n",
      "Epoch [2/10], Step [164/938], Loss: 2.1675\n",
      "Epoch [2/10], Step [166/938], Loss: 2.1590\n",
      "Epoch [2/10], Step [168/938], Loss: 2.1700\n",
      "Epoch [2/10], Step [170/938], Loss: 2.1762\n",
      "Epoch [2/10], Step [172/938], Loss: 2.1624\n",
      "Epoch [2/10], Step [174/938], Loss: 2.1708\n",
      "Epoch [2/10], Step [176/938], Loss: 2.1358\n",
      "Epoch [2/10], Step [178/938], Loss: 2.1572\n",
      "Epoch [2/10], Step [180/938], Loss: 2.1679\n",
      "Epoch [2/10], Step [182/938], Loss: 2.1664\n",
      "Epoch [2/10], Step [184/938], Loss: 2.1550\n",
      "Epoch [2/10], Step [186/938], Loss: 2.1780\n",
      "Epoch [2/10], Step [188/938], Loss: 2.1627\n",
      "Epoch [2/10], Step [190/938], Loss: 2.1669\n",
      "Epoch [2/10], Step [192/938], Loss: 2.1730\n",
      "Epoch [2/10], Step [194/938], Loss: 2.1649\n",
      "Epoch [2/10], Step [196/938], Loss: 2.1486\n",
      "Epoch [2/10], Step [198/938], Loss: 2.1856\n",
      "Epoch [2/10], Step [200/938], Loss: 2.1654\n",
      "Epoch [2/10], Step [202/938], Loss: 2.1669\n",
      "Epoch [2/10], Step [204/938], Loss: 2.1550\n",
      "Epoch [2/10], Step [206/938], Loss: 2.1540\n",
      "Epoch [2/10], Step [208/938], Loss: 2.1412\n",
      "Epoch [2/10], Step [210/938], Loss: 2.1578\n",
      "Epoch [2/10], Step [212/938], Loss: 2.1611\n",
      "Epoch [2/10], Step [214/938], Loss: 2.1694\n",
      "Epoch [2/10], Step [216/938], Loss: 2.1246\n",
      "Epoch [2/10], Step [218/938], Loss: 2.1575\n",
      "Epoch [2/10], Step [220/938], Loss: 2.1807\n",
      "Epoch [2/10], Step [222/938], Loss: 2.1607\n",
      "Epoch [2/10], Step [224/938], Loss: 2.1693\n",
      "Epoch [2/10], Step [226/938], Loss: 2.1722\n",
      "Epoch [2/10], Step [228/938], Loss: 2.1336\n",
      "Epoch [2/10], Step [230/938], Loss: 2.1455\n",
      "Epoch [2/10], Step [232/938], Loss: 2.1517\n",
      "Epoch [2/10], Step [234/938], Loss: 2.1300\n",
      "Epoch [2/10], Step [236/938], Loss: 2.1540\n",
      "Epoch [2/10], Step [238/938], Loss: 2.1466\n",
      "Epoch [2/10], Step [240/938], Loss: 2.1729\n",
      "Epoch [2/10], Step [242/938], Loss: 2.1168\n",
      "Epoch [2/10], Step [244/938], Loss: 2.1362\n",
      "Epoch [2/10], Step [246/938], Loss: 2.1639\n",
      "Epoch [2/10], Step [248/938], Loss: 2.1430\n",
      "Epoch [2/10], Step [250/938], Loss: 2.1331\n",
      "Epoch [2/10], Step [252/938], Loss: 2.1296\n",
      "Epoch [2/10], Step [254/938], Loss: 2.1565\n",
      "Epoch [2/10], Step [256/938], Loss: 2.1389\n",
      "Epoch [2/10], Step [258/938], Loss: 2.1518\n",
      "Epoch [2/10], Step [260/938], Loss: 2.1269\n",
      "Epoch [2/10], Step [262/938], Loss: 2.1350\n",
      "Epoch [2/10], Step [264/938], Loss: 2.1642\n",
      "Epoch [2/10], Step [266/938], Loss: 2.1651\n",
      "Epoch [2/10], Step [268/938], Loss: 2.1441\n",
      "Epoch [2/10], Step [270/938], Loss: 2.1434\n",
      "Epoch [2/10], Step [272/938], Loss: 2.1187\n",
      "Epoch [2/10], Step [274/938], Loss: 2.1613\n",
      "Epoch [2/10], Step [276/938], Loss: 2.1463\n",
      "Epoch [2/10], Step [278/938], Loss: 2.1619\n",
      "Epoch [2/10], Step [280/938], Loss: 2.1470\n",
      "Epoch [2/10], Step [282/938], Loss: 2.1278\n",
      "Epoch [2/10], Step [284/938], Loss: 2.1325\n",
      "Epoch [2/10], Step [286/938], Loss: 2.1470\n",
      "Epoch [2/10], Step [288/938], Loss: 2.1225\n",
      "Epoch [2/10], Step [290/938], Loss: 2.1371\n",
      "Epoch [2/10], Step [292/938], Loss: 2.1458\n",
      "Epoch [2/10], Step [294/938], Loss: 2.1616\n",
      "Epoch [2/10], Step [296/938], Loss: 2.1562\n",
      "Epoch [2/10], Step [298/938], Loss: 2.1526\n",
      "Epoch [2/10], Step [300/938], Loss: 2.1310\n",
      "Epoch [2/10], Step [302/938], Loss: 2.1259\n",
      "Epoch [2/10], Step [304/938], Loss: 2.1344\n",
      "Epoch [2/10], Step [306/938], Loss: 2.1252\n",
      "Epoch [2/10], Step [308/938], Loss: 2.1209\n",
      "Epoch [2/10], Step [310/938], Loss: 2.1415\n",
      "Epoch [2/10], Step [312/938], Loss: 2.1031\n",
      "Epoch [2/10], Step [314/938], Loss: 2.1233\n",
      "Epoch [2/10], Step [316/938], Loss: 2.1385\n",
      "Epoch [2/10], Step [318/938], Loss: 2.1284\n",
      "Epoch [2/10], Step [320/938], Loss: 2.1492\n",
      "Epoch [2/10], Step [322/938], Loss: 2.1275\n",
      "Epoch [2/10], Step [324/938], Loss: 2.1346\n",
      "Epoch [2/10], Step [326/938], Loss: 2.1159\n",
      "Epoch [2/10], Step [328/938], Loss: 2.1391\n",
      "Epoch [2/10], Step [330/938], Loss: 2.1541\n",
      "Epoch [2/10], Step [332/938], Loss: 2.1415\n",
      "Epoch [2/10], Step [334/938], Loss: 2.1207\n",
      "Epoch [2/10], Step [336/938], Loss: 2.1206\n",
      "Epoch [2/10], Step [338/938], Loss: 2.1208\n",
      "Epoch [2/10], Step [340/938], Loss: 2.1370\n",
      "Epoch [2/10], Step [342/938], Loss: 2.1182\n",
      "Epoch [2/10], Step [344/938], Loss: 2.1330\n",
      "Epoch [2/10], Step [346/938], Loss: 2.0916\n",
      "Epoch [2/10], Step [348/938], Loss: 2.1032\n",
      "Epoch [2/10], Step [350/938], Loss: 2.1232\n",
      "Epoch [2/10], Step [352/938], Loss: 2.1228\n",
      "Epoch [2/10], Step [354/938], Loss: 2.1568\n",
      "Epoch [2/10], Step [356/938], Loss: 2.1222\n",
      "Epoch [2/10], Step [358/938], Loss: 2.1161\n",
      "Epoch [2/10], Step [360/938], Loss: 2.1292\n",
      "Epoch [2/10], Step [362/938], Loss: 2.1099\n",
      "Epoch [2/10], Step [364/938], Loss: 2.1038\n",
      "Epoch [2/10], Step [366/938], Loss: 2.1456\n",
      "Epoch [2/10], Step [368/938], Loss: 2.1283\n",
      "Epoch [2/10], Step [370/938], Loss: 2.1119\n",
      "Epoch [2/10], Step [372/938], Loss: 2.1075\n",
      "Epoch [2/10], Step [374/938], Loss: 2.1193\n",
      "Epoch [2/10], Step [376/938], Loss: 2.1326\n",
      "Epoch [2/10], Step [378/938], Loss: 2.1106\n",
      "Epoch [2/10], Step [380/938], Loss: 2.1119\n",
      "Epoch [2/10], Step [382/938], Loss: 2.1414\n",
      "Epoch [2/10], Step [384/938], Loss: 2.1033\n",
      "Epoch [2/10], Step [386/938], Loss: 2.0897\n",
      "Epoch [2/10], Step [388/938], Loss: 2.1148\n",
      "Epoch [2/10], Step [390/938], Loss: 2.1361\n",
      "Epoch [2/10], Step [392/938], Loss: 2.1048\n",
      "Epoch [2/10], Step [394/938], Loss: 2.1210\n",
      "Epoch [2/10], Step [396/938], Loss: 2.0904\n",
      "Epoch [2/10], Step [398/938], Loss: 2.1265\n",
      "Epoch [2/10], Step [400/938], Loss: 2.0828\n",
      "Epoch [2/10], Step [402/938], Loss: 2.1462\n",
      "Epoch [2/10], Step [404/938], Loss: 2.0777\n",
      "Epoch [2/10], Step [406/938], Loss: 2.1297\n",
      "Epoch [2/10], Step [408/938], Loss: 2.1184\n",
      "Epoch [2/10], Step [410/938], Loss: 2.0739\n",
      "Epoch [2/10], Step [412/938], Loss: 2.0781\n",
      "Epoch [2/10], Step [414/938], Loss: 2.1089\n",
      "Epoch [2/10], Step [416/938], Loss: 2.1057\n",
      "Epoch [2/10], Step [418/938], Loss: 2.0975\n",
      "Epoch [2/10], Step [420/938], Loss: 2.1245\n",
      "Epoch [2/10], Step [422/938], Loss: 2.1273\n",
      "Epoch [2/10], Step [424/938], Loss: 2.0888\n",
      "Epoch [2/10], Step [426/938], Loss: 2.1095\n",
      "Epoch [2/10], Step [428/938], Loss: 2.0861\n",
      "Epoch [2/10], Step [430/938], Loss: 2.1310\n",
      "Epoch [2/10], Step [432/938], Loss: 2.0843\n",
      "Epoch [2/10], Step [434/938], Loss: 2.0965\n",
      "Epoch [2/10], Step [436/938], Loss: 2.1097\n",
      "Epoch [2/10], Step [438/938], Loss: 2.1195\n",
      "Epoch [2/10], Step [440/938], Loss: 2.1096\n",
      "Epoch [2/10], Step [442/938], Loss: 2.1015\n",
      "Epoch [2/10], Step [444/938], Loss: 2.1177\n",
      "Epoch [2/10], Step [446/938], Loss: 2.1069\n",
      "Epoch [2/10], Step [448/938], Loss: 2.1069\n",
      "Epoch [2/10], Step [450/938], Loss: 2.0941\n",
      "Epoch [2/10], Step [452/938], Loss: 2.0791\n",
      "Epoch [2/10], Step [454/938], Loss: 2.0682\n",
      "Epoch [2/10], Step [456/938], Loss: 2.0775\n",
      "Epoch [2/10], Step [458/938], Loss: 2.0646\n",
      "Epoch [2/10], Step [460/938], Loss: 2.1096\n",
      "Epoch [2/10], Step [462/938], Loss: 2.0737\n",
      "Epoch [2/10], Step [464/938], Loss: 2.0899\n",
      "Epoch [2/10], Step [466/938], Loss: 2.0695\n",
      "Epoch [2/10], Step [468/938], Loss: 2.0990\n",
      "Epoch [2/10], Step [470/938], Loss: 2.0778\n",
      "Epoch [2/10], Step [472/938], Loss: 2.1099\n",
      "Epoch [2/10], Step [474/938], Loss: 2.0948\n",
      "Epoch [2/10], Step [476/938], Loss: 2.0853\n",
      "Epoch [2/10], Step [478/938], Loss: 2.0561\n",
      "Epoch [2/10], Step [480/938], Loss: 2.0737\n",
      "Epoch [2/10], Step [482/938], Loss: 2.1073\n",
      "Epoch [2/10], Step [484/938], Loss: 2.0630\n",
      "Epoch [2/10], Step [486/938], Loss: 2.0705\n",
      "Epoch [2/10], Step [488/938], Loss: 2.0870\n",
      "Epoch [2/10], Step [490/938], Loss: 2.0786\n",
      "Epoch [2/10], Step [492/938], Loss: 2.0910\n",
      "Epoch [2/10], Step [494/938], Loss: 2.0976\n",
      "Epoch [2/10], Step [496/938], Loss: 2.0535\n",
      "Epoch [2/10], Step [498/938], Loss: 2.0993\n",
      "Epoch [2/10], Step [500/938], Loss: 2.0976\n",
      "Epoch [2/10], Step [502/938], Loss: 2.0585\n",
      "Epoch [2/10], Step [504/938], Loss: 2.0848\n",
      "Epoch [2/10], Step [506/938], Loss: 2.0581\n",
      "Epoch [2/10], Step [508/938], Loss: 2.0942\n",
      "Epoch [2/10], Step [510/938], Loss: 2.0865\n",
      "Epoch [2/10], Step [512/938], Loss: 2.0529\n",
      "Epoch [2/10], Step [514/938], Loss: 2.0425\n",
      "Epoch [2/10], Step [516/938], Loss: 2.0852\n",
      "Epoch [2/10], Step [518/938], Loss: 2.1132\n",
      "Epoch [2/10], Step [520/938], Loss: 2.0433\n",
      "Epoch [2/10], Step [522/938], Loss: 2.1006\n",
      "Epoch [2/10], Step [524/938], Loss: 2.0674\n",
      "Epoch [2/10], Step [526/938], Loss: 2.0570\n",
      "Epoch [2/10], Step [528/938], Loss: 2.1019\n",
      "Epoch [2/10], Step [530/938], Loss: 2.0482\n",
      "Epoch [2/10], Step [532/938], Loss: 2.0359\n",
      "Epoch [2/10], Step [534/938], Loss: 2.0485\n",
      "Epoch [2/10], Step [536/938], Loss: 2.0486\n",
      "Epoch [2/10], Step [538/938], Loss: 2.1036\n",
      "Epoch [2/10], Step [540/938], Loss: 2.0425\n",
      "Epoch [2/10], Step [542/938], Loss: 2.0842\n",
      "Epoch [2/10], Step [544/938], Loss: 2.0495\n",
      "Epoch [2/10], Step [546/938], Loss: 2.0943\n",
      "Epoch [2/10], Step [548/938], Loss: 2.0928\n",
      "Epoch [2/10], Step [550/938], Loss: 2.0462\n",
      "Epoch [2/10], Step [552/938], Loss: 2.0838\n",
      "Epoch [2/10], Step [554/938], Loss: 2.0301\n",
      "Epoch [2/10], Step [556/938], Loss: 2.0890\n",
      "Epoch [2/10], Step [558/938], Loss: 2.0543\n",
      "Epoch [2/10], Step [560/938], Loss: 2.0776\n",
      "Epoch [2/10], Step [562/938], Loss: 2.0419\n",
      "Epoch [2/10], Step [564/938], Loss: 2.0463\n",
      "Epoch [2/10], Step [566/938], Loss: 2.0706\n",
      "Epoch [2/10], Step [568/938], Loss: 2.0561\n",
      "Epoch [2/10], Step [570/938], Loss: 2.0795\n",
      "Epoch [2/10], Step [572/938], Loss: 1.9987\n",
      "Epoch [2/10], Step [574/938], Loss: 2.0215\n",
      "Epoch [2/10], Step [576/938], Loss: 2.0692\n",
      "Epoch [2/10], Step [578/938], Loss: 2.0386\n",
      "Epoch [2/10], Step [580/938], Loss: 2.0516\n",
      "Epoch [2/10], Step [582/938], Loss: 1.9990\n",
      "Epoch [2/10], Step [584/938], Loss: 2.0307\n",
      "Epoch [2/10], Step [586/938], Loss: 2.0252\n",
      "Epoch [2/10], Step [588/938], Loss: 2.0444\n",
      "Epoch [2/10], Step [590/938], Loss: 2.0794\n",
      "Epoch [2/10], Step [592/938], Loss: 2.0652\n",
      "Epoch [2/10], Step [594/938], Loss: 2.0355\n",
      "Epoch [2/10], Step [596/938], Loss: 2.0642\n",
      "Epoch [2/10], Step [598/938], Loss: 2.0596\n",
      "Epoch [2/10], Step [600/938], Loss: 2.0275\n",
      "Epoch [2/10], Step [602/938], Loss: 2.0475\n",
      "Epoch [2/10], Step [604/938], Loss: 2.0207\n",
      "Epoch [2/10], Step [606/938], Loss: 2.0287\n",
      "Epoch [2/10], Step [608/938], Loss: 2.0254\n",
      "Epoch [2/10], Step [610/938], Loss: 1.9843\n",
      "Epoch [2/10], Step [612/938], Loss: 2.0216\n",
      "Epoch [2/10], Step [614/938], Loss: 2.0095\n",
      "Epoch [2/10], Step [616/938], Loss: 1.9865\n",
      "Epoch [2/10], Step [618/938], Loss: 2.0305\n",
      "Epoch [2/10], Step [620/938], Loss: 2.0245\n",
      "Epoch [2/10], Step [622/938], Loss: 2.0147\n",
      "Epoch [2/10], Step [624/938], Loss: 2.0475\n",
      "Epoch [2/10], Step [626/938], Loss: 2.0397\n",
      "Epoch [2/10], Step [628/938], Loss: 2.0121\n",
      "Epoch [2/10], Step [630/938], Loss: 2.0336\n",
      "Epoch [2/10], Step [632/938], Loss: 2.0103\n",
      "Epoch [2/10], Step [634/938], Loss: 2.0131\n",
      "Epoch [2/10], Step [636/938], Loss: 2.0251\n",
      "Epoch [2/10], Step [638/938], Loss: 2.0151\n",
      "Epoch [2/10], Step [640/938], Loss: 2.0086\n",
      "Epoch [2/10], Step [642/938], Loss: 2.0438\n",
      "Epoch [2/10], Step [644/938], Loss: 2.0659\n",
      "Epoch [2/10], Step [646/938], Loss: 2.0098\n",
      "Epoch [2/10], Step [648/938], Loss: 2.0175\n",
      "Epoch [2/10], Step [650/938], Loss: 2.0752\n",
      "Epoch [2/10], Step [652/938], Loss: 2.0131\n",
      "Epoch [2/10], Step [654/938], Loss: 2.0371\n",
      "Epoch [2/10], Step [656/938], Loss: 2.0319\n",
      "Epoch [2/10], Step [658/938], Loss: 1.9767\n",
      "Epoch [2/10], Step [660/938], Loss: 2.0341\n",
      "Epoch [2/10], Step [662/938], Loss: 2.0447\n",
      "Epoch [2/10], Step [664/938], Loss: 2.0168\n",
      "Epoch [2/10], Step [666/938], Loss: 2.0254\n",
      "Epoch [2/10], Step [668/938], Loss: 2.0480\n",
      "Epoch [2/10], Step [670/938], Loss: 2.0081\n",
      "Epoch [2/10], Step [672/938], Loss: 1.9906\n",
      "Epoch [2/10], Step [674/938], Loss: 1.9662\n",
      "Epoch [2/10], Step [676/938], Loss: 2.0446\n",
      "Epoch [2/10], Step [678/938], Loss: 2.0032\n",
      "Epoch [2/10], Step [680/938], Loss: 1.9953\n",
      "Epoch [2/10], Step [682/938], Loss: 1.9555\n",
      "Epoch [2/10], Step [684/938], Loss: 1.9663\n",
      "Epoch [2/10], Step [686/938], Loss: 1.9856\n",
      "Epoch [2/10], Step [688/938], Loss: 1.9962\n",
      "Epoch [2/10], Step [690/938], Loss: 1.9725\n",
      "Epoch [2/10], Step [692/938], Loss: 1.9815\n",
      "Epoch [2/10], Step [694/938], Loss: 1.9954\n",
      "Epoch [2/10], Step [696/938], Loss: 1.9696\n",
      "Epoch [2/10], Step [698/938], Loss: 1.9579\n",
      "Epoch [2/10], Step [700/938], Loss: 1.9958\n",
      "Epoch [2/10], Step [702/938], Loss: 1.9729\n",
      "Epoch [2/10], Step [704/938], Loss: 1.9770\n",
      "Epoch [2/10], Step [706/938], Loss: 2.0294\n",
      "Epoch [2/10], Step [708/938], Loss: 2.0096\n",
      "Epoch [2/10], Step [710/938], Loss: 1.9964\n",
      "Epoch [2/10], Step [712/938], Loss: 1.9631\n",
      "Epoch [2/10], Step [714/938], Loss: 2.0189\n",
      "Epoch [2/10], Step [716/938], Loss: 1.9469\n",
      "Epoch [2/10], Step [718/938], Loss: 2.0037\n",
      "Epoch [2/10], Step [720/938], Loss: 1.9656\n",
      "Epoch [2/10], Step [722/938], Loss: 1.9716\n",
      "Epoch [2/10], Step [724/938], Loss: 2.0213\n",
      "Epoch [2/10], Step [726/938], Loss: 1.9677\n",
      "Epoch [2/10], Step [728/938], Loss: 1.9393\n",
      "Epoch [2/10], Step [730/938], Loss: 2.0096\n",
      "Epoch [2/10], Step [732/938], Loss: 1.9544\n",
      "Epoch [2/10], Step [734/938], Loss: 1.9415\n",
      "Epoch [2/10], Step [736/938], Loss: 1.9649\n",
      "Epoch [2/10], Step [738/938], Loss: 1.9517\n",
      "Epoch [2/10], Step [740/938], Loss: 1.9617\n",
      "Epoch [2/10], Step [742/938], Loss: 1.9254\n",
      "Epoch [2/10], Step [744/938], Loss: 1.9184\n",
      "Epoch [2/10], Step [746/938], Loss: 1.9336\n",
      "Epoch [2/10], Step [748/938], Loss: 1.9625\n",
      "Epoch [2/10], Step [750/938], Loss: 1.9498\n",
      "Epoch [2/10], Step [752/938], Loss: 1.9500\n",
      "Epoch [2/10], Step [754/938], Loss: 1.9951\n",
      "Epoch [2/10], Step [756/938], Loss: 1.9871\n",
      "Epoch [2/10], Step [758/938], Loss: 1.9623\n",
      "Epoch [2/10], Step [760/938], Loss: 1.9382\n",
      "Epoch [2/10], Step [762/938], Loss: 1.9641\n",
      "Epoch [2/10], Step [764/938], Loss: 1.9964\n",
      "Epoch [2/10], Step [766/938], Loss: 1.9491\n",
      "Epoch [2/10], Step [768/938], Loss: 1.9761\n",
      "Epoch [2/10], Step [770/938], Loss: 1.9720\n",
      "Epoch [2/10], Step [772/938], Loss: 1.9410\n",
      "Epoch [2/10], Step [774/938], Loss: 1.9107\n",
      "Epoch [2/10], Step [776/938], Loss: 1.9323\n",
      "Epoch [2/10], Step [778/938], Loss: 1.9264\n",
      "Epoch [2/10], Step [780/938], Loss: 1.9715\n",
      "Epoch [2/10], Step [782/938], Loss: 1.9216\n",
      "Epoch [2/10], Step [784/938], Loss: 1.9756\n",
      "Epoch [2/10], Step [786/938], Loss: 1.9431\n",
      "Epoch [2/10], Step [788/938], Loss: 1.9639\n",
      "Epoch [2/10], Step [790/938], Loss: 1.9353\n",
      "Epoch [2/10], Step [792/938], Loss: 1.9213\n",
      "Epoch [2/10], Step [794/938], Loss: 1.9343\n",
      "Epoch [2/10], Step [796/938], Loss: 1.9560\n",
      "Epoch [2/10], Step [798/938], Loss: 1.9181\n",
      "Epoch [2/10], Step [800/938], Loss: 2.0079\n",
      "Epoch [2/10], Step [802/938], Loss: 1.9314\n",
      "Epoch [2/10], Step [804/938], Loss: 1.9341\n",
      "Epoch [2/10], Step [806/938], Loss: 1.9205\n",
      "Epoch [2/10], Step [808/938], Loss: 1.9190\n",
      "Epoch [2/10], Step [810/938], Loss: 1.9301\n",
      "Epoch [2/10], Step [812/938], Loss: 1.8717\n",
      "Epoch [2/10], Step [814/938], Loss: 1.8963\n",
      "Epoch [2/10], Step [816/938], Loss: 1.9659\n",
      "Epoch [2/10], Step [818/938], Loss: 1.8836\n",
      "Epoch [2/10], Step [820/938], Loss: 1.9612\n",
      "Epoch [2/10], Step [822/938], Loss: 1.9156\n",
      "Epoch [2/10], Step [824/938], Loss: 1.8721\n",
      "Epoch [2/10], Step [826/938], Loss: 1.9242\n",
      "Epoch [2/10], Step [828/938], Loss: 1.9311\n",
      "Epoch [2/10], Step [830/938], Loss: 1.9100\n",
      "Epoch [2/10], Step [832/938], Loss: 1.8610\n",
      "Epoch [2/10], Step [834/938], Loss: 1.9500\n",
      "Epoch [2/10], Step [836/938], Loss: 1.9439\n",
      "Epoch [2/10], Step [838/938], Loss: 1.9587\n",
      "Epoch [2/10], Step [840/938], Loss: 1.9232\n",
      "Epoch [2/10], Step [842/938], Loss: 1.8815\n",
      "Epoch [2/10], Step [844/938], Loss: 1.9347\n",
      "Epoch [2/10], Step [846/938], Loss: 1.8725\n",
      "Epoch [2/10], Step [848/938], Loss: 1.9605\n",
      "Epoch [2/10], Step [850/938], Loss: 1.9159\n",
      "Epoch [2/10], Step [852/938], Loss: 1.9191\n",
      "Epoch [2/10], Step [854/938], Loss: 1.9033\n",
      "Epoch [2/10], Step [856/938], Loss: 1.9226\n",
      "Epoch [2/10], Step [858/938], Loss: 1.9035\n",
      "Epoch [2/10], Step [860/938], Loss: 1.8856\n",
      "Epoch [2/10], Step [862/938], Loss: 1.9423\n",
      "Epoch [2/10], Step [864/938], Loss: 1.9370\n",
      "Epoch [2/10], Step [866/938], Loss: 1.9011\n",
      "Epoch [2/10], Step [868/938], Loss: 1.9159\n",
      "Epoch [2/10], Step [870/938], Loss: 1.8720\n",
      "Epoch [2/10], Step [872/938], Loss: 1.9107\n",
      "Epoch [2/10], Step [874/938], Loss: 1.8876\n",
      "Epoch [2/10], Step [876/938], Loss: 1.8766\n",
      "Epoch [2/10], Step [878/938], Loss: 1.9027\n",
      "Epoch [2/10], Step [880/938], Loss: 1.9074\n",
      "Epoch [2/10], Step [882/938], Loss: 1.8498\n",
      "Epoch [2/10], Step [884/938], Loss: 1.8520\n",
      "Epoch [2/10], Step [886/938], Loss: 1.8434\n",
      "Epoch [2/10], Step [888/938], Loss: 1.9108\n",
      "Epoch [2/10], Step [890/938], Loss: 1.8999\n",
      "Epoch [2/10], Step [892/938], Loss: 1.8971\n",
      "Epoch [2/10], Step [894/938], Loss: 1.8883\n",
      "Epoch [2/10], Step [896/938], Loss: 1.8591\n",
      "Epoch [2/10], Step [898/938], Loss: 1.8881\n",
      "Epoch [2/10], Step [900/938], Loss: 1.8154\n",
      "Epoch [2/10], Step [902/938], Loss: 1.8528\n",
      "Epoch [2/10], Step [904/938], Loss: 1.8730\n",
      "Epoch [2/10], Step [906/938], Loss: 1.9139\n",
      "Epoch [2/10], Step [908/938], Loss: 1.8759\n",
      "Epoch [2/10], Step [910/938], Loss: 1.8882\n",
      "Epoch [2/10], Step [912/938], Loss: 1.8540\n",
      "Epoch [2/10], Step [914/938], Loss: 1.8652\n",
      "Epoch [2/10], Step [916/938], Loss: 1.8945\n",
      "Epoch [2/10], Step [918/938], Loss: 1.8792\n",
      "Epoch [2/10], Step [920/938], Loss: 1.8617\n",
      "Epoch [2/10], Step [922/938], Loss: 1.8051\n",
      "Epoch [2/10], Step [924/938], Loss: 1.8566\n",
      "Epoch [2/10], Step [926/938], Loss: 1.8410\n",
      "Epoch [2/10], Step [928/938], Loss: 1.8409\n",
      "Epoch [2/10], Step [930/938], Loss: 1.8012\n",
      "Epoch [2/10], Step [932/938], Loss: 1.7999\n",
      "Epoch [2/10], Step [934/938], Loss: 1.8916\n",
      "Epoch [2/10], Step [936/938], Loss: 1.8225\n",
      "Epoch [2/10], Step [938/938], Loss: 1.9207\n",
      "Epoch [2/10], Loss: 2.0656\n",
      "Epoch [3/10], Step [2/938], Loss: 1.8738\n",
      "Epoch [3/10], Step [4/938], Loss: 1.9148\n",
      "Epoch [3/10], Step [6/938], Loss: 1.8253\n",
      "Epoch [3/10], Step [8/938], Loss: 1.8621\n",
      "Epoch [3/10], Step [10/938], Loss: 1.8664\n",
      "Epoch [3/10], Step [12/938], Loss: 1.7857\n",
      "Epoch [3/10], Step [14/938], Loss: 1.8459\n",
      "Epoch [3/10], Step [16/938], Loss: 1.8117\n",
      "Epoch [3/10], Step [18/938], Loss: 1.8540\n",
      "Epoch [3/10], Step [20/938], Loss: 1.8777\n",
      "Epoch [3/10], Step [22/938], Loss: 1.8401\n",
      "Epoch [3/10], Step [24/938], Loss: 1.8777\n",
      "Epoch [3/10], Step [26/938], Loss: 1.7528\n",
      "Epoch [3/10], Step [28/938], Loss: 1.8141\n",
      "Epoch [3/10], Step [30/938], Loss: 1.8125\n",
      "Epoch [3/10], Step [32/938], Loss: 1.8035\n",
      "Epoch [3/10], Step [34/938], Loss: 1.7421\n",
      "Epoch [3/10], Step [36/938], Loss: 1.8289\n",
      "Epoch [3/10], Step [38/938], Loss: 1.7958\n",
      "Epoch [3/10], Step [40/938], Loss: 1.8791\n",
      "Epoch [3/10], Step [42/938], Loss: 1.8022\n",
      "Epoch [3/10], Step [44/938], Loss: 1.8465\n",
      "Epoch [3/10], Step [46/938], Loss: 1.7823\n",
      "Epoch [3/10], Step [48/938], Loss: 1.8447\n",
      "Epoch [3/10], Step [50/938], Loss: 1.8001\n",
      "Epoch [3/10], Step [52/938], Loss: 1.8275\n",
      "Epoch [3/10], Step [54/938], Loss: 1.7976\n",
      "Epoch [3/10], Step [56/938], Loss: 1.8531\n",
      "Epoch [3/10], Step [58/938], Loss: 1.7965\n",
      "Epoch [3/10], Step [60/938], Loss: 1.7862\n",
      "Epoch [3/10], Step [62/938], Loss: 1.7976\n",
      "Epoch [3/10], Step [64/938], Loss: 1.8289\n",
      "Epoch [3/10], Step [66/938], Loss: 1.7995\n",
      "Epoch [3/10], Step [68/938], Loss: 1.8461\n",
      "Epoch [3/10], Step [70/938], Loss: 1.7871\n",
      "Epoch [3/10], Step [72/938], Loss: 1.7872\n",
      "Epoch [3/10], Step [74/938], Loss: 1.7831\n",
      "Epoch [3/10], Step [76/938], Loss: 1.8050\n",
      "Epoch [3/10], Step [78/938], Loss: 1.8456\n",
      "Epoch [3/10], Step [80/938], Loss: 1.7361\n",
      "Epoch [3/10], Step [82/938], Loss: 1.7848\n",
      "Epoch [3/10], Step [84/938], Loss: 1.8229\n",
      "Epoch [3/10], Step [86/938], Loss: 1.8506\n",
      "Epoch [3/10], Step [88/938], Loss: 1.6974\n",
      "Epoch [3/10], Step [90/938], Loss: 1.7996\n",
      "Epoch [3/10], Step [92/938], Loss: 1.7898\n",
      "Epoch [3/10], Step [94/938], Loss: 1.7582\n",
      "Epoch [3/10], Step [96/938], Loss: 1.7557\n",
      "Epoch [3/10], Step [98/938], Loss: 1.7853\n",
      "Epoch [3/10], Step [100/938], Loss: 1.7431\n",
      "Epoch [3/10], Step [102/938], Loss: 1.7900\n",
      "Epoch [3/10], Step [104/938], Loss: 1.7578\n",
      "Epoch [3/10], Step [106/938], Loss: 1.8166\n",
      "Epoch [3/10], Step [108/938], Loss: 1.7309\n",
      "Epoch [3/10], Step [110/938], Loss: 1.7242\n",
      "Epoch [3/10], Step [112/938], Loss: 1.7164\n",
      "Epoch [3/10], Step [114/938], Loss: 1.7674\n",
      "Epoch [3/10], Step [116/938], Loss: 1.7500\n",
      "Epoch [3/10], Step [118/938], Loss: 1.8062\n",
      "Epoch [3/10], Step [120/938], Loss: 1.7697\n",
      "Epoch [3/10], Step [122/938], Loss: 1.7842\n",
      "Epoch [3/10], Step [124/938], Loss: 1.6444\n",
      "Epoch [3/10], Step [126/938], Loss: 1.7423\n",
      "Epoch [3/10], Step [128/938], Loss: 1.6891\n",
      "Epoch [3/10], Step [130/938], Loss: 1.7623\n",
      "Epoch [3/10], Step [132/938], Loss: 1.7860\n",
      "Epoch [3/10], Step [134/938], Loss: 1.7269\n",
      "Epoch [3/10], Step [136/938], Loss: 1.6657\n",
      "Epoch [3/10], Step [138/938], Loss: 1.7419\n",
      "Epoch [3/10], Step [140/938], Loss: 1.7553\n",
      "Epoch [3/10], Step [142/938], Loss: 1.7764\n",
      "Epoch [3/10], Step [144/938], Loss: 1.6806\n",
      "Epoch [3/10], Step [146/938], Loss: 1.6907\n",
      "Epoch [3/10], Step [148/938], Loss: 1.7197\n",
      "Epoch [3/10], Step [150/938], Loss: 1.7515\n",
      "Epoch [3/10], Step [152/938], Loss: 1.6938\n",
      "Epoch [3/10], Step [154/938], Loss: 1.7266\n",
      "Epoch [3/10], Step [156/938], Loss: 1.8146\n",
      "Epoch [3/10], Step [158/938], Loss: 1.6372\n",
      "Epoch [3/10], Step [160/938], Loss: 1.6755\n",
      "Epoch [3/10], Step [162/938], Loss: 1.7527\n",
      "Epoch [3/10], Step [164/938], Loss: 1.7754\n",
      "Epoch [3/10], Step [166/938], Loss: 1.7680\n",
      "Epoch [3/10], Step [168/938], Loss: 1.6818\n",
      "Epoch [3/10], Step [170/938], Loss: 1.7194\n",
      "Epoch [3/10], Step [172/938], Loss: 1.6925\n",
      "Epoch [3/10], Step [174/938], Loss: 1.7459\n",
      "Epoch [3/10], Step [176/938], Loss: 1.6055\n",
      "Epoch [3/10], Step [178/938], Loss: 1.7039\n",
      "Epoch [3/10], Step [180/938], Loss: 1.6590\n",
      "Epoch [3/10], Step [182/938], Loss: 1.7103\n",
      "Epoch [3/10], Step [184/938], Loss: 1.7090\n",
      "Epoch [3/10], Step [186/938], Loss: 1.7368\n",
      "Epoch [3/10], Step [188/938], Loss: 1.6633\n",
      "Epoch [3/10], Step [190/938], Loss: 1.6924\n",
      "Epoch [3/10], Step [192/938], Loss: 1.6755\n",
      "Epoch [3/10], Step [194/938], Loss: 1.6398\n",
      "Epoch [3/10], Step [196/938], Loss: 1.6072\n",
      "Epoch [3/10], Step [198/938], Loss: 1.7072\n",
      "Epoch [3/10], Step [200/938], Loss: 1.6710\n",
      "Epoch [3/10], Step [202/938], Loss: 1.7457\n",
      "Epoch [3/10], Step [204/938], Loss: 1.7316\n",
      "Epoch [3/10], Step [206/938], Loss: 1.5994\n",
      "Epoch [3/10], Step [208/938], Loss: 1.7045\n",
      "Epoch [3/10], Step [210/938], Loss: 1.6471\n",
      "Epoch [3/10], Step [212/938], Loss: 1.6814\n",
      "Epoch [3/10], Step [214/938], Loss: 1.6853\n",
      "Epoch [3/10], Step [216/938], Loss: 1.7124\n",
      "Epoch [3/10], Step [218/938], Loss: 1.6224\n",
      "Epoch [3/10], Step [220/938], Loss: 1.7150\n",
      "Epoch [3/10], Step [222/938], Loss: 1.6942\n",
      "Epoch [3/10], Step [224/938], Loss: 1.6756\n",
      "Epoch [3/10], Step [226/938], Loss: 1.6288\n",
      "Epoch [3/10], Step [228/938], Loss: 1.6056\n",
      "Epoch [3/10], Step [230/938], Loss: 1.6660\n",
      "Epoch [3/10], Step [232/938], Loss: 1.6380\n",
      "Epoch [3/10], Step [234/938], Loss: 1.6282\n",
      "Epoch [3/10], Step [236/938], Loss: 1.6477\n",
      "Epoch [3/10], Step [238/938], Loss: 1.6654\n",
      "Epoch [3/10], Step [240/938], Loss: 1.5661\n",
      "Epoch [3/10], Step [242/938], Loss: 1.5779\n",
      "Epoch [3/10], Step [244/938], Loss: 1.6890\n",
      "Epoch [3/10], Step [246/938], Loss: 1.6065\n",
      "Epoch [3/10], Step [248/938], Loss: 1.6338\n",
      "Epoch [3/10], Step [250/938], Loss: 1.6657\n",
      "Epoch [3/10], Step [252/938], Loss: 1.5155\n",
      "Epoch [3/10], Step [254/938], Loss: 1.6618\n",
      "Epoch [3/10], Step [256/938], Loss: 1.6473\n",
      "Epoch [3/10], Step [258/938], Loss: 1.7639\n",
      "Epoch [3/10], Step [260/938], Loss: 1.6495\n",
      "Epoch [3/10], Step [262/938], Loss: 1.6166\n",
      "Epoch [3/10], Step [264/938], Loss: 1.6366\n",
      "Epoch [3/10], Step [266/938], Loss: 1.5823\n",
      "Epoch [3/10], Step [268/938], Loss: 1.6070\n",
      "Epoch [3/10], Step [270/938], Loss: 1.5584\n",
      "Epoch [3/10], Step [272/938], Loss: 1.6132\n",
      "Epoch [3/10], Step [274/938], Loss: 1.5879\n",
      "Epoch [3/10], Step [276/938], Loss: 1.6079\n",
      "Epoch [3/10], Step [278/938], Loss: 1.6272\n",
      "Epoch [3/10], Step [280/938], Loss: 1.6049\n",
      "Epoch [3/10], Step [282/938], Loss: 1.5913\n",
      "Epoch [3/10], Step [284/938], Loss: 1.7195\n",
      "Epoch [3/10], Step [286/938], Loss: 1.5694\n",
      "Epoch [3/10], Step [288/938], Loss: 1.5710\n",
      "Epoch [3/10], Step [290/938], Loss: 1.5574\n",
      "Epoch [3/10], Step [292/938], Loss: 1.6144\n",
      "Epoch [3/10], Step [294/938], Loss: 1.5822\n",
      "Epoch [3/10], Step [296/938], Loss: 1.4929\n",
      "Epoch [3/10], Step [298/938], Loss: 1.5775\n",
      "Epoch [3/10], Step [300/938], Loss: 1.5744\n",
      "Epoch [3/10], Step [302/938], Loss: 1.5030\n",
      "Epoch [3/10], Step [304/938], Loss: 1.5474\n",
      "Epoch [3/10], Step [306/938], Loss: 1.5643\n",
      "Epoch [3/10], Step [308/938], Loss: 1.6041\n",
      "Epoch [3/10], Step [310/938], Loss: 1.5546\n",
      "Epoch [3/10], Step [312/938], Loss: 1.5902\n",
      "Epoch [3/10], Step [314/938], Loss: 1.5585\n",
      "Epoch [3/10], Step [316/938], Loss: 1.4917\n",
      "Epoch [3/10], Step [318/938], Loss: 1.6359\n",
      "Epoch [3/10], Step [320/938], Loss: 1.5479\n",
      "Epoch [3/10], Step [322/938], Loss: 1.6342\n",
      "Epoch [3/10], Step [324/938], Loss: 1.5723\n",
      "Epoch [3/10], Step [326/938], Loss: 1.7012\n",
      "Epoch [3/10], Step [328/938], Loss: 1.4500\n",
      "Epoch [3/10], Step [330/938], Loss: 1.5812\n",
      "Epoch [3/10], Step [332/938], Loss: 1.4438\n",
      "Epoch [3/10], Step [334/938], Loss: 1.5356\n",
      "Epoch [3/10], Step [336/938], Loss: 1.5231\n",
      "Epoch [3/10], Step [338/938], Loss: 1.5116\n",
      "Epoch [3/10], Step [340/938], Loss: 1.5385\n",
      "Epoch [3/10], Step [342/938], Loss: 1.4666\n",
      "Epoch [3/10], Step [344/938], Loss: 1.4809\n",
      "Epoch [3/10], Step [346/938], Loss: 1.5284\n",
      "Epoch [3/10], Step [348/938], Loss: 1.4377\n",
      "Epoch [3/10], Step [350/938], Loss: 1.5569\n",
      "Epoch [3/10], Step [352/938], Loss: 1.5801\n",
      "Epoch [3/10], Step [354/938], Loss: 1.5823\n",
      "Epoch [3/10], Step [356/938], Loss: 1.4495\n",
      "Epoch [3/10], Step [358/938], Loss: 1.5535\n",
      "Epoch [3/10], Step [360/938], Loss: 1.5916\n",
      "Epoch [3/10], Step [362/938], Loss: 1.4883\n",
      "Epoch [3/10], Step [364/938], Loss: 1.4786\n",
      "Epoch [3/10], Step [366/938], Loss: 1.4910\n",
      "Epoch [3/10], Step [368/938], Loss: 1.4751\n",
      "Epoch [3/10], Step [370/938], Loss: 1.5208\n",
      "Epoch [3/10], Step [372/938], Loss: 1.4876\n",
      "Epoch [3/10], Step [374/938], Loss: 1.4640\n",
      "Epoch [3/10], Step [376/938], Loss: 1.4841\n",
      "Epoch [3/10], Step [378/938], Loss: 1.5786\n",
      "Epoch [3/10], Step [380/938], Loss: 1.5032\n",
      "Epoch [3/10], Step [382/938], Loss: 1.4883\n",
      "Epoch [3/10], Step [384/938], Loss: 1.4154\n",
      "Epoch [3/10], Step [386/938], Loss: 1.5559\n",
      "Epoch [3/10], Step [388/938], Loss: 1.5010\n",
      "Epoch [3/10], Step [390/938], Loss: 1.4876\n",
      "Epoch [3/10], Step [392/938], Loss: 1.5748\n",
      "Epoch [3/10], Step [394/938], Loss: 1.4273\n",
      "Epoch [3/10], Step [396/938], Loss: 1.6453\n",
      "Epoch [3/10], Step [398/938], Loss: 1.4941\n",
      "Epoch [3/10], Step [400/938], Loss: 1.4958\n",
      "Epoch [3/10], Step [402/938], Loss: 1.4959\n",
      "Epoch [3/10], Step [404/938], Loss: 1.5688\n",
      "Epoch [3/10], Step [406/938], Loss: 1.3857\n",
      "Epoch [3/10], Step [408/938], Loss: 1.3448\n",
      "Epoch [3/10], Step [410/938], Loss: 1.4822\n",
      "Epoch [3/10], Step [412/938], Loss: 1.4643\n",
      "Epoch [3/10], Step [414/938], Loss: 1.5590\n",
      "Epoch [3/10], Step [416/938], Loss: 1.3633\n",
      "Epoch [3/10], Step [418/938], Loss: 1.4657\n",
      "Epoch [3/10], Step [420/938], Loss: 1.4905\n",
      "Epoch [3/10], Step [422/938], Loss: 1.3954\n",
      "Epoch [3/10], Step [424/938], Loss: 1.4754\n",
      "Epoch [3/10], Step [426/938], Loss: 1.4697\n",
      "Epoch [3/10], Step [428/938], Loss: 1.4425\n",
      "Epoch [3/10], Step [430/938], Loss: 1.4554\n",
      "Epoch [3/10], Step [432/938], Loss: 1.4647\n",
      "Epoch [3/10], Step [434/938], Loss: 1.4785\n",
      "Epoch [3/10], Step [436/938], Loss: 1.3757\n",
      "Epoch [3/10], Step [438/938], Loss: 1.4076\n",
      "Epoch [3/10], Step [440/938], Loss: 1.4395\n",
      "Epoch [3/10], Step [442/938], Loss: 1.4335\n",
      "Epoch [3/10], Step [444/938], Loss: 1.4197\n",
      "Epoch [3/10], Step [446/938], Loss: 1.4343\n",
      "Epoch [3/10], Step [448/938], Loss: 1.4039\n",
      "Epoch [3/10], Step [450/938], Loss: 1.4487\n",
      "Epoch [3/10], Step [452/938], Loss: 1.4151\n",
      "Epoch [3/10], Step [454/938], Loss: 1.4198\n",
      "Epoch [3/10], Step [456/938], Loss: 1.4657\n",
      "Epoch [3/10], Step [458/938], Loss: 1.3609\n",
      "Epoch [3/10], Step [460/938], Loss: 1.4011\n",
      "Epoch [3/10], Step [462/938], Loss: 1.4289\n",
      "Epoch [3/10], Step [464/938], Loss: 1.3802\n",
      "Epoch [3/10], Step [466/938], Loss: 1.4986\n",
      "Epoch [3/10], Step [468/938], Loss: 1.3538\n",
      "Epoch [3/10], Step [470/938], Loss: 1.4333\n",
      "Epoch [3/10], Step [472/938], Loss: 1.3753\n",
      "Epoch [3/10], Step [474/938], Loss: 1.4341\n",
      "Epoch [3/10], Step [476/938], Loss: 1.4401\n",
      "Epoch [3/10], Step [478/938], Loss: 1.3387\n",
      "Epoch [3/10], Step [480/938], Loss: 1.3957\n",
      "Epoch [3/10], Step [482/938], Loss: 1.3631\n",
      "Epoch [3/10], Step [484/938], Loss: 1.4354\n",
      "Epoch [3/10], Step [486/938], Loss: 1.4666\n",
      "Epoch [3/10], Step [488/938], Loss: 1.4524\n",
      "Epoch [3/10], Step [490/938], Loss: 1.4400\n",
      "Epoch [3/10], Step [492/938], Loss: 1.3878\n",
      "Epoch [3/10], Step [494/938], Loss: 1.2662\n",
      "Epoch [3/10], Step [496/938], Loss: 1.4157\n",
      "Epoch [3/10], Step [498/938], Loss: 1.4126\n",
      "Epoch [3/10], Step [500/938], Loss: 1.3070\n",
      "Epoch [3/10], Step [502/938], Loss: 1.3767\n",
      "Epoch [3/10], Step [504/938], Loss: 1.4656\n",
      "Epoch [3/10], Step [506/938], Loss: 1.4402\n",
      "Epoch [3/10], Step [508/938], Loss: 1.3827\n",
      "Epoch [3/10], Step [510/938], Loss: 1.3543\n",
      "Epoch [3/10], Step [512/938], Loss: 1.3599\n",
      "Epoch [3/10], Step [514/938], Loss: 1.3267\n",
      "Epoch [3/10], Step [516/938], Loss: 1.4581\n",
      "Epoch [3/10], Step [518/938], Loss: 1.2597\n",
      "Epoch [3/10], Step [520/938], Loss: 1.3944\n",
      "Epoch [3/10], Step [522/938], Loss: 1.1613\n",
      "Epoch [3/10], Step [524/938], Loss: 1.4474\n",
      "Epoch [3/10], Step [526/938], Loss: 1.3155\n",
      "Epoch [3/10], Step [528/938], Loss: 1.4039\n",
      "Epoch [3/10], Step [530/938], Loss: 1.3388\n",
      "Epoch [3/10], Step [532/938], Loss: 1.3230\n",
      "Epoch [3/10], Step [534/938], Loss: 1.3109\n",
      "Epoch [3/10], Step [536/938], Loss: 1.3168\n",
      "Epoch [3/10], Step [538/938], Loss: 1.4031\n",
      "Epoch [3/10], Step [540/938], Loss: 1.2570\n",
      "Epoch [3/10], Step [542/938], Loss: 1.3734\n",
      "Epoch [3/10], Step [544/938], Loss: 1.2041\n",
      "Epoch [3/10], Step [546/938], Loss: 1.2804\n",
      "Epoch [3/10], Step [548/938], Loss: 1.2234\n",
      "Epoch [3/10], Step [550/938], Loss: 1.3644\n",
      "Epoch [3/10], Step [552/938], Loss: 1.2406\n",
      "Epoch [3/10], Step [554/938], Loss: 1.4055\n",
      "Epoch [3/10], Step [556/938], Loss: 1.3922\n",
      "Epoch [3/10], Step [558/938], Loss: 1.4331\n",
      "Epoch [3/10], Step [560/938], Loss: 1.1938\n",
      "Epoch [3/10], Step [562/938], Loss: 1.2955\n",
      "Epoch [3/10], Step [564/938], Loss: 1.2838\n",
      "Epoch [3/10], Step [566/938], Loss: 1.2810\n",
      "Epoch [3/10], Step [568/938], Loss: 1.2943\n",
      "Epoch [3/10], Step [570/938], Loss: 1.3455\n",
      "Epoch [3/10], Step [572/938], Loss: 1.1887\n",
      "Epoch [3/10], Step [574/938], Loss: 1.2779\n",
      "Epoch [3/10], Step [576/938], Loss: 1.3184\n",
      "Epoch [3/10], Step [578/938], Loss: 1.3284\n",
      "Epoch [3/10], Step [580/938], Loss: 1.1351\n",
      "Epoch [3/10], Step [582/938], Loss: 1.2054\n",
      "Epoch [3/10], Step [584/938], Loss: 1.2657\n",
      "Epoch [3/10], Step [586/938], Loss: 1.3719\n",
      "Epoch [3/10], Step [588/938], Loss: 1.3315\n",
      "Epoch [3/10], Step [590/938], Loss: 1.2508\n",
      "Epoch [3/10], Step [592/938], Loss: 1.1326\n",
      "Epoch [3/10], Step [594/938], Loss: 1.1789\n",
      "Epoch [3/10], Step [596/938], Loss: 1.2539\n",
      "Epoch [3/10], Step [598/938], Loss: 1.2784\n",
      "Epoch [3/10], Step [600/938], Loss: 1.2667\n",
      "Epoch [3/10], Step [602/938], Loss: 1.0593\n",
      "Epoch [3/10], Step [604/938], Loss: 1.2784\n",
      "Epoch [3/10], Step [606/938], Loss: 1.2893\n",
      "Epoch [3/10], Step [608/938], Loss: 1.2887\n",
      "Epoch [3/10], Step [610/938], Loss: 1.2325\n",
      "Epoch [3/10], Step [612/938], Loss: 1.2089\n",
      "Epoch [3/10], Step [614/938], Loss: 1.2980\n",
      "Epoch [3/10], Step [616/938], Loss: 1.3657\n",
      "Epoch [3/10], Step [618/938], Loss: 1.2181\n",
      "Epoch [3/10], Step [620/938], Loss: 1.2270\n",
      "Epoch [3/10], Step [622/938], Loss: 1.1794\n",
      "Epoch [3/10], Step [624/938], Loss: 1.1484\n",
      "Epoch [3/10], Step [626/938], Loss: 1.2101\n",
      "Epoch [3/10], Step [628/938], Loss: 1.3114\n",
      "Epoch [3/10], Step [630/938], Loss: 1.2487\n",
      "Epoch [3/10], Step [632/938], Loss: 1.2359\n",
      "Epoch [3/10], Step [634/938], Loss: 1.2138\n",
      "Epoch [3/10], Step [636/938], Loss: 1.1736\n",
      "Epoch [3/10], Step [638/938], Loss: 1.1774\n",
      "Epoch [3/10], Step [640/938], Loss: 1.3296\n",
      "Epoch [3/10], Step [642/938], Loss: 1.1781\n",
      "Epoch [3/10], Step [644/938], Loss: 1.2097\n",
      "Epoch [3/10], Step [646/938], Loss: 1.1833\n",
      "Epoch [3/10], Step [648/938], Loss: 1.2190\n",
      "Epoch [3/10], Step [650/938], Loss: 1.2387\n",
      "Epoch [3/10], Step [652/938], Loss: 1.2379\n",
      "Epoch [3/10], Step [654/938], Loss: 1.2033\n",
      "Epoch [3/10], Step [656/938], Loss: 1.3058\n",
      "Epoch [3/10], Step [658/938], Loss: 1.1914\n",
      "Epoch [3/10], Step [660/938], Loss: 1.2452\n",
      "Epoch [3/10], Step [662/938], Loss: 1.2356\n",
      "Epoch [3/10], Step [664/938], Loss: 1.1949\n",
      "Epoch [3/10], Step [666/938], Loss: 1.3078\n",
      "Epoch [3/10], Step [668/938], Loss: 1.2715\n",
      "Epoch [3/10], Step [670/938], Loss: 1.2157\n",
      "Epoch [3/10], Step [672/938], Loss: 1.2510\n",
      "Epoch [3/10], Step [674/938], Loss: 1.1997\n",
      "Epoch [3/10], Step [676/938], Loss: 1.1815\n",
      "Epoch [3/10], Step [678/938], Loss: 1.2189\n",
      "Epoch [3/10], Step [680/938], Loss: 1.0921\n",
      "Epoch [3/10], Step [682/938], Loss: 1.2621\n",
      "Epoch [3/10], Step [684/938], Loss: 1.1424\n",
      "Epoch [3/10], Step [686/938], Loss: 1.2763\n",
      "Epoch [3/10], Step [688/938], Loss: 1.1546\n",
      "Epoch [3/10], Step [690/938], Loss: 1.1729\n",
      "Epoch [3/10], Step [692/938], Loss: 1.0652\n",
      "Epoch [3/10], Step [694/938], Loss: 1.1457\n",
      "Epoch [3/10], Step [696/938], Loss: 1.1251\n",
      "Epoch [3/10], Step [698/938], Loss: 1.1996\n",
      "Epoch [3/10], Step [700/938], Loss: 1.1550\n",
      "Epoch [3/10], Step [702/938], Loss: 1.0380\n",
      "Epoch [3/10], Step [704/938], Loss: 1.0718\n",
      "Epoch [3/10], Step [706/938], Loss: 1.0955\n",
      "Epoch [3/10], Step [708/938], Loss: 1.1127\n",
      "Epoch [3/10], Step [710/938], Loss: 1.1593\n",
      "Epoch [3/10], Step [712/938], Loss: 1.1688\n",
      "Epoch [3/10], Step [714/938], Loss: 1.1756\n",
      "Epoch [3/10], Step [716/938], Loss: 1.0851\n",
      "Epoch [3/10], Step [718/938], Loss: 1.1802\n",
      "Epoch [3/10], Step [720/938], Loss: 1.1019\n",
      "Epoch [3/10], Step [722/938], Loss: 1.0986\n",
      "Epoch [3/10], Step [724/938], Loss: 1.1553\n",
      "Epoch [3/10], Step [726/938], Loss: 1.0479\n",
      "Epoch [3/10], Step [728/938], Loss: 1.1149\n",
      "Epoch [3/10], Step [730/938], Loss: 1.0689\n",
      "Epoch [3/10], Step [732/938], Loss: 1.1144\n",
      "Epoch [3/10], Step [734/938], Loss: 1.1838\n",
      "Epoch [3/10], Step [736/938], Loss: 0.9461\n",
      "Epoch [3/10], Step [738/938], Loss: 1.1340\n",
      "Epoch [3/10], Step [740/938], Loss: 1.1090\n",
      "Epoch [3/10], Step [742/938], Loss: 1.1034\n",
      "Epoch [3/10], Step [744/938], Loss: 1.2088\n",
      "Epoch [3/10], Step [746/938], Loss: 1.2488\n",
      "Epoch [3/10], Step [748/938], Loss: 1.1284\n",
      "Epoch [3/10], Step [750/938], Loss: 1.0577\n",
      "Epoch [3/10], Step [752/938], Loss: 1.1891\n",
      "Epoch [3/10], Step [754/938], Loss: 1.0702\n",
      "Epoch [3/10], Step [756/938], Loss: 1.0869\n",
      "Epoch [3/10], Step [758/938], Loss: 1.1010\n",
      "Epoch [3/10], Step [760/938], Loss: 1.3017\n",
      "Epoch [3/10], Step [762/938], Loss: 1.1331\n",
      "Epoch [3/10], Step [764/938], Loss: 1.0823\n",
      "Epoch [3/10], Step [766/938], Loss: 1.1867\n",
      "Epoch [3/10], Step [768/938], Loss: 1.1340\n",
      "Epoch [3/10], Step [770/938], Loss: 1.0746\n",
      "Epoch [3/10], Step [772/938], Loss: 1.1429\n",
      "Epoch [3/10], Step [774/938], Loss: 1.1528\n",
      "Epoch [3/10], Step [776/938], Loss: 1.0591\n",
      "Epoch [3/10], Step [778/938], Loss: 1.1455\n",
      "Epoch [3/10], Step [780/938], Loss: 1.1248\n",
      "Epoch [3/10], Step [782/938], Loss: 1.0789\n",
      "Epoch [3/10], Step [784/938], Loss: 1.0861\n",
      "Epoch [3/10], Step [786/938], Loss: 1.2668\n",
      "Epoch [3/10], Step [788/938], Loss: 1.1237\n",
      "Epoch [3/10], Step [790/938], Loss: 1.0703\n",
      "Epoch [3/10], Step [792/938], Loss: 1.0477\n",
      "Epoch [3/10], Step [794/938], Loss: 1.1401\n",
      "Epoch [3/10], Step [796/938], Loss: 1.0688\n",
      "Epoch [3/10], Step [798/938], Loss: 0.9882\n",
      "Epoch [3/10], Step [800/938], Loss: 0.9782\n",
      "Epoch [3/10], Step [802/938], Loss: 1.0056\n",
      "Epoch [3/10], Step [804/938], Loss: 1.1417\n",
      "Epoch [3/10], Step [806/938], Loss: 1.0130\n",
      "Epoch [3/10], Step [808/938], Loss: 1.1809\n",
      "Epoch [3/10], Step [810/938], Loss: 1.0618\n",
      "Epoch [3/10], Step [812/938], Loss: 1.1781\n",
      "Epoch [3/10], Step [814/938], Loss: 1.0736\n",
      "Epoch [3/10], Step [816/938], Loss: 1.0777\n",
      "Epoch [3/10], Step [818/938], Loss: 1.0643\n",
      "Epoch [3/10], Step [820/938], Loss: 1.0336\n",
      "Epoch [3/10], Step [822/938], Loss: 1.0305\n",
      "Epoch [3/10], Step [824/938], Loss: 1.0347\n",
      "Epoch [3/10], Step [826/938], Loss: 1.1164\n",
      "Epoch [3/10], Step [828/938], Loss: 1.0014\n",
      "Epoch [3/10], Step [830/938], Loss: 1.1649\n",
      "Epoch [3/10], Step [832/938], Loss: 0.9288\n",
      "Epoch [3/10], Step [834/938], Loss: 1.0022\n",
      "Epoch [3/10], Step [836/938], Loss: 1.0266\n",
      "Epoch [3/10], Step [838/938], Loss: 0.9720\n",
      "Epoch [3/10], Step [840/938], Loss: 1.0826\n",
      "Epoch [3/10], Step [842/938], Loss: 0.9627\n",
      "Epoch [3/10], Step [844/938], Loss: 1.0979\n",
      "Epoch [3/10], Step [846/938], Loss: 1.1023\n",
      "Epoch [3/10], Step [848/938], Loss: 0.9970\n",
      "Epoch [3/10], Step [850/938], Loss: 1.0344\n",
      "Epoch [3/10], Step [852/938], Loss: 1.0508\n",
      "Epoch [3/10], Step [854/938], Loss: 1.0547\n",
      "Epoch [3/10], Step [856/938], Loss: 1.0848\n",
      "Epoch [3/10], Step [858/938], Loss: 1.0080\n",
      "Epoch [3/10], Step [860/938], Loss: 1.0830\n",
      "Epoch [3/10], Step [862/938], Loss: 1.0976\n",
      "Epoch [3/10], Step [864/938], Loss: 0.8460\n",
      "Epoch [3/10], Step [866/938], Loss: 0.9621\n",
      "Epoch [3/10], Step [868/938], Loss: 0.9606\n",
      "Epoch [3/10], Step [870/938], Loss: 0.9805\n",
      "Epoch [3/10], Step [872/938], Loss: 0.9743\n",
      "Epoch [3/10], Step [874/938], Loss: 0.8562\n",
      "Epoch [3/10], Step [876/938], Loss: 1.0644\n",
      "Epoch [3/10], Step [878/938], Loss: 0.8496\n",
      "Epoch [3/10], Step [880/938], Loss: 0.8787\n",
      "Epoch [3/10], Step [882/938], Loss: 0.9674\n",
      "Epoch [3/10], Step [884/938], Loss: 1.0121\n",
      "Epoch [3/10], Step [886/938], Loss: 0.9659\n",
      "Epoch [3/10], Step [888/938], Loss: 0.9261\n",
      "Epoch [3/10], Step [890/938], Loss: 0.9364\n",
      "Epoch [3/10], Step [892/938], Loss: 0.9081\n",
      "Epoch [3/10], Step [894/938], Loss: 1.0386\n",
      "Epoch [3/10], Step [896/938], Loss: 0.9366\n",
      "Epoch [3/10], Step [898/938], Loss: 0.9982\n",
      "Epoch [3/10], Step [900/938], Loss: 0.9981\n",
      "Epoch [3/10], Step [902/938], Loss: 0.9761\n",
      "Epoch [3/10], Step [904/938], Loss: 0.9292\n",
      "Epoch [3/10], Step [906/938], Loss: 0.8683\n",
      "Epoch [3/10], Step [908/938], Loss: 0.8885\n",
      "Epoch [3/10], Step [910/938], Loss: 1.1123\n",
      "Epoch [3/10], Step [912/938], Loss: 1.0064\n",
      "Epoch [3/10], Step [914/938], Loss: 1.0243\n",
      "Epoch [3/10], Step [916/938], Loss: 0.9917\n",
      "Epoch [3/10], Step [918/938], Loss: 1.0159\n",
      "Epoch [3/10], Step [920/938], Loss: 0.8578\n",
      "Epoch [3/10], Step [922/938], Loss: 0.9620\n",
      "Epoch [3/10], Step [924/938], Loss: 0.9181\n",
      "Epoch [3/10], Step [926/938], Loss: 0.8691\n",
      "Epoch [3/10], Step [928/938], Loss: 0.9456\n",
      "Epoch [3/10], Step [930/938], Loss: 0.8707\n",
      "Epoch [3/10], Step [932/938], Loss: 0.9183\n",
      "Epoch [3/10], Step [934/938], Loss: 1.1226\n",
      "Epoch [3/10], Step [936/938], Loss: 0.9829\n",
      "Epoch [3/10], Step [938/938], Loss: 1.0242\n",
      "Epoch [3/10], Loss: 1.4006\n",
      "Epoch [4/10], Step [2/938], Loss: 0.9329\n",
      "Epoch [4/10], Step [4/938], Loss: 1.0278\n",
      "Epoch [4/10], Step [6/938], Loss: 0.9117\n",
      "Epoch [4/10], Step [8/938], Loss: 0.9889\n",
      "Epoch [4/10], Step [10/938], Loss: 0.9962\n",
      "Epoch [4/10], Step [12/938], Loss: 0.9864\n",
      "Epoch [4/10], Step [14/938], Loss: 0.9786\n",
      "Epoch [4/10], Step [16/938], Loss: 0.9029\n",
      "Epoch [4/10], Step [18/938], Loss: 0.8440\n",
      "Epoch [4/10], Step [20/938], Loss: 0.9506\n",
      "Epoch [4/10], Step [22/938], Loss: 1.0299\n",
      "Epoch [4/10], Step [24/938], Loss: 0.9031\n",
      "Epoch [4/10], Step [26/938], Loss: 0.8118\n",
      "Epoch [4/10], Step [28/938], Loss: 1.0548\n",
      "Epoch [4/10], Step [30/938], Loss: 0.9728\n",
      "Epoch [4/10], Step [32/938], Loss: 1.0780\n",
      "Epoch [4/10], Step [34/938], Loss: 0.9378\n",
      "Epoch [4/10], Step [36/938], Loss: 0.8795\n",
      "Epoch [4/10], Step [38/938], Loss: 0.8798\n",
      "Epoch [4/10], Step [40/938], Loss: 0.8728\n",
      "Epoch [4/10], Step [42/938], Loss: 0.8519\n",
      "Epoch [4/10], Step [44/938], Loss: 0.9305\n",
      "Epoch [4/10], Step [46/938], Loss: 1.0017\n",
      "Epoch [4/10], Step [48/938], Loss: 1.0103\n",
      "Epoch [4/10], Step [50/938], Loss: 0.8989\n",
      "Epoch [4/10], Step [52/938], Loss: 0.8327\n",
      "Epoch [4/10], Step [54/938], Loss: 0.8770\n",
      "Epoch [4/10], Step [56/938], Loss: 1.0852\n",
      "Epoch [4/10], Step [58/938], Loss: 0.9963\n",
      "Epoch [4/10], Step [60/938], Loss: 0.9370\n",
      "Epoch [4/10], Step [62/938], Loss: 0.9949\n",
      "Epoch [4/10], Step [64/938], Loss: 0.9907\n",
      "Epoch [4/10], Step [66/938], Loss: 1.0012\n",
      "Epoch [4/10], Step [68/938], Loss: 0.8631\n",
      "Epoch [4/10], Step [70/938], Loss: 0.8677\n",
      "Epoch [4/10], Step [72/938], Loss: 0.9105\n",
      "Epoch [4/10], Step [74/938], Loss: 0.7516\n",
      "Epoch [4/10], Step [76/938], Loss: 0.8374\n",
      "Epoch [4/10], Step [78/938], Loss: 0.9610\n",
      "Epoch [4/10], Step [80/938], Loss: 0.9418\n",
      "Epoch [4/10], Step [82/938], Loss: 0.7896\n",
      "Epoch [4/10], Step [84/938], Loss: 0.8637\n",
      "Epoch [4/10], Step [86/938], Loss: 0.9147\n",
      "Epoch [4/10], Step [88/938], Loss: 0.9226\n",
      "Epoch [4/10], Step [90/938], Loss: 0.8346\n",
      "Epoch [4/10], Step [92/938], Loss: 0.8230\n",
      "Epoch [4/10], Step [94/938], Loss: 0.7322\n",
      "Epoch [4/10], Step [96/938], Loss: 0.8910\n",
      "Epoch [4/10], Step [98/938], Loss: 0.9469\n",
      "Epoch [4/10], Step [100/938], Loss: 0.9456\n",
      "Epoch [4/10], Step [102/938], Loss: 0.9406\n",
      "Epoch [4/10], Step [104/938], Loss: 0.7871\n",
      "Epoch [4/10], Step [106/938], Loss: 0.9786\n",
      "Epoch [4/10], Step [108/938], Loss: 0.9353\n",
      "Epoch [4/10], Step [110/938], Loss: 0.7763\n",
      "Epoch [4/10], Step [112/938], Loss: 0.8227\n",
      "Epoch [4/10], Step [114/938], Loss: 0.8960\n",
      "Epoch [4/10], Step [116/938], Loss: 0.7747\n",
      "Epoch [4/10], Step [118/938], Loss: 0.8455\n",
      "Epoch [4/10], Step [120/938], Loss: 0.8675\n",
      "Epoch [4/10], Step [122/938], Loss: 0.7515\n",
      "Epoch [4/10], Step [124/938], Loss: 0.8391\n",
      "Epoch [4/10], Step [126/938], Loss: 0.8836\n",
      "Epoch [4/10], Step [128/938], Loss: 0.7961\n",
      "Epoch [4/10], Step [130/938], Loss: 0.8686\n",
      "Epoch [4/10], Step [132/938], Loss: 0.9862\n",
      "Epoch [4/10], Step [134/938], Loss: 0.7711\n",
      "Epoch [4/10], Step [136/938], Loss: 0.8247\n",
      "Epoch [4/10], Step [138/938], Loss: 0.9083\n",
      "Epoch [4/10], Step [140/938], Loss: 0.9509\n",
      "Epoch [4/10], Step [142/938], Loss: 0.7640\n",
      "Epoch [4/10], Step [144/938], Loss: 0.8799\n",
      "Epoch [4/10], Step [146/938], Loss: 0.9922\n",
      "Epoch [4/10], Step [148/938], Loss: 0.9378\n",
      "Epoch [4/10], Step [150/938], Loss: 0.8066\n",
      "Epoch [4/10], Step [152/938], Loss: 0.8427\n",
      "Epoch [4/10], Step [154/938], Loss: 0.8989\n",
      "Epoch [4/10], Step [156/938], Loss: 0.7474\n",
      "Epoch [4/10], Step [158/938], Loss: 0.8681\n",
      "Epoch [4/10], Step [160/938], Loss: 0.8970\n",
      "Epoch [4/10], Step [162/938], Loss: 0.9592\n",
      "Epoch [4/10], Step [164/938], Loss: 0.8816\n",
      "Epoch [4/10], Step [166/938], Loss: 0.7007\n",
      "Epoch [4/10], Step [168/938], Loss: 0.8386\n",
      "Epoch [4/10], Step [170/938], Loss: 0.7544\n",
      "Epoch [4/10], Step [172/938], Loss: 0.9428\n",
      "Epoch [4/10], Step [174/938], Loss: 0.9262\n",
      "Epoch [4/10], Step [176/938], Loss: 0.7480\n",
      "Epoch [4/10], Step [178/938], Loss: 0.9791\n",
      "Epoch [4/10], Step [180/938], Loss: 0.8178\n",
      "Epoch [4/10], Step [182/938], Loss: 0.8139\n",
      "Epoch [4/10], Step [184/938], Loss: 0.8976\n",
      "Epoch [4/10], Step [186/938], Loss: 0.8312\n",
      "Epoch [4/10], Step [188/938], Loss: 0.7785\n",
      "Epoch [4/10], Step [190/938], Loss: 0.8719\n",
      "Epoch [4/10], Step [192/938], Loss: 0.8130\n",
      "Epoch [4/10], Step [194/938], Loss: 0.8676\n",
      "Epoch [4/10], Step [196/938], Loss: 0.8692\n",
      "Epoch [4/10], Step [198/938], Loss: 0.7204\n",
      "Epoch [4/10], Step [200/938], Loss: 0.7257\n",
      "Epoch [4/10], Step [202/938], Loss: 0.8504\n",
      "Epoch [4/10], Step [204/938], Loss: 0.8901\n",
      "Epoch [4/10], Step [206/938], Loss: 0.8154\n",
      "Epoch [4/10], Step [208/938], Loss: 0.8685\n",
      "Epoch [4/10], Step [210/938], Loss: 0.7393\n",
      "Epoch [4/10], Step [212/938], Loss: 0.7801\n",
      "Epoch [4/10], Step [214/938], Loss: 0.8315\n",
      "Epoch [4/10], Step [216/938], Loss: 0.7324\n",
      "Epoch [4/10], Step [218/938], Loss: 0.8375\n",
      "Epoch [4/10], Step [220/938], Loss: 0.7170\n",
      "Epoch [4/10], Step [222/938], Loss: 0.8037\n",
      "Epoch [4/10], Step [224/938], Loss: 0.7492\n",
      "Epoch [4/10], Step [226/938], Loss: 0.7333\n",
      "Epoch [4/10], Step [228/938], Loss: 0.8342\n",
      "Epoch [4/10], Step [230/938], Loss: 0.7978\n",
      "Epoch [4/10], Step [232/938], Loss: 0.7447\n",
      "Epoch [4/10], Step [234/938], Loss: 0.8093\n",
      "Epoch [4/10], Step [236/938], Loss: 0.7339\n",
      "Epoch [4/10], Step [238/938], Loss: 0.6939\n",
      "Epoch [4/10], Step [240/938], Loss: 0.7690\n",
      "Epoch [4/10], Step [242/938], Loss: 0.8485\n",
      "Epoch [4/10], Step [244/938], Loss: 0.8215\n",
      "Epoch [4/10], Step [246/938], Loss: 0.8433\n",
      "Epoch [4/10], Step [248/938], Loss: 0.7653\n",
      "Epoch [4/10], Step [250/938], Loss: 0.8122\n",
      "Epoch [4/10], Step [252/938], Loss: 0.7645\n",
      "Epoch [4/10], Step [254/938], Loss: 0.8151\n",
      "Epoch [4/10], Step [256/938], Loss: 0.9095\n",
      "Epoch [4/10], Step [258/938], Loss: 0.6391\n",
      "Epoch [4/10], Step [260/938], Loss: 0.6903\n",
      "Epoch [4/10], Step [262/938], Loss: 0.8297\n",
      "Epoch [4/10], Step [264/938], Loss: 0.7814\n",
      "Epoch [4/10], Step [266/938], Loss: 0.8438\n",
      "Epoch [4/10], Step [268/938], Loss: 0.7984\n",
      "Epoch [4/10], Step [270/938], Loss: 0.7773\n",
      "Epoch [4/10], Step [272/938], Loss: 0.8361\n",
      "Epoch [4/10], Step [274/938], Loss: 0.7267\n",
      "Epoch [4/10], Step [276/938], Loss: 0.6694\n",
      "Epoch [4/10], Step [278/938], Loss: 0.8076\n",
      "Epoch [4/10], Step [280/938], Loss: 0.8240\n",
      "Epoch [4/10], Step [282/938], Loss: 0.8809\n",
      "Epoch [4/10], Step [284/938], Loss: 0.8435\n",
      "Epoch [4/10], Step [286/938], Loss: 0.8044\n",
      "Epoch [4/10], Step [288/938], Loss: 0.6786\n",
      "Epoch [4/10], Step [290/938], Loss: 0.6893\n",
      "Epoch [4/10], Step [292/938], Loss: 0.8075\n",
      "Epoch [4/10], Step [294/938], Loss: 0.6955\n",
      "Epoch [4/10], Step [296/938], Loss: 0.6867\n",
      "Epoch [4/10], Step [298/938], Loss: 0.6500\n",
      "Epoch [4/10], Step [300/938], Loss: 0.8171\n",
      "Epoch [4/10], Step [302/938], Loss: 0.8549\n",
      "Epoch [4/10], Step [304/938], Loss: 0.7992\n",
      "Epoch [4/10], Step [306/938], Loss: 0.7439\n",
      "Epoch [4/10], Step [308/938], Loss: 0.6782\n",
      "Epoch [4/10], Step [310/938], Loss: 0.7268\n",
      "Epoch [4/10], Step [312/938], Loss: 0.7112\n",
      "Epoch [4/10], Step [314/938], Loss: 0.7482\n",
      "Epoch [4/10], Step [316/938], Loss: 0.6297\n",
      "Epoch [4/10], Step [318/938], Loss: 0.7496\n",
      "Epoch [4/10], Step [320/938], Loss: 0.7774\n",
      "Epoch [4/10], Step [322/938], Loss: 0.6677\n",
      "Epoch [4/10], Step [324/938], Loss: 0.7289\n",
      "Epoch [4/10], Step [326/938], Loss: 0.7512\n",
      "Epoch [4/10], Step [328/938], Loss: 0.6450\n",
      "Epoch [4/10], Step [330/938], Loss: 0.7468\n",
      "Epoch [4/10], Step [332/938], Loss: 0.6606\n",
      "Epoch [4/10], Step [334/938], Loss: 0.8081\n",
      "Epoch [4/10], Step [336/938], Loss: 0.7062\n",
      "Epoch [4/10], Step [338/938], Loss: 0.7237\n",
      "Epoch [4/10], Step [340/938], Loss: 0.7324\n",
      "Epoch [4/10], Step [342/938], Loss: 0.8617\n",
      "Epoch [4/10], Step [344/938], Loss: 0.6776\n",
      "Epoch [4/10], Step [346/938], Loss: 0.6577\n",
      "Epoch [4/10], Step [348/938], Loss: 0.9627\n",
      "Epoch [4/10], Step [350/938], Loss: 0.7943\n",
      "Epoch [4/10], Step [352/938], Loss: 0.7319\n",
      "Epoch [4/10], Step [354/938], Loss: 0.8536\n",
      "Epoch [4/10], Step [356/938], Loss: 0.8464\n",
      "Epoch [4/10], Step [358/938], Loss: 0.8232\n",
      "Epoch [4/10], Step [360/938], Loss: 0.9927\n",
      "Epoch [4/10], Step [362/938], Loss: 0.6873\n",
      "Epoch [4/10], Step [364/938], Loss: 0.6916\n",
      "Epoch [4/10], Step [366/938], Loss: 0.7497\n",
      "Epoch [4/10], Step [368/938], Loss: 0.7310\n",
      "Epoch [4/10], Step [370/938], Loss: 0.7945\n",
      "Epoch [4/10], Step [372/938], Loss: 0.6685\n",
      "Epoch [4/10], Step [374/938], Loss: 0.7732\n",
      "Epoch [4/10], Step [376/938], Loss: 0.7416\n",
      "Epoch [4/10], Step [378/938], Loss: 0.7524\n",
      "Epoch [4/10], Step [380/938], Loss: 0.7393\n",
      "Epoch [4/10], Step [382/938], Loss: 0.6670\n",
      "Epoch [4/10], Step [384/938], Loss: 0.7076\n",
      "Epoch [4/10], Step [386/938], Loss: 0.6668\n",
      "Epoch [4/10], Step [388/938], Loss: 0.6962\n",
      "Epoch [4/10], Step [390/938], Loss: 0.5891\n",
      "Epoch [4/10], Step [392/938], Loss: 0.7415\n",
      "Epoch [4/10], Step [394/938], Loss: 0.7864\n",
      "Epoch [4/10], Step [396/938], Loss: 0.7280\n",
      "Epoch [4/10], Step [398/938], Loss: 0.7616\n",
      "Epoch [4/10], Step [400/938], Loss: 0.5941\n",
      "Epoch [4/10], Step [402/938], Loss: 0.8078\n",
      "Epoch [4/10], Step [404/938], Loss: 0.7000\n",
      "Epoch [4/10], Step [406/938], Loss: 0.6634\n",
      "Epoch [4/10], Step [408/938], Loss: 0.7601\n",
      "Epoch [4/10], Step [410/938], Loss: 0.7706\n",
      "Epoch [4/10], Step [412/938], Loss: 0.6566\n",
      "Epoch [4/10], Step [414/938], Loss: 0.6494\n",
      "Epoch [4/10], Step [416/938], Loss: 0.7406\n",
      "Epoch [4/10], Step [418/938], Loss: 0.8009\n",
      "Epoch [4/10], Step [420/938], Loss: 0.8508\n",
      "Epoch [4/10], Step [422/938], Loss: 0.7814\n",
      "Epoch [4/10], Step [424/938], Loss: 0.6393\n",
      "Epoch [4/10], Step [426/938], Loss: 0.7332\n",
      "Epoch [4/10], Step [428/938], Loss: 0.5996\n",
      "Epoch [4/10], Step [430/938], Loss: 0.6945\n",
      "Epoch [4/10], Step [432/938], Loss: 0.8509\n",
      "Epoch [4/10], Step [434/938], Loss: 0.7280\n",
      "Epoch [4/10], Step [436/938], Loss: 0.6855\n",
      "Epoch [4/10], Step [438/938], Loss: 0.6703\n",
      "Epoch [4/10], Step [440/938], Loss: 0.6700\n",
      "Epoch [4/10], Step [442/938], Loss: 0.7513\n",
      "Epoch [4/10], Step [444/938], Loss: 0.6774\n",
      "Epoch [4/10], Step [446/938], Loss: 0.6882\n",
      "Epoch [4/10], Step [448/938], Loss: 0.7269\n",
      "Epoch [4/10], Step [450/938], Loss: 0.7276\n",
      "Epoch [4/10], Step [452/938], Loss: 0.7613\n",
      "Epoch [4/10], Step [454/938], Loss: 0.6549\n",
      "Epoch [4/10], Step [456/938], Loss: 0.6700\n",
      "Epoch [4/10], Step [458/938], Loss: 0.7229\n",
      "Epoch [4/10], Step [460/938], Loss: 0.6693\n",
      "Epoch [4/10], Step [462/938], Loss: 0.5736\n",
      "Epoch [4/10], Step [464/938], Loss: 0.6307\n",
      "Epoch [4/10], Step [466/938], Loss: 0.6394\n",
      "Epoch [4/10], Step [468/938], Loss: 0.6020\n",
      "Epoch [4/10], Step [470/938], Loss: 0.6898\n",
      "Epoch [4/10], Step [472/938], Loss: 0.7477\n",
      "Epoch [4/10], Step [474/938], Loss: 0.6811\n",
      "Epoch [4/10], Step [476/938], Loss: 0.6538\n",
      "Epoch [4/10], Step [478/938], Loss: 0.7885\n",
      "Epoch [4/10], Step [480/938], Loss: 0.6866\n",
      "Epoch [4/10], Step [482/938], Loss: 0.8381\n",
      "Epoch [4/10], Step [484/938], Loss: 0.6680\n",
      "Epoch [4/10], Step [486/938], Loss: 0.8323\n",
      "Epoch [4/10], Step [488/938], Loss: 0.5108\n",
      "Epoch [4/10], Step [490/938], Loss: 0.6422\n",
      "Epoch [4/10], Step [492/938], Loss: 0.7273\n",
      "Epoch [4/10], Step [494/938], Loss: 0.6765\n",
      "Epoch [4/10], Step [496/938], Loss: 0.7669\n",
      "Epoch [4/10], Step [498/938], Loss: 0.5776\n",
      "Epoch [4/10], Step [500/938], Loss: 0.6672\n",
      "Epoch [4/10], Step [502/938], Loss: 0.6928\n",
      "Epoch [4/10], Step [504/938], Loss: 0.6608\n",
      "Epoch [4/10], Step [506/938], Loss: 0.5398\n",
      "Epoch [4/10], Step [508/938], Loss: 0.6790\n",
      "Epoch [4/10], Step [510/938], Loss: 0.6565\n",
      "Epoch [4/10], Step [512/938], Loss: 0.6702\n",
      "Epoch [4/10], Step [514/938], Loss: 0.7401\n",
      "Epoch [4/10], Step [516/938], Loss: 0.6501\n",
      "Epoch [4/10], Step [518/938], Loss: 0.6242\n",
      "Epoch [4/10], Step [520/938], Loss: 0.6134\n",
      "Epoch [4/10], Step [522/938], Loss: 0.6541\n",
      "Epoch [4/10], Step [524/938], Loss: 0.5960\n",
      "Epoch [4/10], Step [526/938], Loss: 0.4947\n",
      "Epoch [4/10], Step [528/938], Loss: 0.5104\n",
      "Epoch [4/10], Step [530/938], Loss: 0.6368\n",
      "Epoch [4/10], Step [532/938], Loss: 0.6114\n",
      "Epoch [4/10], Step [534/938], Loss: 0.6521\n",
      "Epoch [4/10], Step [536/938], Loss: 0.7930\n",
      "Epoch [4/10], Step [538/938], Loss: 0.6262\n",
      "Epoch [4/10], Step [540/938], Loss: 0.7848\n",
      "Epoch [4/10], Step [542/938], Loss: 0.6013\n",
      "Epoch [4/10], Step [544/938], Loss: 0.7883\n",
      "Epoch [4/10], Step [546/938], Loss: 0.6031\n",
      "Epoch [4/10], Step [548/938], Loss: 0.7283\n",
      "Epoch [4/10], Step [550/938], Loss: 0.6565\n",
      "Epoch [4/10], Step [552/938], Loss: 0.5652\n",
      "Epoch [4/10], Step [554/938], Loss: 0.6211\n",
      "Epoch [4/10], Step [556/938], Loss: 0.6690\n",
      "Epoch [4/10], Step [558/938], Loss: 0.6986\n",
      "Epoch [4/10], Step [560/938], Loss: 0.7279\n",
      "Epoch [4/10], Step [562/938], Loss: 0.7330\n",
      "Epoch [4/10], Step [564/938], Loss: 0.6677\n",
      "Epoch [4/10], Step [566/938], Loss: 0.5463\n",
      "Epoch [4/10], Step [568/938], Loss: 0.7388\n",
      "Epoch [4/10], Step [570/938], Loss: 0.6704\n",
      "Epoch [4/10], Step [572/938], Loss: 0.7634\n",
      "Epoch [4/10], Step [574/938], Loss: 0.7622\n",
      "Epoch [4/10], Step [576/938], Loss: 0.5880\n",
      "Epoch [4/10], Step [578/938], Loss: 0.6185\n",
      "Epoch [4/10], Step [580/938], Loss: 0.6379\n",
      "Epoch [4/10], Step [582/938], Loss: 0.5920\n",
      "Epoch [4/10], Step [584/938], Loss: 0.6156\n",
      "Epoch [4/10], Step [586/938], Loss: 0.8616\n",
      "Epoch [4/10], Step [588/938], Loss: 0.6214\n",
      "Epoch [4/10], Step [590/938], Loss: 0.6785\n",
      "Epoch [4/10], Step [592/938], Loss: 0.7376\n",
      "Epoch [4/10], Step [594/938], Loss: 0.6936\n",
      "Epoch [4/10], Step [596/938], Loss: 0.7246\n",
      "Epoch [4/10], Step [598/938], Loss: 0.5789\n",
      "Epoch [4/10], Step [600/938], Loss: 0.5914\n",
      "Epoch [4/10], Step [602/938], Loss: 0.5546\n",
      "Epoch [4/10], Step [604/938], Loss: 0.6386\n",
      "Epoch [4/10], Step [606/938], Loss: 0.5392\n",
      "Epoch [4/10], Step [608/938], Loss: 0.5933\n",
      "Epoch [4/10], Step [610/938], Loss: 0.5690\n",
      "Epoch [4/10], Step [612/938], Loss: 0.5863\n",
      "Epoch [4/10], Step [614/938], Loss: 0.7670\n",
      "Epoch [4/10], Step [616/938], Loss: 0.6034\n",
      "Epoch [4/10], Step [618/938], Loss: 0.7177\n",
      "Epoch [4/10], Step [620/938], Loss: 0.6047\n",
      "Epoch [4/10], Step [622/938], Loss: 0.5401\n",
      "Epoch [4/10], Step [624/938], Loss: 0.6249\n",
      "Epoch [4/10], Step [626/938], Loss: 0.6205\n",
      "Epoch [4/10], Step [628/938], Loss: 0.5762\n",
      "Epoch [4/10], Step [630/938], Loss: 0.4500\n",
      "Epoch [4/10], Step [632/938], Loss: 0.5126\n",
      "Epoch [4/10], Step [634/938], Loss: 0.6222\n",
      "Epoch [4/10], Step [636/938], Loss: 0.6590\n",
      "Epoch [4/10], Step [638/938], Loss: 0.6938\n",
      "Epoch [4/10], Step [640/938], Loss: 0.6196\n",
      "Epoch [4/10], Step [642/938], Loss: 0.6220\n",
      "Epoch [4/10], Step [644/938], Loss: 0.6427\n",
      "Epoch [4/10], Step [646/938], Loss: 0.6274\n",
      "Epoch [4/10], Step [648/938], Loss: 0.5275\n",
      "Epoch [4/10], Step [650/938], Loss: 0.6071\n",
      "Epoch [4/10], Step [652/938], Loss: 0.5899\n",
      "Epoch [4/10], Step [654/938], Loss: 0.7986\n",
      "Epoch [4/10], Step [656/938], Loss: 0.5586\n",
      "Epoch [4/10], Step [658/938], Loss: 0.6426\n",
      "Epoch [4/10], Step [660/938], Loss: 0.6243\n",
      "Epoch [4/10], Step [662/938], Loss: 0.6089\n",
      "Epoch [4/10], Step [664/938], Loss: 0.6741\n",
      "Epoch [4/10], Step [666/938], Loss: 0.6526\n",
      "Epoch [4/10], Step [668/938], Loss: 0.5410\n",
      "Epoch [4/10], Step [670/938], Loss: 0.6545\n",
      "Epoch [4/10], Step [672/938], Loss: 0.6338\n",
      "Epoch [4/10], Step [674/938], Loss: 0.4837\n",
      "Epoch [4/10], Step [676/938], Loss: 0.6244\n",
      "Epoch [4/10], Step [678/938], Loss: 0.5373\n",
      "Epoch [4/10], Step [680/938], Loss: 0.5212\n",
      "Epoch [4/10], Step [682/938], Loss: 0.5781\n",
      "Epoch [4/10], Step [684/938], Loss: 0.6340\n",
      "Epoch [4/10], Step [686/938], Loss: 0.6500\n",
      "Epoch [4/10], Step [688/938], Loss: 0.6034\n",
      "Epoch [4/10], Step [690/938], Loss: 0.5663\n",
      "Epoch [4/10], Step [692/938], Loss: 0.7317\n",
      "Epoch [4/10], Step [694/938], Loss: 0.7163\n",
      "Epoch [4/10], Step [696/938], Loss: 0.6552\n",
      "Epoch [4/10], Step [698/938], Loss: 0.5300\n",
      "Epoch [4/10], Step [700/938], Loss: 0.6347\n",
      "Epoch [4/10], Step [702/938], Loss: 0.6672\n",
      "Epoch [4/10], Step [704/938], Loss: 0.4710\n",
      "Epoch [4/10], Step [706/938], Loss: 0.5808\n",
      "Epoch [4/10], Step [708/938], Loss: 0.5216\n",
      "Epoch [4/10], Step [710/938], Loss: 0.5491\n",
      "Epoch [4/10], Step [712/938], Loss: 0.5516\n",
      "Epoch [4/10], Step [714/938], Loss: 0.7468\n",
      "Epoch [4/10], Step [716/938], Loss: 0.5194\n",
      "Epoch [4/10], Step [718/938], Loss: 0.6689\n",
      "Epoch [4/10], Step [720/938], Loss: 0.5564\n",
      "Epoch [4/10], Step [722/938], Loss: 0.5927\n",
      "Epoch [4/10], Step [724/938], Loss: 0.6224\n",
      "Epoch [4/10], Step [726/938], Loss: 0.5856\n",
      "Epoch [4/10], Step [728/938], Loss: 0.6085\n",
      "Epoch [4/10], Step [730/938], Loss: 0.6216\n",
      "Epoch [4/10], Step [732/938], Loss: 0.5355\n",
      "Epoch [4/10], Step [734/938], Loss: 0.5454\n",
      "Epoch [4/10], Step [736/938], Loss: 0.6778\n",
      "Epoch [4/10], Step [738/938], Loss: 0.5932\n",
      "Epoch [4/10], Step [740/938], Loss: 0.6588\n",
      "Epoch [4/10], Step [742/938], Loss: 0.6801\n",
      "Epoch [4/10], Step [744/938], Loss: 0.6560\n",
      "Epoch [4/10], Step [746/938], Loss: 0.3978\n",
      "Epoch [4/10], Step [748/938], Loss: 0.7765\n",
      "Epoch [4/10], Step [750/938], Loss: 0.5881\n",
      "Epoch [4/10], Step [752/938], Loss: 0.5450\n",
      "Epoch [4/10], Step [754/938], Loss: 0.5140\n",
      "Epoch [4/10], Step [756/938], Loss: 0.6720\n",
      "Epoch [4/10], Step [758/938], Loss: 0.5144\n",
      "Epoch [4/10], Step [760/938], Loss: 0.5164\n",
      "Epoch [4/10], Step [762/938], Loss: 0.7773\n",
      "Epoch [4/10], Step [764/938], Loss: 0.5642\n",
      "Epoch [4/10], Step [766/938], Loss: 0.5242\n",
      "Epoch [4/10], Step [768/938], Loss: 0.7188\n",
      "Epoch [4/10], Step [770/938], Loss: 0.6502\n",
      "Epoch [4/10], Step [772/938], Loss: 0.6620\n",
      "Epoch [4/10], Step [774/938], Loss: 0.5275\n",
      "Epoch [4/10], Step [776/938], Loss: 0.4083\n",
      "Epoch [4/10], Step [778/938], Loss: 0.6915\n",
      "Epoch [4/10], Step [780/938], Loss: 0.6550\n",
      "Epoch [4/10], Step [782/938], Loss: 0.5748\n",
      "Epoch [4/10], Step [784/938], Loss: 0.5624\n",
      "Epoch [4/10], Step [786/938], Loss: 0.5975\n",
      "Epoch [4/10], Step [788/938], Loss: 0.6849\n",
      "Epoch [4/10], Step [790/938], Loss: 0.6366\n",
      "Epoch [4/10], Step [792/938], Loss: 0.5827\n",
      "Epoch [4/10], Step [794/938], Loss: 0.5587\n",
      "Epoch [4/10], Step [796/938], Loss: 0.6772\n",
      "Epoch [4/10], Step [798/938], Loss: 0.6234\n",
      "Epoch [4/10], Step [800/938], Loss: 0.5653\n",
      "Epoch [4/10], Step [802/938], Loss: 0.6418\n",
      "Epoch [4/10], Step [804/938], Loss: 0.5609\n",
      "Epoch [4/10], Step [806/938], Loss: 0.5750\n",
      "Epoch [4/10], Step [808/938], Loss: 0.4228\n",
      "Epoch [4/10], Step [810/938], Loss: 0.5865\n",
      "Epoch [4/10], Step [812/938], Loss: 0.6672\n",
      "Epoch [4/10], Step [814/938], Loss: 0.5047\n",
      "Epoch [4/10], Step [816/938], Loss: 0.5942\n",
      "Epoch [4/10], Step [818/938], Loss: 0.5478\n",
      "Epoch [4/10], Step [820/938], Loss: 0.6620\n",
      "Epoch [4/10], Step [822/938], Loss: 0.4605\n",
      "Epoch [4/10], Step [824/938], Loss: 0.5702\n",
      "Epoch [4/10], Step [826/938], Loss: 0.5490\n",
      "Epoch [4/10], Step [828/938], Loss: 0.5045\n",
      "Epoch [4/10], Step [830/938], Loss: 0.6143\n",
      "Epoch [4/10], Step [832/938], Loss: 0.5296\n",
      "Epoch [4/10], Step [834/938], Loss: 0.5509\n",
      "Epoch [4/10], Step [836/938], Loss: 0.4702\n",
      "Epoch [4/10], Step [838/938], Loss: 0.4950\n",
      "Epoch [4/10], Step [840/938], Loss: 0.4750\n",
      "Epoch [4/10], Step [842/938], Loss: 0.6132\n",
      "Epoch [4/10], Step [844/938], Loss: 0.7623\n",
      "Epoch [4/10], Step [846/938], Loss: 0.7865\n",
      "Epoch [4/10], Step [848/938], Loss: 0.4631\n",
      "Epoch [4/10], Step [850/938], Loss: 0.6466\n",
      "Epoch [4/10], Step [852/938], Loss: 0.4112\n",
      "Epoch [4/10], Step [854/938], Loss: 0.5368\n",
      "Epoch [4/10], Step [856/938], Loss: 0.4505\n",
      "Epoch [4/10], Step [858/938], Loss: 0.3609\n",
      "Epoch [4/10], Step [860/938], Loss: 0.4960\n",
      "Epoch [4/10], Step [862/938], Loss: 0.4332\n",
      "Epoch [4/10], Step [864/938], Loss: 0.4639\n",
      "Epoch [4/10], Step [866/938], Loss: 0.5261\n",
      "Epoch [4/10], Step [868/938], Loss: 0.4827\n",
      "Epoch [4/10], Step [870/938], Loss: 0.5102\n",
      "Epoch [4/10], Step [872/938], Loss: 0.4493\n",
      "Epoch [4/10], Step [874/938], Loss: 0.5682\n",
      "Epoch [4/10], Step [876/938], Loss: 0.4776\n",
      "Epoch [4/10], Step [878/938], Loss: 0.8119\n",
      "Epoch [4/10], Step [880/938], Loss: 0.5098\n",
      "Epoch [4/10], Step [882/938], Loss: 0.5787\n",
      "Epoch [4/10], Step [884/938], Loss: 0.5510\n",
      "Epoch [4/10], Step [886/938], Loss: 0.5540\n",
      "Epoch [4/10], Step [888/938], Loss: 0.6621\n",
      "Epoch [4/10], Step [890/938], Loss: 0.4929\n",
      "Epoch [4/10], Step [892/938], Loss: 0.5859\n",
      "Epoch [4/10], Step [894/938], Loss: 0.6349\n",
      "Epoch [4/10], Step [896/938], Loss: 0.7407\n",
      "Epoch [4/10], Step [898/938], Loss: 0.5214\n",
      "Epoch [4/10], Step [900/938], Loss: 0.5332\n",
      "Epoch [4/10], Step [902/938], Loss: 0.6355\n",
      "Epoch [4/10], Step [904/938], Loss: 0.6182\n",
      "Epoch [4/10], Step [906/938], Loss: 0.4724\n",
      "Epoch [4/10], Step [908/938], Loss: 0.4961\n",
      "Epoch [4/10], Step [910/938], Loss: 0.5799\n",
      "Epoch [4/10], Step [912/938], Loss: 0.7424\n",
      "Epoch [4/10], Step [914/938], Loss: 0.4201\n",
      "Epoch [4/10], Step [916/938], Loss: 0.5935\n",
      "Epoch [4/10], Step [918/938], Loss: 0.4687\n",
      "Epoch [4/10], Step [920/938], Loss: 0.6197\n",
      "Epoch [4/10], Step [922/938], Loss: 0.5450\n",
      "Epoch [4/10], Step [924/938], Loss: 0.5494\n",
      "Epoch [4/10], Step [926/938], Loss: 0.5833\n",
      "Epoch [4/10], Step [928/938], Loss: 0.4828\n",
      "Epoch [4/10], Step [930/938], Loss: 0.5939\n",
      "Epoch [4/10], Step [932/938], Loss: 0.5515\n",
      "Epoch [4/10], Step [934/938], Loss: 0.4954\n",
      "Epoch [4/10], Step [936/938], Loss: 0.6505\n",
      "Epoch [4/10], Step [938/938], Loss: 0.5148\n",
      "Epoch [4/10], Loss: 0.7061\n",
      "Epoch [5/10], Step [2/938], Loss: 0.5472\n",
      "Epoch [5/10], Step [4/938], Loss: 0.4957\n",
      "Epoch [5/10], Step [6/938], Loss: 0.4512\n",
      "Epoch [5/10], Step [8/938], Loss: 0.5563\n",
      "Epoch [5/10], Step [10/938], Loss: 0.5517\n",
      "Epoch [5/10], Step [12/938], Loss: 0.5177\n",
      "Epoch [5/10], Step [14/938], Loss: 0.5531\n",
      "Epoch [5/10], Step [16/938], Loss: 0.4910\n",
      "Epoch [5/10], Step [18/938], Loss: 0.5139\n",
      "Epoch [5/10], Step [20/938], Loss: 0.5373\n",
      "Epoch [5/10], Step [22/938], Loss: 0.5134\n",
      "Epoch [5/10], Step [24/938], Loss: 0.5316\n",
      "Epoch [5/10], Step [26/938], Loss: 0.5262\n",
      "Epoch [5/10], Step [28/938], Loss: 0.5369\n",
      "Epoch [5/10], Step [30/938], Loss: 0.6537\n",
      "Epoch [5/10], Step [32/938], Loss: 0.5570\n",
      "Epoch [5/10], Step [34/938], Loss: 0.5004\n",
      "Epoch [5/10], Step [36/938], Loss: 0.6138\n",
      "Epoch [5/10], Step [38/938], Loss: 0.5309\n",
      "Epoch [5/10], Step [40/938], Loss: 0.5153\n",
      "Epoch [5/10], Step [42/938], Loss: 0.5398\n",
      "Epoch [5/10], Step [44/938], Loss: 0.4905\n",
      "Epoch [5/10], Step [46/938], Loss: 0.5916\n",
      "Epoch [5/10], Step [48/938], Loss: 0.4557\n",
      "Epoch [5/10], Step [50/938], Loss: 0.8482\n",
      "Epoch [5/10], Step [52/938], Loss: 0.4516\n",
      "Epoch [5/10], Step [54/938], Loss: 0.4900\n",
      "Epoch [5/10], Step [56/938], Loss: 0.4692\n",
      "Epoch [5/10], Step [58/938], Loss: 0.4784\n",
      "Epoch [5/10], Step [60/938], Loss: 0.4322\n",
      "Epoch [5/10], Step [62/938], Loss: 0.5170\n",
      "Epoch [5/10], Step [64/938], Loss: 0.6392\n",
      "Epoch [5/10], Step [66/938], Loss: 0.4184\n",
      "Epoch [5/10], Step [68/938], Loss: 0.4359\n",
      "Epoch [5/10], Step [70/938], Loss: 0.4879\n",
      "Epoch [5/10], Step [72/938], Loss: 0.6122\n",
      "Epoch [5/10], Step [74/938], Loss: 0.5127\n",
      "Epoch [5/10], Step [76/938], Loss: 0.5305\n",
      "Epoch [5/10], Step [78/938], Loss: 0.5473\n",
      "Epoch [5/10], Step [80/938], Loss: 0.5967\n",
      "Epoch [5/10], Step [82/938], Loss: 0.6331\n",
      "Epoch [5/10], Step [84/938], Loss: 0.5574\n",
      "Epoch [5/10], Step [86/938], Loss: 0.6865\n",
      "Epoch [5/10], Step [88/938], Loss: 0.6391\n",
      "Epoch [5/10], Step [90/938], Loss: 0.5212\n",
      "Epoch [5/10], Step [92/938], Loss: 0.5052\n",
      "Epoch [5/10], Step [94/938], Loss: 0.6447\n",
      "Epoch [5/10], Step [96/938], Loss: 0.4650\n",
      "Epoch [5/10], Step [98/938], Loss: 0.4445\n",
      "Epoch [5/10], Step [100/938], Loss: 0.5940\n",
      "Epoch [5/10], Step [102/938], Loss: 0.4923\n",
      "Epoch [5/10], Step [104/938], Loss: 0.4357\n",
      "Epoch [5/10], Step [106/938], Loss: 0.5189\n",
      "Epoch [5/10], Step [108/938], Loss: 0.5443\n",
      "Epoch [5/10], Step [110/938], Loss: 0.5397\n",
      "Epoch [5/10], Step [112/938], Loss: 0.5193\n",
      "Epoch [5/10], Step [114/938], Loss: 0.5887\n",
      "Epoch [5/10], Step [116/938], Loss: 0.5176\n",
      "Epoch [5/10], Step [118/938], Loss: 0.3935\n",
      "Epoch [5/10], Step [120/938], Loss: 0.4963\n",
      "Epoch [5/10], Step [122/938], Loss: 0.5113\n",
      "Epoch [5/10], Step [124/938], Loss: 0.4227\n",
      "Epoch [5/10], Step [126/938], Loss: 0.6027\n",
      "Epoch [5/10], Step [128/938], Loss: 0.6002\n",
      "Epoch [5/10], Step [130/938], Loss: 0.5528\n",
      "Epoch [5/10], Step [132/938], Loss: 0.6242\n",
      "Epoch [5/10], Step [134/938], Loss: 0.4607\n",
      "Epoch [5/10], Step [136/938], Loss: 0.5534\n",
      "Epoch [5/10], Step [138/938], Loss: 0.5253\n",
      "Epoch [5/10], Step [140/938], Loss: 0.6452\n",
      "Epoch [5/10], Step [142/938], Loss: 0.4176\n",
      "Epoch [5/10], Step [144/938], Loss: 0.5794\n",
      "Epoch [5/10], Step [146/938], Loss: 0.4210\n",
      "Epoch [5/10], Step [148/938], Loss: 0.4167\n",
      "Epoch [5/10], Step [150/938], Loss: 0.4918\n",
      "Epoch [5/10], Step [152/938], Loss: 0.5848\n",
      "Epoch [5/10], Step [154/938], Loss: 0.5227\n",
      "Epoch [5/10], Step [156/938], Loss: 0.6901\n",
      "Epoch [5/10], Step [158/938], Loss: 0.5109\n",
      "Epoch [5/10], Step [160/938], Loss: 0.4767\n",
      "Epoch [5/10], Step [162/938], Loss: 0.4631\n",
      "Epoch [5/10], Step [164/938], Loss: 0.5257\n",
      "Epoch [5/10], Step [166/938], Loss: 0.4096\n",
      "Epoch [5/10], Step [168/938], Loss: 0.4679\n",
      "Epoch [5/10], Step [170/938], Loss: 0.4228\n",
      "Epoch [5/10], Step [172/938], Loss: 0.5258\n",
      "Epoch [5/10], Step [174/938], Loss: 0.7286\n",
      "Epoch [5/10], Step [176/938], Loss: 0.4186\n",
      "Epoch [5/10], Step [178/938], Loss: 0.4415\n",
      "Epoch [5/10], Step [180/938], Loss: 0.4011\n",
      "Epoch [5/10], Step [182/938], Loss: 0.4223\n",
      "Epoch [5/10], Step [184/938], Loss: 0.6090\n",
      "Epoch [5/10], Step [186/938], Loss: 0.5894\n",
      "Epoch [5/10], Step [188/938], Loss: 0.4768\n",
      "Epoch [5/10], Step [190/938], Loss: 0.5548\n",
      "Epoch [5/10], Step [192/938], Loss: 0.5537\n",
      "Epoch [5/10], Step [194/938], Loss: 0.5546\n",
      "Epoch [5/10], Step [196/938], Loss: 0.4513\n",
      "Epoch [5/10], Step [198/938], Loss: 0.4355\n",
      "Epoch [5/10], Step [200/938], Loss: 0.4290\n",
      "Epoch [5/10], Step [202/938], Loss: 0.5633\n",
      "Epoch [5/10], Step [204/938], Loss: 0.6075\n",
      "Epoch [5/10], Step [206/938], Loss: 0.3935\n",
      "Epoch [5/10], Step [208/938], Loss: 0.4842\n",
      "Epoch [5/10], Step [210/938], Loss: 0.4587\n",
      "Epoch [5/10], Step [212/938], Loss: 0.5534\n",
      "Epoch [5/10], Step [214/938], Loss: 0.5699\n",
      "Epoch [5/10], Step [216/938], Loss: 0.4580\n",
      "Epoch [5/10], Step [218/938], Loss: 0.4171\n",
      "Epoch [5/10], Step [220/938], Loss: 0.6152\n",
      "Epoch [5/10], Step [222/938], Loss: 0.3824\n",
      "Epoch [5/10], Step [224/938], Loss: 0.6117\n",
      "Epoch [5/10], Step [226/938], Loss: 0.5162\n",
      "Epoch [5/10], Step [228/938], Loss: 0.4004\n",
      "Epoch [5/10], Step [230/938], Loss: 0.5549\n",
      "Epoch [5/10], Step [232/938], Loss: 0.4402\n",
      "Epoch [5/10], Step [234/938], Loss: 0.4639\n",
      "Epoch [5/10], Step [236/938], Loss: 0.5152\n",
      "Epoch [5/10], Step [238/938], Loss: 0.5577\n",
      "Epoch [5/10], Step [240/938], Loss: 0.4556\n",
      "Epoch [5/10], Step [242/938], Loss: 0.5163\n",
      "Epoch [5/10], Step [244/938], Loss: 0.4226\n",
      "Epoch [5/10], Step [246/938], Loss: 0.7070\n",
      "Epoch [5/10], Step [248/938], Loss: 0.4948\n",
      "Epoch [5/10], Step [250/938], Loss: 0.6836\n",
      "Epoch [5/10], Step [252/938], Loss: 0.5251\n",
      "Epoch [5/10], Step [254/938], Loss: 0.5110\n",
      "Epoch [5/10], Step [256/938], Loss: 0.6848\n",
      "Epoch [5/10], Step [258/938], Loss: 0.4890\n",
      "Epoch [5/10], Step [260/938], Loss: 0.2934\n",
      "Epoch [5/10], Step [262/938], Loss: 0.5184\n",
      "Epoch [5/10], Step [264/938], Loss: 0.5271\n",
      "Epoch [5/10], Step [266/938], Loss: 0.5122\n",
      "Epoch [5/10], Step [268/938], Loss: 0.5507\n",
      "Epoch [5/10], Step [270/938], Loss: 0.5563\n",
      "Epoch [5/10], Step [272/938], Loss: 0.5181\n",
      "Epoch [5/10], Step [274/938], Loss: 0.5578\n",
      "Epoch [5/10], Step [276/938], Loss: 0.5865\n",
      "Epoch [5/10], Step [278/938], Loss: 0.7058\n",
      "Epoch [5/10], Step [280/938], Loss: 0.5271\n",
      "Epoch [5/10], Step [282/938], Loss: 0.5111\n",
      "Epoch [5/10], Step [284/938], Loss: 0.4417\n",
      "Epoch [5/10], Step [286/938], Loss: 0.4338\n",
      "Epoch [5/10], Step [288/938], Loss: 0.5016\n",
      "Epoch [5/10], Step [290/938], Loss: 0.5894\n",
      "Epoch [5/10], Step [292/938], Loss: 0.5211\n",
      "Epoch [5/10], Step [294/938], Loss: 0.5280\n",
      "Epoch [5/10], Step [296/938], Loss: 0.3602\n",
      "Epoch [5/10], Step [298/938], Loss: 0.3857\n",
      "Epoch [5/10], Step [300/938], Loss: 0.3588\n",
      "Epoch [5/10], Step [302/938], Loss: 0.5073\n",
      "Epoch [5/10], Step [304/938], Loss: 0.6029\n",
      "Epoch [5/10], Step [306/938], Loss: 0.5099\n",
      "Epoch [5/10], Step [308/938], Loss: 0.4345\n",
      "Epoch [5/10], Step [310/938], Loss: 0.4322\n",
      "Epoch [5/10], Step [312/938], Loss: 0.4950\n",
      "Epoch [5/10], Step [314/938], Loss: 0.5351\n",
      "Epoch [5/10], Step [316/938], Loss: 0.4995\n",
      "Epoch [5/10], Step [318/938], Loss: 0.5274\n",
      "Epoch [5/10], Step [320/938], Loss: 0.4504\n",
      "Epoch [5/10], Step [322/938], Loss: 0.5906\n",
      "Epoch [5/10], Step [324/938], Loss: 0.6011\n",
      "Epoch [5/10], Step [326/938], Loss: 0.4456\n",
      "Epoch [5/10], Step [328/938], Loss: 0.5299\n",
      "Epoch [5/10], Step [330/938], Loss: 0.5491\n",
      "Epoch [5/10], Step [332/938], Loss: 0.4865\n",
      "Epoch [5/10], Step [334/938], Loss: 0.3496\n",
      "Epoch [5/10], Step [336/938], Loss: 0.3418\n",
      "Epoch [5/10], Step [338/938], Loss: 0.5292\n",
      "Epoch [5/10], Step [340/938], Loss: 0.5801\n",
      "Epoch [5/10], Step [342/938], Loss: 0.4863\n",
      "Epoch [5/10], Step [344/938], Loss: 0.4245\n",
      "Epoch [5/10], Step [346/938], Loss: 0.6319\n",
      "Epoch [5/10], Step [348/938], Loss: 0.4653\n",
      "Epoch [5/10], Step [350/938], Loss: 0.4932\n",
      "Epoch [5/10], Step [352/938], Loss: 0.5992\n",
      "Epoch [5/10], Step [354/938], Loss: 0.3733\n",
      "Epoch [5/10], Step [356/938], Loss: 0.5246\n",
      "Epoch [5/10], Step [358/938], Loss: 0.4549\n",
      "Epoch [5/10], Step [360/938], Loss: 0.4998\n",
      "Epoch [5/10], Step [362/938], Loss: 0.4555\n",
      "Epoch [5/10], Step [364/938], Loss: 0.4527\n",
      "Epoch [5/10], Step [366/938], Loss: 0.3363\n",
      "Epoch [5/10], Step [368/938], Loss: 0.5009\n",
      "Epoch [5/10], Step [370/938], Loss: 0.3840\n",
      "Epoch [5/10], Step [372/938], Loss: 0.3655\n",
      "Epoch [5/10], Step [374/938], Loss: 0.5641\n",
      "Epoch [5/10], Step [376/938], Loss: 0.4167\n",
      "Epoch [5/10], Step [378/938], Loss: 0.5164\n",
      "Epoch [5/10], Step [380/938], Loss: 0.4828\n",
      "Epoch [5/10], Step [382/938], Loss: 0.6134\n",
      "Epoch [5/10], Step [384/938], Loss: 0.4828\n",
      "Epoch [5/10], Step [386/938], Loss: 0.4124\n",
      "Epoch [5/10], Step [388/938], Loss: 0.4028\n",
      "Epoch [5/10], Step [390/938], Loss: 0.4610\n",
      "Epoch [5/10], Step [392/938], Loss: 0.6411\n",
      "Epoch [5/10], Step [394/938], Loss: 0.6119\n",
      "Epoch [5/10], Step [396/938], Loss: 0.6222\n",
      "Epoch [5/10], Step [398/938], Loss: 0.5487\n",
      "Epoch [5/10], Step [400/938], Loss: 0.6504\n",
      "Epoch [5/10], Step [402/938], Loss: 0.5099\n",
      "Epoch [5/10], Step [404/938], Loss: 0.4133\n",
      "Epoch [5/10], Step [406/938], Loss: 0.3836\n",
      "Epoch [5/10], Step [408/938], Loss: 0.4515\n",
      "Epoch [5/10], Step [410/938], Loss: 0.4537\n",
      "Epoch [5/10], Step [412/938], Loss: 0.4451\n",
      "Epoch [5/10], Step [414/938], Loss: 0.5969\n",
      "Epoch [5/10], Step [416/938], Loss: 0.4055\n",
      "Epoch [5/10], Step [418/938], Loss: 0.4365\n",
      "Epoch [5/10], Step [420/938], Loss: 0.3506\n",
      "Epoch [5/10], Step [422/938], Loss: 0.5368\n",
      "Epoch [5/10], Step [424/938], Loss: 0.3685\n",
      "Epoch [5/10], Step [426/938], Loss: 0.4192\n",
      "Epoch [5/10], Step [428/938], Loss: 0.5011\n",
      "Epoch [5/10], Step [430/938], Loss: 0.4569\n",
      "Epoch [5/10], Step [432/938], Loss: 0.3965\n",
      "Epoch [5/10], Step [434/938], Loss: 0.5018\n",
      "Epoch [5/10], Step [436/938], Loss: 0.4017\n",
      "Epoch [5/10], Step [438/938], Loss: 0.4929\n",
      "Epoch [5/10], Step [440/938], Loss: 0.7053\n",
      "Epoch [5/10], Step [442/938], Loss: 0.4526\n",
      "Epoch [5/10], Step [444/938], Loss: 0.6732\n",
      "Epoch [5/10], Step [446/938], Loss: 0.5228\n",
      "Epoch [5/10], Step [448/938], Loss: 0.5440\n",
      "Epoch [5/10], Step [450/938], Loss: 0.3759\n",
      "Epoch [5/10], Step [452/938], Loss: 0.4225\n",
      "Epoch [5/10], Step [454/938], Loss: 0.5834\n",
      "Epoch [5/10], Step [456/938], Loss: 0.4928\n",
      "Epoch [5/10], Step [458/938], Loss: 0.4688\n",
      "Epoch [5/10], Step [460/938], Loss: 0.4413\n",
      "Epoch [5/10], Step [462/938], Loss: 0.3309\n",
      "Epoch [5/10], Step [464/938], Loss: 0.4247\n",
      "Epoch [5/10], Step [466/938], Loss: 0.5007\n",
      "Epoch [5/10], Step [468/938], Loss: 0.3706\n",
      "Epoch [5/10], Step [470/938], Loss: 0.6084\n",
      "Epoch [5/10], Step [472/938], Loss: 0.4118\n",
      "Epoch [5/10], Step [474/938], Loss: 0.4873\n",
      "Epoch [5/10], Step [476/938], Loss: 0.4768\n",
      "Epoch [5/10], Step [478/938], Loss: 0.4872\n",
      "Epoch [5/10], Step [480/938], Loss: 0.5414\n",
      "Epoch [5/10], Step [482/938], Loss: 0.3541\n",
      "Epoch [5/10], Step [484/938], Loss: 0.5565\n",
      "Epoch [5/10], Step [486/938], Loss: 0.4357\n",
      "Epoch [5/10], Step [488/938], Loss: 0.4733\n",
      "Epoch [5/10], Step [490/938], Loss: 0.3450\n",
      "Epoch [5/10], Step [492/938], Loss: 0.5679\n",
      "Epoch [5/10], Step [494/938], Loss: 0.5044\n",
      "Epoch [5/10], Step [496/938], Loss: 0.4800\n",
      "Epoch [5/10], Step [498/938], Loss: 0.4250\n",
      "Epoch [5/10], Step [500/938], Loss: 0.3866\n",
      "Epoch [5/10], Step [502/938], Loss: 0.4900\n",
      "Epoch [5/10], Step [504/938], Loss: 0.5285\n",
      "Epoch [5/10], Step [506/938], Loss: 0.3133\n",
      "Epoch [5/10], Step [508/938], Loss: 0.4925\n",
      "Epoch [5/10], Step [510/938], Loss: 0.6327\n",
      "Epoch [5/10], Step [512/938], Loss: 0.4655\n",
      "Epoch [5/10], Step [514/938], Loss: 0.4904\n",
      "Epoch [5/10], Step [516/938], Loss: 0.3876\n",
      "Epoch [5/10], Step [518/938], Loss: 0.4546\n",
      "Epoch [5/10], Step [520/938], Loss: 0.4889\n",
      "Epoch [5/10], Step [522/938], Loss: 0.6959\n",
      "Epoch [5/10], Step [524/938], Loss: 0.4289\n",
      "Epoch [5/10], Step [526/938], Loss: 0.4601\n",
      "Epoch [5/10], Step [528/938], Loss: 0.4236\n",
      "Epoch [5/10], Step [530/938], Loss: 0.4874\n",
      "Epoch [5/10], Step [532/938], Loss: 0.6146\n",
      "Epoch [5/10], Step [534/938], Loss: 0.5084\n",
      "Epoch [5/10], Step [536/938], Loss: 0.4391\n",
      "Epoch [5/10], Step [538/938], Loss: 0.5242\n",
      "Epoch [5/10], Step [540/938], Loss: 0.4226\n",
      "Epoch [5/10], Step [542/938], Loss: 0.3276\n",
      "Epoch [5/10], Step [544/938], Loss: 0.4662\n",
      "Epoch [5/10], Step [546/938], Loss: 0.4867\n",
      "Epoch [5/10], Step [548/938], Loss: 0.5088\n",
      "Epoch [5/10], Step [550/938], Loss: 0.3446\n",
      "Epoch [5/10], Step [552/938], Loss: 0.4086\n",
      "Epoch [5/10], Step [554/938], Loss: 0.4634\n",
      "Epoch [5/10], Step [556/938], Loss: 0.3982\n",
      "Epoch [5/10], Step [558/938], Loss: 0.4945\n",
      "Epoch [5/10], Step [560/938], Loss: 0.3836\n",
      "Epoch [5/10], Step [562/938], Loss: 0.5513\n",
      "Epoch [5/10], Step [564/938], Loss: 0.4638\n",
      "Epoch [5/10], Step [566/938], Loss: 0.3408\n",
      "Epoch [5/10], Step [568/938], Loss: 0.4273\n",
      "Epoch [5/10], Step [570/938], Loss: 0.3629\n",
      "Epoch [5/10], Step [572/938], Loss: 0.4511\n",
      "Epoch [5/10], Step [574/938], Loss: 0.5284\n",
      "Epoch [5/10], Step [576/938], Loss: 0.4645\n",
      "Epoch [5/10], Step [578/938], Loss: 0.3303\n",
      "Epoch [5/10], Step [580/938], Loss: 0.5226\n",
      "Epoch [5/10], Step [582/938], Loss: 0.3419\n",
      "Epoch [5/10], Step [584/938], Loss: 0.6109\n",
      "Epoch [5/10], Step [586/938], Loss: 0.4580\n",
      "Epoch [5/10], Step [588/938], Loss: 0.4445\n",
      "Epoch [5/10], Step [590/938], Loss: 0.6092\n",
      "Epoch [5/10], Step [592/938], Loss: 0.5333\n",
      "Epoch [5/10], Step [594/938], Loss: 0.4960\n",
      "Epoch [5/10], Step [596/938], Loss: 0.5147\n",
      "Epoch [5/10], Step [598/938], Loss: 0.5030\n",
      "Epoch [5/10], Step [600/938], Loss: 0.5127\n",
      "Epoch [5/10], Step [602/938], Loss: 0.4876\n",
      "Epoch [5/10], Step [604/938], Loss: 0.2555\n",
      "Epoch [5/10], Step [606/938], Loss: 0.4498\n",
      "Epoch [5/10], Step [608/938], Loss: 0.4369\n",
      "Epoch [5/10], Step [610/938], Loss: 0.6279\n",
      "Epoch [5/10], Step [612/938], Loss: 0.6821\n",
      "Epoch [5/10], Step [614/938], Loss: 0.3982\n",
      "Epoch [5/10], Step [616/938], Loss: 0.6636\n",
      "Epoch [5/10], Step [618/938], Loss: 0.4094\n",
      "Epoch [5/10], Step [620/938], Loss: 0.4390\n",
      "Epoch [5/10], Step [622/938], Loss: 0.6209\n",
      "Epoch [5/10], Step [624/938], Loss: 0.6269\n",
      "Epoch [5/10], Step [626/938], Loss: 0.2990\n",
      "Epoch [5/10], Step [628/938], Loss: 0.5134\n",
      "Epoch [5/10], Step [630/938], Loss: 0.3061\n",
      "Epoch [5/10], Step [632/938], Loss: 0.4745\n",
      "Epoch [5/10], Step [634/938], Loss: 0.4984\n",
      "Epoch [5/10], Step [636/938], Loss: 0.2103\n",
      "Epoch [5/10], Step [638/938], Loss: 0.4452\n",
      "Epoch [5/10], Step [640/938], Loss: 0.4286\n",
      "Epoch [5/10], Step [642/938], Loss: 0.5194\n",
      "Epoch [5/10], Step [644/938], Loss: 0.2945\n",
      "Epoch [5/10], Step [646/938], Loss: 0.4998\n",
      "Epoch [5/10], Step [648/938], Loss: 0.4671\n",
      "Epoch [5/10], Step [650/938], Loss: 0.4223\n",
      "Epoch [5/10], Step [652/938], Loss: 0.4603\n",
      "Epoch [5/10], Step [654/938], Loss: 0.4036\n",
      "Epoch [5/10], Step [656/938], Loss: 0.5483\n",
      "Epoch [5/10], Step [658/938], Loss: 0.4265\n",
      "Epoch [5/10], Step [660/938], Loss: 0.4412\n",
      "Epoch [5/10], Step [662/938], Loss: 0.4211\n",
      "Epoch [5/10], Step [664/938], Loss: 0.4700\n",
      "Epoch [5/10], Step [666/938], Loss: 0.4563\n",
      "Epoch [5/10], Step [668/938], Loss: 0.3948\n",
      "Epoch [5/10], Step [670/938], Loss: 0.4310\n",
      "Epoch [5/10], Step [672/938], Loss: 0.4001\n",
      "Epoch [5/10], Step [674/938], Loss: 0.4952\n",
      "Epoch [5/10], Step [676/938], Loss: 0.4535\n",
      "Epoch [5/10], Step [678/938], Loss: 0.3512\n",
      "Epoch [5/10], Step [680/938], Loss: 0.4397\n",
      "Epoch [5/10], Step [682/938], Loss: 0.4162\n",
      "Epoch [5/10], Step [684/938], Loss: 0.5342\n",
      "Epoch [5/10], Step [686/938], Loss: 0.4935\n",
      "Epoch [5/10], Step [688/938], Loss: 0.4348\n",
      "Epoch [5/10], Step [690/938], Loss: 0.4118\n",
      "Epoch [5/10], Step [692/938], Loss: 0.5385\n",
      "Epoch [5/10], Step [694/938], Loss: 0.4601\n",
      "Epoch [5/10], Step [696/938], Loss: 0.3314\n",
      "Epoch [5/10], Step [698/938], Loss: 0.4332\n",
      "Epoch [5/10], Step [700/938], Loss: 0.4571\n",
      "Epoch [5/10], Step [702/938], Loss: 0.3735\n",
      "Epoch [5/10], Step [704/938], Loss: 0.4324\n",
      "Epoch [5/10], Step [706/938], Loss: 0.4164\n",
      "Epoch [5/10], Step [708/938], Loss: 0.2769\n",
      "Epoch [5/10], Step [710/938], Loss: 0.4314\n",
      "Epoch [5/10], Step [712/938], Loss: 0.3837\n",
      "Epoch [5/10], Step [714/938], Loss: 0.3403\n",
      "Epoch [5/10], Step [716/938], Loss: 0.5890\n",
      "Epoch [5/10], Step [718/938], Loss: 0.4049\n",
      "Epoch [5/10], Step [720/938], Loss: 0.3832\n",
      "Epoch [5/10], Step [722/938], Loss: 0.5035\n",
      "Epoch [5/10], Step [724/938], Loss: 0.3549\n",
      "Epoch [5/10], Step [726/938], Loss: 0.3550\n",
      "Epoch [5/10], Step [728/938], Loss: 0.5019\n",
      "Epoch [5/10], Step [730/938], Loss: 0.4148\n",
      "Epoch [5/10], Step [732/938], Loss: 0.4736\n",
      "Epoch [5/10], Step [734/938], Loss: 0.3823\n",
      "Epoch [5/10], Step [736/938], Loss: 0.3610\n",
      "Epoch [5/10], Step [738/938], Loss: 0.4046\n",
      "Epoch [5/10], Step [740/938], Loss: 0.4395\n",
      "Epoch [5/10], Step [742/938], Loss: 0.4874\n",
      "Epoch [5/10], Step [744/938], Loss: 0.4619\n",
      "Epoch [5/10], Step [746/938], Loss: 0.3786\n",
      "Epoch [5/10], Step [748/938], Loss: 0.5123\n",
      "Epoch [5/10], Step [750/938], Loss: 0.4521\n",
      "Epoch [5/10], Step [752/938], Loss: 0.4625\n",
      "Epoch [5/10], Step [754/938], Loss: 0.3666\n",
      "Epoch [5/10], Step [756/938], Loss: 0.4530\n",
      "Epoch [5/10], Step [758/938], Loss: 0.4482\n",
      "Epoch [5/10], Step [760/938], Loss: 0.5671\n",
      "Epoch [5/10], Step [762/938], Loss: 0.3798\n",
      "Epoch [5/10], Step [764/938], Loss: 0.5693\n",
      "Epoch [5/10], Step [766/938], Loss: 0.2782\n",
      "Epoch [5/10], Step [768/938], Loss: 0.3827\n",
      "Epoch [5/10], Step [770/938], Loss: 0.4153\n",
      "Epoch [5/10], Step [772/938], Loss: 0.4561\n",
      "Epoch [5/10], Step [774/938], Loss: 0.3792\n",
      "Epoch [5/10], Step [776/938], Loss: 0.4407\n",
      "Epoch [5/10], Step [778/938], Loss: 0.4375\n",
      "Epoch [5/10], Step [780/938], Loss: 0.5267\n",
      "Epoch [5/10], Step [782/938], Loss: 0.4105\n",
      "Epoch [5/10], Step [784/938], Loss: 0.5020\n",
      "Epoch [5/10], Step [786/938], Loss: 0.4943\n",
      "Epoch [5/10], Step [788/938], Loss: 0.4786\n",
      "Epoch [5/10], Step [790/938], Loss: 0.4019\n",
      "Epoch [5/10], Step [792/938], Loss: 0.3851\n",
      "Epoch [5/10], Step [794/938], Loss: 0.4598\n",
      "Epoch [5/10], Step [796/938], Loss: 0.3917\n",
      "Epoch [5/10], Step [798/938], Loss: 0.4386\n",
      "Epoch [5/10], Step [800/938], Loss: 0.3993\n",
      "Epoch [5/10], Step [802/938], Loss: 0.5562\n",
      "Epoch [5/10], Step [804/938], Loss: 0.4713\n",
      "Epoch [5/10], Step [806/938], Loss: 0.5198\n",
      "Epoch [5/10], Step [808/938], Loss: 0.4004\n",
      "Epoch [5/10], Step [810/938], Loss: 0.3075\n",
      "Epoch [5/10], Step [812/938], Loss: 0.4328\n",
      "Epoch [5/10], Step [814/938], Loss: 0.2987\n",
      "Epoch [5/10], Step [816/938], Loss: 0.3684\n",
      "Epoch [5/10], Step [818/938], Loss: 0.3385\n",
      "Epoch [5/10], Step [820/938], Loss: 0.5467\n",
      "Epoch [5/10], Step [822/938], Loss: 0.3482\n",
      "Epoch [5/10], Step [824/938], Loss: 0.5072\n",
      "Epoch [5/10], Step [826/938], Loss: 0.3482\n",
      "Epoch [5/10], Step [828/938], Loss: 0.4494\n",
      "Epoch [5/10], Step [830/938], Loss: 0.3282\n",
      "Epoch [5/10], Step [832/938], Loss: 0.4578\n",
      "Epoch [5/10], Step [834/938], Loss: 0.2964\n",
      "Epoch [5/10], Step [836/938], Loss: 0.3911\n",
      "Epoch [5/10], Step [838/938], Loss: 0.3051\n",
      "Epoch [5/10], Step [840/938], Loss: 0.3487\n",
      "Epoch [5/10], Step [842/938], Loss: 0.4714\n",
      "Epoch [5/10], Step [844/938], Loss: 0.3351\n",
      "Epoch [5/10], Step [846/938], Loss: 0.3824\n",
      "Epoch [5/10], Step [848/938], Loss: 0.4330\n",
      "Epoch [5/10], Step [850/938], Loss: 0.4328\n",
      "Epoch [5/10], Step [852/938], Loss: 0.3650\n",
      "Epoch [5/10], Step [854/938], Loss: 0.3874\n",
      "Epoch [5/10], Step [856/938], Loss: 0.4952\n",
      "Epoch [5/10], Step [858/938], Loss: 0.5559\n",
      "Epoch [5/10], Step [860/938], Loss: 0.3318\n",
      "Epoch [5/10], Step [862/938], Loss: 0.3929\n",
      "Epoch [5/10], Step [864/938], Loss: 0.4431\n",
      "Epoch [5/10], Step [866/938], Loss: 0.4083\n",
      "Epoch [5/10], Step [868/938], Loss: 0.4417\n",
      "Epoch [5/10], Step [870/938], Loss: 0.4477\n",
      "Epoch [5/10], Step [872/938], Loss: 0.2669\n",
      "Epoch [5/10], Step [874/938], Loss: 0.3495\n",
      "Epoch [5/10], Step [876/938], Loss: 0.3362\n",
      "Epoch [5/10], Step [878/938], Loss: 0.5651\n",
      "Epoch [5/10], Step [880/938], Loss: 0.3940\n",
      "Epoch [5/10], Step [882/938], Loss: 0.4351\n",
      "Epoch [5/10], Step [884/938], Loss: 0.2909\n",
      "Epoch [5/10], Step [886/938], Loss: 0.5583\n",
      "Epoch [5/10], Step [888/938], Loss: 0.5527\n",
      "Epoch [5/10], Step [890/938], Loss: 0.3801\n",
      "Epoch [5/10], Step [892/938], Loss: 0.5737\n",
      "Epoch [5/10], Step [894/938], Loss: 0.3622\n",
      "Epoch [5/10], Step [896/938], Loss: 0.3772\n",
      "Epoch [5/10], Step [898/938], Loss: 0.4422\n",
      "Epoch [5/10], Step [900/938], Loss: 0.4731\n",
      "Epoch [5/10], Step [902/938], Loss: 0.2741\n",
      "Epoch [5/10], Step [904/938], Loss: 0.4194\n",
      "Epoch [5/10], Step [906/938], Loss: 0.4603\n",
      "Epoch [5/10], Step [908/938], Loss: 0.3619\n",
      "Epoch [5/10], Step [910/938], Loss: 0.3821\n",
      "Epoch [5/10], Step [912/938], Loss: 0.4351\n",
      "Epoch [5/10], Step [914/938], Loss: 0.4120\n",
      "Epoch [5/10], Step [916/938], Loss: 0.4008\n",
      "Epoch [5/10], Step [918/938], Loss: 0.4425\n",
      "Epoch [5/10], Step [920/938], Loss: 0.4366\n",
      "Epoch [5/10], Step [922/938], Loss: 0.4013\n",
      "Epoch [5/10], Step [924/938], Loss: 0.3292\n",
      "Epoch [5/10], Step [926/938], Loss: 0.4304\n",
      "Epoch [5/10], Step [928/938], Loss: 0.2648\n",
      "Epoch [5/10], Step [930/938], Loss: 0.4194\n",
      "Epoch [5/10], Step [932/938], Loss: 0.2684\n",
      "Epoch [5/10], Step [934/938], Loss: 0.4233\n",
      "Epoch [5/10], Step [936/938], Loss: 0.4687\n",
      "Epoch [5/10], Step [938/938], Loss: 0.6285\n",
      "Epoch [5/10], Loss: 0.4676\n",
      "Epoch [6/10], Step [2/938], Loss: 0.3024\n",
      "Epoch [6/10], Step [4/938], Loss: 0.3503\n",
      "Epoch [6/10], Step [6/938], Loss: 0.5941\n",
      "Epoch [6/10], Step [8/938], Loss: 0.3770\n",
      "Epoch [6/10], Step [10/938], Loss: 0.5294\n",
      "Epoch [6/10], Step [12/938], Loss: 0.3552\n",
      "Epoch [6/10], Step [14/938], Loss: 0.2996\n",
      "Epoch [6/10], Step [16/938], Loss: 0.4304\n",
      "Epoch [6/10], Step [18/938], Loss: 0.4763\n",
      "Epoch [6/10], Step [20/938], Loss: 0.4752\n",
      "Epoch [6/10], Step [22/938], Loss: 0.4507\n",
      "Epoch [6/10], Step [24/938], Loss: 0.3546\n",
      "Epoch [6/10], Step [26/938], Loss: 0.4538\n",
      "Epoch [6/10], Step [28/938], Loss: 0.2418\n",
      "Epoch [6/10], Step [30/938], Loss: 0.4497\n",
      "Epoch [6/10], Step [32/938], Loss: 0.4639\n",
      "Epoch [6/10], Step [34/938], Loss: 0.4655\n",
      "Epoch [6/10], Step [36/938], Loss: 0.4830\n",
      "Epoch [6/10], Step [38/938], Loss: 0.3029\n",
      "Epoch [6/10], Step [40/938], Loss: 0.4266\n",
      "Epoch [6/10], Step [42/938], Loss: 0.4159\n",
      "Epoch [6/10], Step [44/938], Loss: 0.2606\n",
      "Epoch [6/10], Step [46/938], Loss: 0.3499\n",
      "Epoch [6/10], Step [48/938], Loss: 0.4708\n",
      "Epoch [6/10], Step [50/938], Loss: 0.5464\n",
      "Epoch [6/10], Step [52/938], Loss: 0.4542\n",
      "Epoch [6/10], Step [54/938], Loss: 0.3224\n",
      "Epoch [6/10], Step [56/938], Loss: 0.3169\n",
      "Epoch [6/10], Step [58/938], Loss: 0.4194\n",
      "Epoch [6/10], Step [60/938], Loss: 0.3756\n",
      "Epoch [6/10], Step [62/938], Loss: 0.2435\n",
      "Epoch [6/10], Step [64/938], Loss: 0.4425\n",
      "Epoch [6/10], Step [66/938], Loss: 0.2833\n",
      "Epoch [6/10], Step [68/938], Loss: 0.2936\n",
      "Epoch [6/10], Step [70/938], Loss: 0.4050\n",
      "Epoch [6/10], Step [72/938], Loss: 0.4114\n",
      "Epoch [6/10], Step [74/938], Loss: 0.6077\n",
      "Epoch [6/10], Step [76/938], Loss: 0.3651\n",
      "Epoch [6/10], Step [78/938], Loss: 0.4523\n",
      "Epoch [6/10], Step [80/938], Loss: 0.3640\n",
      "Epoch [6/10], Step [82/938], Loss: 0.2917\n",
      "Epoch [6/10], Step [84/938], Loss: 0.5302\n",
      "Epoch [6/10], Step [86/938], Loss: 0.3089\n",
      "Epoch [6/10], Step [88/938], Loss: 0.3297\n",
      "Epoch [6/10], Step [90/938], Loss: 0.3572\n",
      "Epoch [6/10], Step [92/938], Loss: 0.2240\n",
      "Epoch [6/10], Step [94/938], Loss: 0.3004\n",
      "Epoch [6/10], Step [96/938], Loss: 0.5766\n",
      "Epoch [6/10], Step [98/938], Loss: 0.4072\n",
      "Epoch [6/10], Step [100/938], Loss: 0.3805\n",
      "Epoch [6/10], Step [102/938], Loss: 0.3076\n",
      "Epoch [6/10], Step [104/938], Loss: 0.3484\n",
      "Epoch [6/10], Step [106/938], Loss: 0.3487\n",
      "Epoch [6/10], Step [108/938], Loss: 0.4245\n",
      "Epoch [6/10], Step [110/938], Loss: 0.4159\n",
      "Epoch [6/10], Step [112/938], Loss: 0.4109\n",
      "Epoch [6/10], Step [114/938], Loss: 0.3713\n",
      "Epoch [6/10], Step [116/938], Loss: 0.3249\n",
      "Epoch [6/10], Step [118/938], Loss: 0.2554\n",
      "Epoch [6/10], Step [120/938], Loss: 0.4082\n",
      "Epoch [6/10], Step [122/938], Loss: 0.4273\n",
      "Epoch [6/10], Step [124/938], Loss: 0.3357\n",
      "Epoch [6/10], Step [126/938], Loss: 0.4165\n",
      "Epoch [6/10], Step [128/938], Loss: 0.3129\n",
      "Epoch [6/10], Step [130/938], Loss: 0.4378\n",
      "Epoch [6/10], Step [132/938], Loss: 0.4460\n",
      "Epoch [6/10], Step [134/938], Loss: 0.3960\n",
      "Epoch [6/10], Step [136/938], Loss: 0.5454\n",
      "Epoch [6/10], Step [138/938], Loss: 0.3182\n",
      "Epoch [6/10], Step [140/938], Loss: 0.3848\n",
      "Epoch [6/10], Step [142/938], Loss: 0.3825\n",
      "Epoch [6/10], Step [144/938], Loss: 0.5361\n",
      "Epoch [6/10], Step [146/938], Loss: 0.5050\n",
      "Epoch [6/10], Step [148/938], Loss: 0.3291\n",
      "Epoch [6/10], Step [150/938], Loss: 0.3356\n",
      "Epoch [6/10], Step [152/938], Loss: 0.4357\n",
      "Epoch [6/10], Step [154/938], Loss: 0.4087\n",
      "Epoch [6/10], Step [156/938], Loss: 0.3917\n",
      "Epoch [6/10], Step [158/938], Loss: 0.2797\n",
      "Epoch [6/10], Step [160/938], Loss: 0.5509\n",
      "Epoch [6/10], Step [162/938], Loss: 0.3585\n",
      "Epoch [6/10], Step [164/938], Loss: 0.4306\n",
      "Epoch [6/10], Step [166/938], Loss: 0.4760\n",
      "Epoch [6/10], Step [168/938], Loss: 0.3613\n",
      "Epoch [6/10], Step [170/938], Loss: 0.3676\n",
      "Epoch [6/10], Step [172/938], Loss: 0.4222\n",
      "Epoch [6/10], Step [174/938], Loss: 0.3045\n",
      "Epoch [6/10], Step [176/938], Loss: 0.4739\n",
      "Epoch [6/10], Step [178/938], Loss: 0.2835\n",
      "Epoch [6/10], Step [180/938], Loss: 0.4430\n",
      "Epoch [6/10], Step [182/938], Loss: 0.4755\n",
      "Epoch [6/10], Step [184/938], Loss: 0.2702\n",
      "Epoch [6/10], Step [186/938], Loss: 0.3103\n",
      "Epoch [6/10], Step [188/938], Loss: 0.4994\n",
      "Epoch [6/10], Step [190/938], Loss: 0.4723\n",
      "Epoch [6/10], Step [192/938], Loss: 0.3063\n",
      "Epoch [6/10], Step [194/938], Loss: 0.3536\n",
      "Epoch [6/10], Step [196/938], Loss: 0.6875\n",
      "Epoch [6/10], Step [198/938], Loss: 0.3921\n",
      "Epoch [6/10], Step [200/938], Loss: 0.3339\n",
      "Epoch [6/10], Step [202/938], Loss: 0.4007\n",
      "Epoch [6/10], Step [204/938], Loss: 0.7726\n",
      "Epoch [6/10], Step [206/938], Loss: 0.3259\n",
      "Epoch [6/10], Step [208/938], Loss: 0.4200\n",
      "Epoch [6/10], Step [210/938], Loss: 0.4310\n",
      "Epoch [6/10], Step [212/938], Loss: 0.2921\n",
      "Epoch [6/10], Step [214/938], Loss: 0.3627\n",
      "Epoch [6/10], Step [216/938], Loss: 0.3022\n",
      "Epoch [6/10], Step [218/938], Loss: 0.3561\n",
      "Epoch [6/10], Step [220/938], Loss: 0.3346\n",
      "Epoch [6/10], Step [222/938], Loss: 0.2791\n",
      "Epoch [6/10], Step [224/938], Loss: 0.4975\n",
      "Epoch [6/10], Step [226/938], Loss: 0.4742\n",
      "Epoch [6/10], Step [228/938], Loss: 0.3573\n",
      "Epoch [6/10], Step [230/938], Loss: 0.5298\n",
      "Epoch [6/10], Step [232/938], Loss: 0.3527\n",
      "Epoch [6/10], Step [234/938], Loss: 0.2842\n",
      "Epoch [6/10], Step [236/938], Loss: 0.3348\n",
      "Epoch [6/10], Step [238/938], Loss: 0.4722\n",
      "Epoch [6/10], Step [240/938], Loss: 0.2900\n",
      "Epoch [6/10], Step [242/938], Loss: 0.4914\n",
      "Epoch [6/10], Step [244/938], Loss: 0.2687\n",
      "Epoch [6/10], Step [246/938], Loss: 0.3805\n",
      "Epoch [6/10], Step [248/938], Loss: 0.4983\n",
      "Epoch [6/10], Step [250/938], Loss: 0.3570\n",
      "Epoch [6/10], Step [252/938], Loss: 0.4942\n",
      "Epoch [6/10], Step [254/938], Loss: 0.4278\n",
      "Epoch [6/10], Step [256/938], Loss: 0.4040\n",
      "Epoch [6/10], Step [258/938], Loss: 0.3311\n",
      "Epoch [6/10], Step [260/938], Loss: 0.2935\n",
      "Epoch [6/10], Step [262/938], Loss: 0.5072\n",
      "Epoch [6/10], Step [264/938], Loss: 0.4651\n",
      "Epoch [6/10], Step [266/938], Loss: 0.3222\n",
      "Epoch [6/10], Step [268/938], Loss: 0.3826\n",
      "Epoch [6/10], Step [270/938], Loss: 0.5324\n",
      "Epoch [6/10], Step [272/938], Loss: 0.4390\n",
      "Epoch [6/10], Step [274/938], Loss: 0.2070\n",
      "Epoch [6/10], Step [276/938], Loss: 0.3374\n",
      "Epoch [6/10], Step [278/938], Loss: 0.3097\n",
      "Epoch [6/10], Step [280/938], Loss: 0.4248\n",
      "Epoch [6/10], Step [282/938], Loss: 0.4486\n",
      "Epoch [6/10], Step [284/938], Loss: 0.3965\n",
      "Epoch [6/10], Step [286/938], Loss: 0.6312\n",
      "Epoch [6/10], Step [288/938], Loss: 0.2421\n",
      "Epoch [6/10], Step [290/938], Loss: 0.4338\n",
      "Epoch [6/10], Step [292/938], Loss: 0.3479\n",
      "Epoch [6/10], Step [294/938], Loss: 0.3832\n",
      "Epoch [6/10], Step [296/938], Loss: 0.3265\n",
      "Epoch [6/10], Step [298/938], Loss: 0.4920\n",
      "Epoch [6/10], Step [300/938], Loss: 0.3136\n",
      "Epoch [6/10], Step [302/938], Loss: 0.3869\n",
      "Epoch [6/10], Step [304/938], Loss: 0.4216\n",
      "Epoch [6/10], Step [306/938], Loss: 0.4977\n",
      "Epoch [6/10], Step [308/938], Loss: 0.4267\n",
      "Epoch [6/10], Step [310/938], Loss: 0.5701\n",
      "Epoch [6/10], Step [312/938], Loss: 0.3810\n",
      "Epoch [6/10], Step [314/938], Loss: 0.3442\n",
      "Epoch [6/10], Step [316/938], Loss: 0.5280\n",
      "Epoch [6/10], Step [318/938], Loss: 0.5312\n",
      "Epoch [6/10], Step [320/938], Loss: 0.3537\n",
      "Epoch [6/10], Step [322/938], Loss: 0.2643\n",
      "Epoch [6/10], Step [324/938], Loss: 0.3482\n",
      "Epoch [6/10], Step [326/938], Loss: 0.4379\n",
      "Epoch [6/10], Step [328/938], Loss: 0.5112\n",
      "Epoch [6/10], Step [330/938], Loss: 0.3110\n",
      "Epoch [6/10], Step [332/938], Loss: 0.4899\n",
      "Epoch [6/10], Step [334/938], Loss: 0.3355\n",
      "Epoch [6/10], Step [336/938], Loss: 0.2585\n",
      "Epoch [6/10], Step [338/938], Loss: 0.3572\n",
      "Epoch [6/10], Step [340/938], Loss: 0.4437\n",
      "Epoch [6/10], Step [342/938], Loss: 0.3106\n",
      "Epoch [6/10], Step [344/938], Loss: 0.4938\n",
      "Epoch [6/10], Step [346/938], Loss: 0.4933\n",
      "Epoch [6/10], Step [348/938], Loss: 0.4552\n",
      "Epoch [6/10], Step [350/938], Loss: 0.4647\n",
      "Epoch [6/10], Step [352/938], Loss: 0.3554\n",
      "Epoch [6/10], Step [354/938], Loss: 0.3810\n",
      "Epoch [6/10], Step [356/938], Loss: 0.3235\n",
      "Epoch [6/10], Step [358/938], Loss: 0.5494\n",
      "Epoch [6/10], Step [360/938], Loss: 0.3832\n",
      "Epoch [6/10], Step [362/938], Loss: 0.2749\n",
      "Epoch [6/10], Step [364/938], Loss: 0.3715\n",
      "Epoch [6/10], Step [366/938], Loss: 0.3237\n",
      "Epoch [6/10], Step [368/938], Loss: 0.3697\n",
      "Epoch [6/10], Step [370/938], Loss: 0.2466\n",
      "Epoch [6/10], Step [372/938], Loss: 0.3410\n",
      "Epoch [6/10], Step [374/938], Loss: 0.2560\n",
      "Epoch [6/10], Step [376/938], Loss: 0.4858\n",
      "Epoch [6/10], Step [378/938], Loss: 0.5138\n",
      "Epoch [6/10], Step [380/938], Loss: 0.2784\n",
      "Epoch [6/10], Step [382/938], Loss: 0.3367\n",
      "Epoch [6/10], Step [384/938], Loss: 0.2476\n",
      "Epoch [6/10], Step [386/938], Loss: 0.3805\n",
      "Epoch [6/10], Step [388/938], Loss: 0.3542\n",
      "Epoch [6/10], Step [390/938], Loss: 0.4305\n",
      "Epoch [6/10], Step [392/938], Loss: 0.3319\n",
      "Epoch [6/10], Step [394/938], Loss: 0.4123\n",
      "Epoch [6/10], Step [396/938], Loss: 0.3572\n",
      "Epoch [6/10], Step [398/938], Loss: 0.3264\n",
      "Epoch [6/10], Step [400/938], Loss: 0.2355\n",
      "Epoch [6/10], Step [402/938], Loss: 0.4774\n",
      "Epoch [6/10], Step [404/938], Loss: 0.3327\n",
      "Epoch [6/10], Step [406/938], Loss: 0.3644\n",
      "Epoch [6/10], Step [408/938], Loss: 0.3985\n",
      "Epoch [6/10], Step [410/938], Loss: 0.2939\n",
      "Epoch [6/10], Step [412/938], Loss: 0.3797\n",
      "Epoch [6/10], Step [414/938], Loss: 0.4145\n",
      "Epoch [6/10], Step [416/938], Loss: 0.3945\n",
      "Epoch [6/10], Step [418/938], Loss: 0.4664\n",
      "Epoch [6/10], Step [420/938], Loss: 0.3749\n",
      "Epoch [6/10], Step [422/938], Loss: 0.3612\n",
      "Epoch [6/10], Step [424/938], Loss: 0.4254\n",
      "Epoch [6/10], Step [426/938], Loss: 0.3072\n",
      "Epoch [6/10], Step [428/938], Loss: 0.2951\n",
      "Epoch [6/10], Step [430/938], Loss: 0.2624\n",
      "Epoch [6/10], Step [432/938], Loss: 0.3867\n",
      "Epoch [6/10], Step [434/938], Loss: 0.5206\n",
      "Epoch [6/10], Step [436/938], Loss: 0.3840\n",
      "Epoch [6/10], Step [438/938], Loss: 0.4443\n",
      "Epoch [6/10], Step [440/938], Loss: 0.3644\n",
      "Epoch [6/10], Step [442/938], Loss: 0.2372\n",
      "Epoch [6/10], Step [444/938], Loss: 0.3885\n",
      "Epoch [6/10], Step [446/938], Loss: 0.3786\n",
      "Epoch [6/10], Step [448/938], Loss: 0.2556\n",
      "Epoch [6/10], Step [450/938], Loss: 0.3420\n",
      "Epoch [6/10], Step [452/938], Loss: 0.4156\n",
      "Epoch [6/10], Step [454/938], Loss: 0.3224\n",
      "Epoch [6/10], Step [456/938], Loss: 0.2763\n",
      "Epoch [6/10], Step [458/938], Loss: 0.3893\n",
      "Epoch [6/10], Step [460/938], Loss: 0.3555\n",
      "Epoch [6/10], Step [462/938], Loss: 0.3076\n",
      "Epoch [6/10], Step [464/938], Loss: 0.2608\n",
      "Epoch [6/10], Step [466/938], Loss: 0.3165\n",
      "Epoch [6/10], Step [468/938], Loss: 0.2288\n",
      "Epoch [6/10], Step [470/938], Loss: 0.4277\n",
      "Epoch [6/10], Step [472/938], Loss: 0.3332\n",
      "Epoch [6/10], Step [474/938], Loss: 0.4128\n",
      "Epoch [6/10], Step [476/938], Loss: 0.2916\n",
      "Epoch [6/10], Step [478/938], Loss: 0.4834\n",
      "Epoch [6/10], Step [480/938], Loss: 0.3748\n",
      "Epoch [6/10], Step [482/938], Loss: 0.3944\n",
      "Epoch [6/10], Step [484/938], Loss: 0.5084\n",
      "Epoch [6/10], Step [486/938], Loss: 0.3818\n",
      "Epoch [6/10], Step [488/938], Loss: 0.4934\n",
      "Epoch [6/10], Step [490/938], Loss: 0.3958\n",
      "Epoch [6/10], Step [492/938], Loss: 0.5927\n",
      "Epoch [6/10], Step [494/938], Loss: 0.5636\n",
      "Epoch [6/10], Step [496/938], Loss: 0.3345\n",
      "Epoch [6/10], Step [498/938], Loss: 0.3607\n",
      "Epoch [6/10], Step [500/938], Loss: 0.3984\n",
      "Epoch [6/10], Step [502/938], Loss: 0.4280\n",
      "Epoch [6/10], Step [504/938], Loss: 0.2463\n",
      "Epoch [6/10], Step [506/938], Loss: 0.4333\n",
      "Epoch [6/10], Step [508/938], Loss: 0.2722\n",
      "Epoch [6/10], Step [510/938], Loss: 0.3696\n",
      "Epoch [6/10], Step [512/938], Loss: 0.3833\n",
      "Epoch [6/10], Step [514/938], Loss: 0.3458\n",
      "Epoch [6/10], Step [516/938], Loss: 0.4818\n",
      "Epoch [6/10], Step [518/938], Loss: 0.4557\n",
      "Epoch [6/10], Step [520/938], Loss: 0.4064\n",
      "Epoch [6/10], Step [522/938], Loss: 0.2899\n",
      "Epoch [6/10], Step [524/938], Loss: 0.2848\n",
      "Epoch [6/10], Step [526/938], Loss: 0.4423\n",
      "Epoch [6/10], Step [528/938], Loss: 0.3405\n",
      "Epoch [6/10], Step [530/938], Loss: 0.2735\n",
      "Epoch [6/10], Step [532/938], Loss: 0.3556\n",
      "Epoch [6/10], Step [534/938], Loss: 0.3061\n",
      "Epoch [6/10], Step [536/938], Loss: 0.3854\n",
      "Epoch [6/10], Step [538/938], Loss: 0.2318\n",
      "Epoch [6/10], Step [540/938], Loss: 0.2778\n",
      "Epoch [6/10], Step [542/938], Loss: 0.3046\n",
      "Epoch [6/10], Step [544/938], Loss: 0.4398\n",
      "Epoch [6/10], Step [546/938], Loss: 0.3438\n",
      "Epoch [6/10], Step [548/938], Loss: 0.2975\n",
      "Epoch [6/10], Step [550/938], Loss: 0.3575\n",
      "Epoch [6/10], Step [552/938], Loss: 0.2861\n",
      "Epoch [6/10], Step [554/938], Loss: 0.3438\n",
      "Epoch [6/10], Step [556/938], Loss: 0.3463\n",
      "Epoch [6/10], Step [558/938], Loss: 0.5323\n",
      "Epoch [6/10], Step [560/938], Loss: 0.4428\n",
      "Epoch [6/10], Step [562/938], Loss: 0.2535\n",
      "Epoch [6/10], Step [564/938], Loss: 0.2774\n",
      "Epoch [6/10], Step [566/938], Loss: 0.3772\n",
      "Epoch [6/10], Step [568/938], Loss: 0.3097\n",
      "Epoch [6/10], Step [570/938], Loss: 0.5592\n",
      "Epoch [6/10], Step [572/938], Loss: 0.5220\n",
      "Epoch [6/10], Step [574/938], Loss: 0.3597\n",
      "Epoch [6/10], Step [576/938], Loss: 0.3556\n",
      "Epoch [6/10], Step [578/938], Loss: 0.2685\n",
      "Epoch [6/10], Step [580/938], Loss: 0.2660\n",
      "Epoch [6/10], Step [582/938], Loss: 0.3580\n",
      "Epoch [6/10], Step [584/938], Loss: 0.2503\n",
      "Epoch [6/10], Step [586/938], Loss: 0.3077\n",
      "Epoch [6/10], Step [588/938], Loss: 0.4029\n",
      "Epoch [6/10], Step [590/938], Loss: 0.3664\n",
      "Epoch [6/10], Step [592/938], Loss: 0.3161\n",
      "Epoch [6/10], Step [594/938], Loss: 0.2955\n",
      "Epoch [6/10], Step [596/938], Loss: 0.3063\n",
      "Epoch [6/10], Step [598/938], Loss: 0.4295\n",
      "Epoch [6/10], Step [600/938], Loss: 0.3626\n",
      "Epoch [6/10], Step [602/938], Loss: 0.3596\n",
      "Epoch [6/10], Step [604/938], Loss: 0.2964\n",
      "Epoch [6/10], Step [606/938], Loss: 0.2444\n",
      "Epoch [6/10], Step [608/938], Loss: 0.3679\n",
      "Epoch [6/10], Step [610/938], Loss: 0.3241\n",
      "Epoch [6/10], Step [612/938], Loss: 0.5005\n",
      "Epoch [6/10], Step [614/938], Loss: 0.4264\n",
      "Epoch [6/10], Step [616/938], Loss: 0.4043\n",
      "Epoch [6/10], Step [618/938], Loss: 0.4295\n",
      "Epoch [6/10], Step [620/938], Loss: 0.2477\n",
      "Epoch [6/10], Step [622/938], Loss: 0.3668\n",
      "Epoch [6/10], Step [624/938], Loss: 0.2625\n",
      "Epoch [6/10], Step [626/938], Loss: 0.5032\n",
      "Epoch [6/10], Step [628/938], Loss: 0.2745\n",
      "Epoch [6/10], Step [630/938], Loss: 0.4494\n",
      "Epoch [6/10], Step [632/938], Loss: 0.3993\n",
      "Epoch [6/10], Step [634/938], Loss: 0.3676\n",
      "Epoch [6/10], Step [636/938], Loss: 0.2680\n",
      "Epoch [6/10], Step [638/938], Loss: 0.3380\n",
      "Epoch [6/10], Step [640/938], Loss: 0.3607\n",
      "Epoch [6/10], Step [642/938], Loss: 0.3193\n",
      "Epoch [6/10], Step [644/938], Loss: 0.2195\n",
      "Epoch [6/10], Step [646/938], Loss: 0.4193\n",
      "Epoch [6/10], Step [648/938], Loss: 0.3638\n",
      "Epoch [6/10], Step [650/938], Loss: 0.3413\n",
      "Epoch [6/10], Step [652/938], Loss: 0.5218\n",
      "Epoch [6/10], Step [654/938], Loss: 0.4176\n",
      "Epoch [6/10], Step [656/938], Loss: 0.2550\n",
      "Epoch [6/10], Step [658/938], Loss: 0.1954\n",
      "Epoch [6/10], Step [660/938], Loss: 0.4000\n",
      "Epoch [6/10], Step [662/938], Loss: 0.4639\n",
      "Epoch [6/10], Step [664/938], Loss: 0.3004\n",
      "Epoch [6/10], Step [666/938], Loss: 0.3551\n",
      "Epoch [6/10], Step [668/938], Loss: 0.2975\n",
      "Epoch [6/10], Step [670/938], Loss: 0.4364\n",
      "Epoch [6/10], Step [672/938], Loss: 0.3379\n",
      "Epoch [6/10], Step [674/938], Loss: 0.3781\n",
      "Epoch [6/10], Step [676/938], Loss: 0.4424\n",
      "Epoch [6/10], Step [678/938], Loss: 0.3351\n",
      "Epoch [6/10], Step [680/938], Loss: 0.3898\n",
      "Epoch [6/10], Step [682/938], Loss: 0.3294\n",
      "Epoch [6/10], Step [684/938], Loss: 0.3464\n",
      "Epoch [6/10], Step [686/938], Loss: 0.2762\n",
      "Epoch [6/10], Step [688/938], Loss: 0.4467\n",
      "Epoch [6/10], Step [690/938], Loss: 0.4139\n",
      "Epoch [6/10], Step [692/938], Loss: 0.2493\n",
      "Epoch [6/10], Step [694/938], Loss: 0.4129\n",
      "Epoch [6/10], Step [696/938], Loss: 0.4697\n",
      "Epoch [6/10], Step [698/938], Loss: 0.4477\n",
      "Epoch [6/10], Step [700/938], Loss: 0.4945\n",
      "Epoch [6/10], Step [702/938], Loss: 0.3683\n",
      "Epoch [6/10], Step [704/938], Loss: 0.2952\n",
      "Epoch [6/10], Step [706/938], Loss: 0.2691\n",
      "Epoch [6/10], Step [708/938], Loss: 0.3181\n",
      "Epoch [6/10], Step [710/938], Loss: 0.2784\n",
      "Epoch [6/10], Step [712/938], Loss: 0.2845\n",
      "Epoch [6/10], Step [714/938], Loss: 0.2004\n",
      "Epoch [6/10], Step [716/938], Loss: 0.4287\n",
      "Epoch [6/10], Step [718/938], Loss: 0.4196\n",
      "Epoch [6/10], Step [720/938], Loss: 0.2883\n",
      "Epoch [6/10], Step [722/938], Loss: 0.3214\n",
      "Epoch [6/10], Step [724/938], Loss: 0.3167\n",
      "Epoch [6/10], Step [726/938], Loss: 0.3919\n",
      "Epoch [6/10], Step [728/938], Loss: 0.2691\n",
      "Epoch [6/10], Step [730/938], Loss: 0.2898\n",
      "Epoch [6/10], Step [732/938], Loss: 0.5093\n",
      "Epoch [6/10], Step [734/938], Loss: 0.2742\n",
      "Epoch [6/10], Step [736/938], Loss: 0.3980\n",
      "Epoch [6/10], Step [738/938], Loss: 0.2705\n",
      "Epoch [6/10], Step [740/938], Loss: 0.3560\n",
      "Epoch [6/10], Step [742/938], Loss: 0.3443\n",
      "Epoch [6/10], Step [744/938], Loss: 0.3357\n",
      "Epoch [6/10], Step [746/938], Loss: 0.3354\n",
      "Epoch [6/10], Step [748/938], Loss: 0.2425\n",
      "Epoch [6/10], Step [750/938], Loss: 0.5216\n",
      "Epoch [6/10], Step [752/938], Loss: 0.2879\n",
      "Epoch [6/10], Step [754/938], Loss: 0.3040\n",
      "Epoch [6/10], Step [756/938], Loss: 0.4650\n",
      "Epoch [6/10], Step [758/938], Loss: 0.2698\n",
      "Epoch [6/10], Step [760/938], Loss: 0.3448\n",
      "Epoch [6/10], Step [762/938], Loss: 0.4191\n",
      "Epoch [6/10], Step [764/938], Loss: 0.5245\n",
      "Epoch [6/10], Step [766/938], Loss: 0.3724\n",
      "Epoch [6/10], Step [768/938], Loss: 0.3356\n",
      "Epoch [6/10], Step [770/938], Loss: 0.2733\n",
      "Epoch [6/10], Step [772/938], Loss: 0.3199\n",
      "Epoch [6/10], Step [774/938], Loss: 0.2764\n",
      "Epoch [6/10], Step [776/938], Loss: 0.2924\n",
      "Epoch [6/10], Step [778/938], Loss: 0.5044\n",
      "Epoch [6/10], Step [780/938], Loss: 0.2923\n",
      "Epoch [6/10], Step [782/938], Loss: 0.2998\n",
      "Epoch [6/10], Step [784/938], Loss: 0.5638\n",
      "Epoch [6/10], Step [786/938], Loss: 0.3795\n",
      "Epoch [6/10], Step [788/938], Loss: 0.2153\n",
      "Epoch [6/10], Step [790/938], Loss: 0.3745\n",
      "Epoch [6/10], Step [792/938], Loss: 0.3586\n",
      "Epoch [6/10], Step [794/938], Loss: 0.5094\n",
      "Epoch [6/10], Step [796/938], Loss: 0.3523\n",
      "Epoch [6/10], Step [798/938], Loss: 0.3687\n",
      "Epoch [6/10], Step [800/938], Loss: 0.2443\n",
      "Epoch [6/10], Step [802/938], Loss: 0.3964\n",
      "Epoch [6/10], Step [804/938], Loss: 0.2226\n",
      "Epoch [6/10], Step [806/938], Loss: 0.2884\n",
      "Epoch [6/10], Step [808/938], Loss: 0.2792\n",
      "Epoch [6/10], Step [810/938], Loss: 0.4146\n",
      "Epoch [6/10], Step [812/938], Loss: 0.3688\n",
      "Epoch [6/10], Step [814/938], Loss: 0.3481\n",
      "Epoch [6/10], Step [816/938], Loss: 0.2156\n",
      "Epoch [6/10], Step [818/938], Loss: 0.3121\n",
      "Epoch [6/10], Step [820/938], Loss: 0.5226\n",
      "Epoch [6/10], Step [822/938], Loss: 0.4510\n",
      "Epoch [6/10], Step [824/938], Loss: 0.3907\n",
      "Epoch [6/10], Step [826/938], Loss: 0.3233\n",
      "Epoch [6/10], Step [828/938], Loss: 0.4689\n",
      "Epoch [6/10], Step [830/938], Loss: 0.3379\n",
      "Epoch [6/10], Step [832/938], Loss: 0.4916\n",
      "Epoch [6/10], Step [834/938], Loss: 0.2587\n",
      "Epoch [6/10], Step [836/938], Loss: 0.2975\n",
      "Epoch [6/10], Step [838/938], Loss: 0.4632\n",
      "Epoch [6/10], Step [840/938], Loss: 0.3117\n",
      "Epoch [6/10], Step [842/938], Loss: 0.2929\n",
      "Epoch [6/10], Step [844/938], Loss: 0.2941\n",
      "Epoch [6/10], Step [846/938], Loss: 0.2637\n",
      "Epoch [6/10], Step [848/938], Loss: 0.2424\n",
      "Epoch [6/10], Step [850/938], Loss: 0.4221\n",
      "Epoch [6/10], Step [852/938], Loss: 0.3760\n",
      "Epoch [6/10], Step [854/938], Loss: 0.2323\n",
      "Epoch [6/10], Step [856/938], Loss: 0.3364\n",
      "Epoch [6/10], Step [858/938], Loss: 0.4397\n",
      "Epoch [6/10], Step [860/938], Loss: 0.4658\n",
      "Epoch [6/10], Step [862/938], Loss: 0.2930\n",
      "Epoch [6/10], Step [864/938], Loss: 0.5026\n",
      "Epoch [6/10], Step [866/938], Loss: 0.3010\n",
      "Epoch [6/10], Step [868/938], Loss: 0.3835\n",
      "Epoch [6/10], Step [870/938], Loss: 0.2331\n",
      "Epoch [6/10], Step [872/938], Loss: 0.3209\n",
      "Epoch [6/10], Step [874/938], Loss: 0.3680\n",
      "Epoch [6/10], Step [876/938], Loss: 0.3634\n",
      "Epoch [6/10], Step [878/938], Loss: 0.3706\n",
      "Epoch [6/10], Step [880/938], Loss: 0.1880\n",
      "Epoch [6/10], Step [882/938], Loss: 0.2858\n",
      "Epoch [6/10], Step [884/938], Loss: 0.2631\n",
      "Epoch [6/10], Step [886/938], Loss: 0.2766\n",
      "Epoch [6/10], Step [888/938], Loss: 0.3169\n",
      "Epoch [6/10], Step [890/938], Loss: 0.4544\n",
      "Epoch [6/10], Step [892/938], Loss: 0.2755\n",
      "Epoch [6/10], Step [894/938], Loss: 0.4256\n",
      "Epoch [6/10], Step [896/938], Loss: 0.4595\n",
      "Epoch [6/10], Step [898/938], Loss: 0.3948\n",
      "Epoch [6/10], Step [900/938], Loss: 0.3472\n",
      "Epoch [6/10], Step [902/938], Loss: 0.2714\n",
      "Epoch [6/10], Step [904/938], Loss: 0.3245\n",
      "Epoch [6/10], Step [906/938], Loss: 0.3942\n",
      "Epoch [6/10], Step [908/938], Loss: 0.3058\n",
      "Epoch [6/10], Step [910/938], Loss: 0.4271\n",
      "Epoch [6/10], Step [912/938], Loss: 0.2811\n",
      "Epoch [6/10], Step [914/938], Loss: 0.3187\n",
      "Epoch [6/10], Step [916/938], Loss: 0.3029\n",
      "Epoch [6/10], Step [918/938], Loss: 0.2604\n",
      "Epoch [6/10], Step [920/938], Loss: 0.3697\n",
      "Epoch [6/10], Step [922/938], Loss: 0.2080\n",
      "Epoch [6/10], Step [924/938], Loss: 0.3404\n",
      "Epoch [6/10], Step [926/938], Loss: 0.2832\n",
      "Epoch [6/10], Step [928/938], Loss: 0.3059\n",
      "Epoch [6/10], Step [930/938], Loss: 0.4434\n",
      "Epoch [6/10], Step [932/938], Loss: 0.2895\n",
      "Epoch [6/10], Step [934/938], Loss: 0.2828\n",
      "Epoch [6/10], Step [936/938], Loss: 0.3857\n",
      "Epoch [6/10], Step [938/938], Loss: 0.1648\n",
      "Epoch [6/10], Loss: 0.3708\n",
      "Epoch [7/10], Step [2/938], Loss: 0.2098\n",
      "Epoch [7/10], Step [4/938], Loss: 0.4757\n",
      "Epoch [7/10], Step [6/938], Loss: 0.4968\n",
      "Epoch [7/10], Step [8/938], Loss: 0.3796\n",
      "Epoch [7/10], Step [10/938], Loss: 0.3544\n",
      "Epoch [7/10], Step [12/938], Loss: 0.4949\n",
      "Epoch [7/10], Step [14/938], Loss: 0.5074\n",
      "Epoch [7/10], Step [16/938], Loss: 0.2268\n",
      "Epoch [7/10], Step [18/938], Loss: 0.2709\n",
      "Epoch [7/10], Step [20/938], Loss: 0.2367\n",
      "Epoch [7/10], Step [22/938], Loss: 0.3744\n",
      "Epoch [7/10], Step [24/938], Loss: 0.2439\n",
      "Epoch [7/10], Step [26/938], Loss: 0.3474\n",
      "Epoch [7/10], Step [28/938], Loss: 0.2549\n",
      "Epoch [7/10], Step [30/938], Loss: 0.2913\n",
      "Epoch [7/10], Step [32/938], Loss: 0.2595\n",
      "Epoch [7/10], Step [34/938], Loss: 0.3745\n",
      "Epoch [7/10], Step [36/938], Loss: 0.3858\n",
      "Epoch [7/10], Step [38/938], Loss: 0.2771\n",
      "Epoch [7/10], Step [40/938], Loss: 0.4764\n",
      "Epoch [7/10], Step [42/938], Loss: 0.2789\n",
      "Epoch [7/10], Step [44/938], Loss: 0.2367\n",
      "Epoch [7/10], Step [46/938], Loss: 0.2042\n",
      "Epoch [7/10], Step [48/938], Loss: 0.2655\n",
      "Epoch [7/10], Step [50/938], Loss: 0.2641\n",
      "Epoch [7/10], Step [52/938], Loss: 0.2306\n",
      "Epoch [7/10], Step [54/938], Loss: 0.4777\n",
      "Epoch [7/10], Step [56/938], Loss: 0.3447\n",
      "Epoch [7/10], Step [58/938], Loss: 0.4808\n",
      "Epoch [7/10], Step [60/938], Loss: 0.3653\n",
      "Epoch [7/10], Step [62/938], Loss: 0.2568\n",
      "Epoch [7/10], Step [64/938], Loss: 0.2592\n",
      "Epoch [7/10], Step [66/938], Loss: 0.4991\n",
      "Epoch [7/10], Step [68/938], Loss: 0.4031\n",
      "Epoch [7/10], Step [70/938], Loss: 0.4751\n",
      "Epoch [7/10], Step [72/938], Loss: 0.2985\n",
      "Epoch [7/10], Step [74/938], Loss: 0.1874\n",
      "Epoch [7/10], Step [76/938], Loss: 0.3105\n",
      "Epoch [7/10], Step [78/938], Loss: 0.3186\n",
      "Epoch [7/10], Step [80/938], Loss: 0.2456\n",
      "Epoch [7/10], Step [82/938], Loss: 0.3542\n",
      "Epoch [7/10], Step [84/938], Loss: 0.3431\n",
      "Epoch [7/10], Step [86/938], Loss: 0.3564\n",
      "Epoch [7/10], Step [88/938], Loss: 0.3991\n",
      "Epoch [7/10], Step [90/938], Loss: 0.3072\n",
      "Epoch [7/10], Step [92/938], Loss: 0.3403\n",
      "Epoch [7/10], Step [94/938], Loss: 0.2850\n",
      "Epoch [7/10], Step [96/938], Loss: 0.4266\n",
      "Epoch [7/10], Step [98/938], Loss: 0.4238\n",
      "Epoch [7/10], Step [100/938], Loss: 0.2955\n",
      "Epoch [7/10], Step [102/938], Loss: 0.4034\n",
      "Epoch [7/10], Step [104/938], Loss: 0.3082\n",
      "Epoch [7/10], Step [106/938], Loss: 0.3369\n",
      "Epoch [7/10], Step [108/938], Loss: 0.4222\n",
      "Epoch [7/10], Step [110/938], Loss: 0.4034\n",
      "Epoch [7/10], Step [112/938], Loss: 0.4212\n",
      "Epoch [7/10], Step [114/938], Loss: 0.2541\n",
      "Epoch [7/10], Step [116/938], Loss: 0.2566\n",
      "Epoch [7/10], Step [118/938], Loss: 0.3031\n",
      "Epoch [7/10], Step [120/938], Loss: 0.4756\n",
      "Epoch [7/10], Step [122/938], Loss: 0.4438\n",
      "Epoch [7/10], Step [124/938], Loss: 0.4059\n",
      "Epoch [7/10], Step [126/938], Loss: 0.4268\n",
      "Epoch [7/10], Step [128/938], Loss: 0.3951\n",
      "Epoch [7/10], Step [130/938], Loss: 0.3525\n",
      "Epoch [7/10], Step [132/938], Loss: 0.2710\n",
      "Epoch [7/10], Step [134/938], Loss: 0.5899\n",
      "Epoch [7/10], Step [136/938], Loss: 0.3307\n",
      "Epoch [7/10], Step [138/938], Loss: 0.5312\n",
      "Epoch [7/10], Step [140/938], Loss: 0.3912\n",
      "Epoch [7/10], Step [142/938], Loss: 0.2899\n",
      "Epoch [7/10], Step [144/938], Loss: 0.2990\n",
      "Epoch [7/10], Step [146/938], Loss: 0.3376\n",
      "Epoch [7/10], Step [148/938], Loss: 0.2490\n",
      "Epoch [7/10], Step [150/938], Loss: 0.3425\n",
      "Epoch [7/10], Step [152/938], Loss: 0.3490\n",
      "Epoch [7/10], Step [154/938], Loss: 0.2486\n",
      "Epoch [7/10], Step [156/938], Loss: 0.3644\n",
      "Epoch [7/10], Step [158/938], Loss: 0.3109\n",
      "Epoch [7/10], Step [160/938], Loss: 0.4133\n",
      "Epoch [7/10], Step [162/938], Loss: 0.2742\n",
      "Epoch [7/10], Step [164/938], Loss: 0.3670\n",
      "Epoch [7/10], Step [166/938], Loss: 0.3403\n",
      "Epoch [7/10], Step [168/938], Loss: 0.2968\n",
      "Epoch [7/10], Step [170/938], Loss: 0.4006\n",
      "Epoch [7/10], Step [172/938], Loss: 0.2508\n",
      "Epoch [7/10], Step [174/938], Loss: 0.4435\n",
      "Epoch [7/10], Step [176/938], Loss: 0.2874\n",
      "Epoch [7/10], Step [178/938], Loss: 0.3916\n",
      "Epoch [7/10], Step [180/938], Loss: 0.3245\n",
      "Epoch [7/10], Step [182/938], Loss: 0.3959\n",
      "Epoch [7/10], Step [184/938], Loss: 0.3153\n",
      "Epoch [7/10], Step [186/938], Loss: 0.2170\n",
      "Epoch [7/10], Step [188/938], Loss: 0.3890\n",
      "Epoch [7/10], Step [190/938], Loss: 0.1917\n",
      "Epoch [7/10], Step [192/938], Loss: 0.2915\n",
      "Epoch [7/10], Step [194/938], Loss: 0.3011\n",
      "Epoch [7/10], Step [196/938], Loss: 0.3708\n",
      "Epoch [7/10], Step [198/938], Loss: 0.4679\n",
      "Epoch [7/10], Step [200/938], Loss: 0.4044\n",
      "Epoch [7/10], Step [202/938], Loss: 0.3054\n",
      "Epoch [7/10], Step [204/938], Loss: 0.3404\n",
      "Epoch [7/10], Step [206/938], Loss: 0.3093\n",
      "Epoch [7/10], Step [208/938], Loss: 0.2764\n",
      "Epoch [7/10], Step [210/938], Loss: 0.2226\n",
      "Epoch [7/10], Step [212/938], Loss: 0.2770\n",
      "Epoch [7/10], Step [214/938], Loss: 0.2190\n",
      "Epoch [7/10], Step [216/938], Loss: 0.3762\n",
      "Epoch [7/10], Step [218/938], Loss: 0.2039\n",
      "Epoch [7/10], Step [220/938], Loss: 0.3053\n",
      "Epoch [7/10], Step [222/938], Loss: 0.1858\n",
      "Epoch [7/10], Step [224/938], Loss: 0.2252\n",
      "Epoch [7/10], Step [226/938], Loss: 0.2857\n",
      "Epoch [7/10], Step [228/938], Loss: 0.3195\n",
      "Epoch [7/10], Step [230/938], Loss: 0.3146\n",
      "Epoch [7/10], Step [232/938], Loss: 0.3546\n",
      "Epoch [7/10], Step [234/938], Loss: 0.3705\n",
      "Epoch [7/10], Step [236/938], Loss: 0.2369\n",
      "Epoch [7/10], Step [238/938], Loss: 0.1967\n",
      "Epoch [7/10], Step [240/938], Loss: 0.3635\n",
      "Epoch [7/10], Step [242/938], Loss: 0.3401\n",
      "Epoch [7/10], Step [244/938], Loss: 0.4361\n",
      "Epoch [7/10], Step [246/938], Loss: 0.3606\n",
      "Epoch [7/10], Step [248/938], Loss: 0.2749\n",
      "Epoch [7/10], Step [250/938], Loss: 0.4633\n",
      "Epoch [7/10], Step [252/938], Loss: 0.5622\n",
      "Epoch [7/10], Step [254/938], Loss: 0.3304\n",
      "Epoch [7/10], Step [256/938], Loss: 0.3621\n",
      "Epoch [7/10], Step [258/938], Loss: 0.3923\n",
      "Epoch [7/10], Step [260/938], Loss: 0.3131\n",
      "Epoch [7/10], Step [262/938], Loss: 0.4511\n",
      "Epoch [7/10], Step [264/938], Loss: 0.2084\n",
      "Epoch [7/10], Step [266/938], Loss: 0.4379\n",
      "Epoch [7/10], Step [268/938], Loss: 0.2794\n",
      "Epoch [7/10], Step [270/938], Loss: 0.3947\n",
      "Epoch [7/10], Step [272/938], Loss: 0.2141\n",
      "Epoch [7/10], Step [274/938], Loss: 0.3842\n",
      "Epoch [7/10], Step [276/938], Loss: 0.3237\n",
      "Epoch [7/10], Step [278/938], Loss: 0.4415\n",
      "Epoch [7/10], Step [280/938], Loss: 0.3436\n",
      "Epoch [7/10], Step [282/938], Loss: 0.2545\n",
      "Epoch [7/10], Step [284/938], Loss: 0.4159\n",
      "Epoch [7/10], Step [286/938], Loss: 0.2559\n",
      "Epoch [7/10], Step [288/938], Loss: 0.3569\n",
      "Epoch [7/10], Step [290/938], Loss: 0.3180\n",
      "Epoch [7/10], Step [292/938], Loss: 0.2368\n",
      "Epoch [7/10], Step [294/938], Loss: 0.3436\n",
      "Epoch [7/10], Step [296/938], Loss: 0.4366\n",
      "Epoch [7/10], Step [298/938], Loss: 0.5066\n",
      "Epoch [7/10], Step [300/938], Loss: 0.2090\n",
      "Epoch [7/10], Step [302/938], Loss: 0.4582\n",
      "Epoch [7/10], Step [304/938], Loss: 0.3328\n",
      "Epoch [7/10], Step [306/938], Loss: 0.2301\n",
      "Epoch [7/10], Step [308/938], Loss: 0.4011\n",
      "Epoch [7/10], Step [310/938], Loss: 0.3216\n",
      "Epoch [7/10], Step [312/938], Loss: 0.4221\n",
      "Epoch [7/10], Step [314/938], Loss: 0.1165\n",
      "Epoch [7/10], Step [316/938], Loss: 0.3472\n",
      "Epoch [7/10], Step [318/938], Loss: 0.3361\n",
      "Epoch [7/10], Step [320/938], Loss: 0.4106\n",
      "Epoch [7/10], Step [322/938], Loss: 0.3245\n",
      "Epoch [7/10], Step [324/938], Loss: 0.4978\n",
      "Epoch [7/10], Step [326/938], Loss: 0.3613\n",
      "Epoch [7/10], Step [328/938], Loss: 0.2772\n",
      "Epoch [7/10], Step [330/938], Loss: 0.2801\n",
      "Epoch [7/10], Step [332/938], Loss: 0.2189\n",
      "Epoch [7/10], Step [334/938], Loss: 0.2389\n",
      "Epoch [7/10], Step [336/938], Loss: 0.3503\n",
      "Epoch [7/10], Step [338/938], Loss: 0.3550\n",
      "Epoch [7/10], Step [340/938], Loss: 0.6065\n",
      "Epoch [7/10], Step [342/938], Loss: 0.5065\n",
      "Epoch [7/10], Step [344/938], Loss: 0.3837\n",
      "Epoch [7/10], Step [346/938], Loss: 0.2475\n",
      "Epoch [7/10], Step [348/938], Loss: 0.4847\n",
      "Epoch [7/10], Step [350/938], Loss: 0.2585\n",
      "Epoch [7/10], Step [352/938], Loss: 0.3341\n",
      "Epoch [7/10], Step [354/938], Loss: 0.2539\n",
      "Epoch [7/10], Step [356/938], Loss: 0.4080\n",
      "Epoch [7/10], Step [358/938], Loss: 0.3013\n",
      "Epoch [7/10], Step [360/938], Loss: 0.4584\n",
      "Epoch [7/10], Step [362/938], Loss: 0.3376\n",
      "Epoch [7/10], Step [364/938], Loss: 0.1996\n",
      "Epoch [7/10], Step [366/938], Loss: 0.2735\n",
      "Epoch [7/10], Step [368/938], Loss: 0.2976\n",
      "Epoch [7/10], Step [370/938], Loss: 0.2100\n",
      "Epoch [7/10], Step [372/938], Loss: 0.3489\n",
      "Epoch [7/10], Step [374/938], Loss: 0.5188\n",
      "Epoch [7/10], Step [376/938], Loss: 0.1975\n",
      "Epoch [7/10], Step [378/938], Loss: 0.2998\n",
      "Epoch [7/10], Step [380/938], Loss: 0.2361\n",
      "Epoch [7/10], Step [382/938], Loss: 0.3427\n",
      "Epoch [7/10], Step [384/938], Loss: 0.5437\n",
      "Epoch [7/10], Step [386/938], Loss: 0.3539\n",
      "Epoch [7/10], Step [388/938], Loss: 0.3658\n",
      "Epoch [7/10], Step [390/938], Loss: 0.2721\n",
      "Epoch [7/10], Step [392/938], Loss: 0.4217\n",
      "Epoch [7/10], Step [394/938], Loss: 0.3832\n",
      "Epoch [7/10], Step [396/938], Loss: 0.2897\n",
      "Epoch [7/10], Step [398/938], Loss: 0.4372\n",
      "Epoch [7/10], Step [400/938], Loss: 0.3185\n",
      "Epoch [7/10], Step [402/938], Loss: 0.4694\n",
      "Epoch [7/10], Step [404/938], Loss: 0.2571\n",
      "Epoch [7/10], Step [406/938], Loss: 0.3003\n",
      "Epoch [7/10], Step [408/938], Loss: 0.3034\n",
      "Epoch [7/10], Step [410/938], Loss: 0.2283\n",
      "Epoch [7/10], Step [412/938], Loss: 0.1962\n",
      "Epoch [7/10], Step [414/938], Loss: 0.3231\n",
      "Epoch [7/10], Step [416/938], Loss: 0.3248\n",
      "Epoch [7/10], Step [418/938], Loss: 0.3454\n",
      "Epoch [7/10], Step [420/938], Loss: 0.4732\n",
      "Epoch [7/10], Step [422/938], Loss: 0.2560\n",
      "Epoch [7/10], Step [424/938], Loss: 0.3928\n",
      "Epoch [7/10], Step [426/938], Loss: 0.2128\n",
      "Epoch [7/10], Step [428/938], Loss: 0.3080\n",
      "Epoch [7/10], Step [430/938], Loss: 0.3332\n",
      "Epoch [7/10], Step [432/938], Loss: 0.1773\n",
      "Epoch [7/10], Step [434/938], Loss: 0.2583\n",
      "Epoch [7/10], Step [436/938], Loss: 0.2876\n",
      "Epoch [7/10], Step [438/938], Loss: 0.3652\n",
      "Epoch [7/10], Step [440/938], Loss: 0.1908\n",
      "Epoch [7/10], Step [442/938], Loss: 0.3362\n",
      "Epoch [7/10], Step [444/938], Loss: 0.3059\n",
      "Epoch [7/10], Step [446/938], Loss: 0.2897\n",
      "Epoch [7/10], Step [448/938], Loss: 0.4569\n",
      "Epoch [7/10], Step [450/938], Loss: 0.3387\n",
      "Epoch [7/10], Step [452/938], Loss: 0.3710\n",
      "Epoch [7/10], Step [454/938], Loss: 0.3452\n",
      "Epoch [7/10], Step [456/938], Loss: 0.2188\n",
      "Epoch [7/10], Step [458/938], Loss: 0.2293\n",
      "Epoch [7/10], Step [460/938], Loss: 0.4106\n",
      "Epoch [7/10], Step [462/938], Loss: 0.3717\n",
      "Epoch [7/10], Step [464/938], Loss: 0.1862\n",
      "Epoch [7/10], Step [466/938], Loss: 0.3203\n",
      "Epoch [7/10], Step [468/938], Loss: 0.2649\n",
      "Epoch [7/10], Step [470/938], Loss: 0.3502\n",
      "Epoch [7/10], Step [472/938], Loss: 0.3916\n",
      "Epoch [7/10], Step [474/938], Loss: 0.3048\n",
      "Epoch [7/10], Step [476/938], Loss: 0.4125\n",
      "Epoch [7/10], Step [478/938], Loss: 0.3601\n",
      "Epoch [7/10], Step [480/938], Loss: 0.2479\n",
      "Epoch [7/10], Step [482/938], Loss: 0.2331\n",
      "Epoch [7/10], Step [484/938], Loss: 0.1862\n",
      "Epoch [7/10], Step [486/938], Loss: 0.2840\n",
      "Epoch [7/10], Step [488/938], Loss: 0.4452\n",
      "Epoch [7/10], Step [490/938], Loss: 0.2371\n",
      "Epoch [7/10], Step [492/938], Loss: 0.3936\n",
      "Epoch [7/10], Step [494/938], Loss: 0.2077\n",
      "Epoch [7/10], Step [496/938], Loss: 0.2669\n",
      "Epoch [7/10], Step [498/938], Loss: 0.3439\n",
      "Epoch [7/10], Step [500/938], Loss: 0.3010\n",
      "Epoch [7/10], Step [502/938], Loss: 0.1675\n",
      "Epoch [7/10], Step [504/938], Loss: 0.3520\n",
      "Epoch [7/10], Step [506/938], Loss: 0.4401\n",
      "Epoch [7/10], Step [508/938], Loss: 0.3064\n",
      "Epoch [7/10], Step [510/938], Loss: 0.3234\n",
      "Epoch [7/10], Step [512/938], Loss: 0.1912\n",
      "Epoch [7/10], Step [514/938], Loss: 0.1273\n",
      "Epoch [7/10], Step [516/938], Loss: 0.3999\n",
      "Epoch [7/10], Step [518/938], Loss: 0.3115\n",
      "Epoch [7/10], Step [520/938], Loss: 0.3539\n",
      "Epoch [7/10], Step [522/938], Loss: 0.3101\n",
      "Epoch [7/10], Step [524/938], Loss: 0.2864\n",
      "Epoch [7/10], Step [526/938], Loss: 0.3901\n",
      "Epoch [7/10], Step [528/938], Loss: 0.5058\n",
      "Epoch [7/10], Step [530/938], Loss: 0.2471\n",
      "Epoch [7/10], Step [532/938], Loss: 0.2781\n",
      "Epoch [7/10], Step [534/938], Loss: 0.2990\n",
      "Epoch [7/10], Step [536/938], Loss: 0.1806\n",
      "Epoch [7/10], Step [538/938], Loss: 0.1490\n",
      "Epoch [7/10], Step [540/938], Loss: 0.3169\n",
      "Epoch [7/10], Step [542/938], Loss: 0.3399\n",
      "Epoch [7/10], Step [544/938], Loss: 0.3422\n",
      "Epoch [7/10], Step [546/938], Loss: 0.2679\n",
      "Epoch [7/10], Step [548/938], Loss: 0.3495\n",
      "Epoch [7/10], Step [550/938], Loss: 0.2869\n",
      "Epoch [7/10], Step [552/938], Loss: 0.3523\n",
      "Epoch [7/10], Step [554/938], Loss: 0.2843\n",
      "Epoch [7/10], Step [556/938], Loss: 0.3415\n",
      "Epoch [7/10], Step [558/938], Loss: 0.4900\n",
      "Epoch [7/10], Step [560/938], Loss: 0.4458\n",
      "Epoch [7/10], Step [562/938], Loss: 0.3801\n",
      "Epoch [7/10], Step [564/938], Loss: 0.2583\n",
      "Epoch [7/10], Step [566/938], Loss: 0.2157\n",
      "Epoch [7/10], Step [568/938], Loss: 0.1829\n",
      "Epoch [7/10], Step [570/938], Loss: 0.2472\n",
      "Epoch [7/10], Step [572/938], Loss: 0.2617\n",
      "Epoch [7/10], Step [574/938], Loss: 0.2006\n",
      "Epoch [7/10], Step [576/938], Loss: 0.2961\n",
      "Epoch [7/10], Step [578/938], Loss: 0.2555\n",
      "Epoch [7/10], Step [580/938], Loss: 0.2058\n",
      "Epoch [7/10], Step [582/938], Loss: 0.5306\n",
      "Epoch [7/10], Step [584/938], Loss: 0.2281\n",
      "Epoch [7/10], Step [586/938], Loss: 0.3191\n",
      "Epoch [7/10], Step [588/938], Loss: 0.1634\n",
      "Epoch [7/10], Step [590/938], Loss: 0.4028\n",
      "Epoch [7/10], Step [592/938], Loss: 0.3396\n",
      "Epoch [7/10], Step [594/938], Loss: 0.3648\n",
      "Epoch [7/10], Step [596/938], Loss: 0.3110\n",
      "Epoch [7/10], Step [598/938], Loss: 0.1899\n",
      "Epoch [7/10], Step [600/938], Loss: 0.2764\n",
      "Epoch [7/10], Step [602/938], Loss: 0.3356\n",
      "Epoch [7/10], Step [604/938], Loss: 0.2046\n",
      "Epoch [7/10], Step [606/938], Loss: 0.3351\n",
      "Epoch [7/10], Step [608/938], Loss: 0.3981\n",
      "Epoch [7/10], Step [610/938], Loss: 0.4924\n",
      "Epoch [7/10], Step [612/938], Loss: 0.3006\n",
      "Epoch [7/10], Step [614/938], Loss: 0.3424\n",
      "Epoch [7/10], Step [616/938], Loss: 0.1999\n",
      "Epoch [7/10], Step [618/938], Loss: 0.4087\n",
      "Epoch [7/10], Step [620/938], Loss: 0.2424\n",
      "Epoch [7/10], Step [622/938], Loss: 0.1730\n",
      "Epoch [7/10], Step [624/938], Loss: 0.3363\n",
      "Epoch [7/10], Step [626/938], Loss: 0.3161\n",
      "Epoch [7/10], Step [628/938], Loss: 0.3601\n",
      "Epoch [7/10], Step [630/938], Loss: 0.2635\n",
      "Epoch [7/10], Step [632/938], Loss: 0.3868\n",
      "Epoch [7/10], Step [634/938], Loss: 0.4762\n",
      "Epoch [7/10], Step [636/938], Loss: 0.2259\n",
      "Epoch [7/10], Step [638/938], Loss: 0.3319\n",
      "Epoch [7/10], Step [640/938], Loss: 0.2486\n",
      "Epoch [7/10], Step [642/938], Loss: 0.1730\n",
      "Epoch [7/10], Step [644/938], Loss: 0.2279\n",
      "Epoch [7/10], Step [646/938], Loss: 0.3697\n",
      "Epoch [7/10], Step [648/938], Loss: 0.2531\n",
      "Epoch [7/10], Step [650/938], Loss: 0.3899\n",
      "Epoch [7/10], Step [652/938], Loss: 0.2270\n",
      "Epoch [7/10], Step [654/938], Loss: 0.3100\n",
      "Epoch [7/10], Step [656/938], Loss: 0.1392\n",
      "Epoch [7/10], Step [658/938], Loss: 0.2405\n",
      "Epoch [7/10], Step [660/938], Loss: 0.3226\n",
      "Epoch [7/10], Step [662/938], Loss: 0.2943\n",
      "Epoch [7/10], Step [664/938], Loss: 0.3740\n",
      "Epoch [7/10], Step [666/938], Loss: 0.3063\n",
      "Epoch [7/10], Step [668/938], Loss: 0.3738\n",
      "Epoch [7/10], Step [670/938], Loss: 0.3067\n",
      "Epoch [7/10], Step [672/938], Loss: 0.1821\n",
      "Epoch [7/10], Step [674/938], Loss: 0.3747\n",
      "Epoch [7/10], Step [676/938], Loss: 0.3742\n",
      "Epoch [7/10], Step [678/938], Loss: 0.2141\n",
      "Epoch [7/10], Step [680/938], Loss: 0.1916\n",
      "Epoch [7/10], Step [682/938], Loss: 0.2589\n",
      "Epoch [7/10], Step [684/938], Loss: 0.3000\n",
      "Epoch [7/10], Step [686/938], Loss: 0.2520\n",
      "Epoch [7/10], Step [688/938], Loss: 0.2884\n",
      "Epoch [7/10], Step [690/938], Loss: 0.3250\n",
      "Epoch [7/10], Step [692/938], Loss: 0.2531\n",
      "Epoch [7/10], Step [694/938], Loss: 0.4913\n",
      "Epoch [7/10], Step [696/938], Loss: 0.2836\n",
      "Epoch [7/10], Step [698/938], Loss: 0.2721\n",
      "Epoch [7/10], Step [700/938], Loss: 0.2890\n",
      "Epoch [7/10], Step [702/938], Loss: 0.2013\n",
      "Epoch [7/10], Step [704/938], Loss: 0.2697\n",
      "Epoch [7/10], Step [706/938], Loss: 0.3038\n",
      "Epoch [7/10], Step [708/938], Loss: 0.3350\n",
      "Epoch [7/10], Step [710/938], Loss: 0.2808\n",
      "Epoch [7/10], Step [712/938], Loss: 0.2663\n",
      "Epoch [7/10], Step [714/938], Loss: 0.2934\n",
      "Epoch [7/10], Step [716/938], Loss: 0.2990\n",
      "Epoch [7/10], Step [718/938], Loss: 0.2777\n",
      "Epoch [7/10], Step [720/938], Loss: 0.3604\n",
      "Epoch [7/10], Step [722/938], Loss: 0.4120\n",
      "Epoch [7/10], Step [724/938], Loss: 0.2774\n",
      "Epoch [7/10], Step [726/938], Loss: 0.2315\n",
      "Epoch [7/10], Step [728/938], Loss: 0.2563\n",
      "Epoch [7/10], Step [730/938], Loss: 0.2133\n",
      "Epoch [7/10], Step [732/938], Loss: 0.2835\n",
      "Epoch [7/10], Step [734/938], Loss: 0.2754\n",
      "Epoch [7/10], Step [736/938], Loss: 0.3581\n",
      "Epoch [7/10], Step [738/938], Loss: 0.3407\n",
      "Epoch [7/10], Step [740/938], Loss: 0.2655\n",
      "Epoch [7/10], Step [742/938], Loss: 0.2351\n",
      "Epoch [7/10], Step [744/938], Loss: 0.2550\n",
      "Epoch [7/10], Step [746/938], Loss: 0.3500\n",
      "Epoch [7/10], Step [748/938], Loss: 0.3246\n",
      "Epoch [7/10], Step [750/938], Loss: 0.2665\n",
      "Epoch [7/10], Step [752/938], Loss: 0.3022\n",
      "Epoch [7/10], Step [754/938], Loss: 0.1226\n",
      "Epoch [7/10], Step [756/938], Loss: 0.2845\n",
      "Epoch [7/10], Step [758/938], Loss: 0.3384\n",
      "Epoch [7/10], Step [760/938], Loss: 0.2101\n",
      "Epoch [7/10], Step [762/938], Loss: 0.4097\n",
      "Epoch [7/10], Step [764/938], Loss: 0.2674\n",
      "Epoch [7/10], Step [766/938], Loss: 0.3744\n",
      "Epoch [7/10], Step [768/938], Loss: 0.5223\n",
      "Epoch [7/10], Step [770/938], Loss: 0.2450\n",
      "Epoch [7/10], Step [772/938], Loss: 0.3391\n",
      "Epoch [7/10], Step [774/938], Loss: 0.2804\n",
      "Epoch [7/10], Step [776/938], Loss: 0.2072\n",
      "Epoch [7/10], Step [778/938], Loss: 0.2196\n",
      "Epoch [7/10], Step [780/938], Loss: 0.1604\n",
      "Epoch [7/10], Step [782/938], Loss: 0.2834\n",
      "Epoch [7/10], Step [784/938], Loss: 0.3206\n",
      "Epoch [7/10], Step [786/938], Loss: 0.2448\n",
      "Epoch [7/10], Step [788/938], Loss: 0.2791\n",
      "Epoch [7/10], Step [790/938], Loss: 0.2721\n",
      "Epoch [7/10], Step [792/938], Loss: 0.1866\n",
      "Epoch [7/10], Step [794/938], Loss: 0.3630\n",
      "Epoch [7/10], Step [796/938], Loss: 0.2605\n",
      "Epoch [7/10], Step [798/938], Loss: 0.3500\n",
      "Epoch [7/10], Step [800/938], Loss: 0.3813\n",
      "Epoch [7/10], Step [802/938], Loss: 0.2329\n",
      "Epoch [7/10], Step [804/938], Loss: 0.5761\n",
      "Epoch [7/10], Step [806/938], Loss: 0.3583\n",
      "Epoch [7/10], Step [808/938], Loss: 0.3505\n",
      "Epoch [7/10], Step [810/938], Loss: 0.3165\n",
      "Epoch [7/10], Step [812/938], Loss: 0.3980\n",
      "Epoch [7/10], Step [814/938], Loss: 0.1893\n",
      "Epoch [7/10], Step [816/938], Loss: 0.3146\n",
      "Epoch [7/10], Step [818/938], Loss: 0.1543\n",
      "Epoch [7/10], Step [820/938], Loss: 0.2762\n",
      "Epoch [7/10], Step [822/938], Loss: 0.2577\n",
      "Epoch [7/10], Step [824/938], Loss: 0.1732\n",
      "Epoch [7/10], Step [826/938], Loss: 0.1951\n",
      "Epoch [7/10], Step [828/938], Loss: 0.4592\n",
      "Epoch [7/10], Step [830/938], Loss: 0.4218\n",
      "Epoch [7/10], Step [832/938], Loss: 0.1691\n",
      "Epoch [7/10], Step [834/938], Loss: 0.3817\n",
      "Epoch [7/10], Step [836/938], Loss: 0.2076\n",
      "Epoch [7/10], Step [838/938], Loss: 0.5491\n",
      "Epoch [7/10], Step [840/938], Loss: 0.2628\n",
      "Epoch [7/10], Step [842/938], Loss: 0.4768\n",
      "Epoch [7/10], Step [844/938], Loss: 0.3832\n",
      "Epoch [7/10], Step [846/938], Loss: 0.2477\n",
      "Epoch [7/10], Step [848/938], Loss: 0.2512\n",
      "Epoch [7/10], Step [850/938], Loss: 0.3207\n",
      "Epoch [7/10], Step [852/938], Loss: 0.2990\n",
      "Epoch [7/10], Step [854/938], Loss: 0.3006\n",
      "Epoch [7/10], Step [856/938], Loss: 0.3675\n",
      "Epoch [7/10], Step [858/938], Loss: 0.2126\n",
      "Epoch [7/10], Step [860/938], Loss: 0.2435\n",
      "Epoch [7/10], Step [862/938], Loss: 0.4445\n",
      "Epoch [7/10], Step [864/938], Loss: 0.1600\n",
      "Epoch [7/10], Step [866/938], Loss: 0.3193\n",
      "Epoch [7/10], Step [868/938], Loss: 0.4687\n",
      "Epoch [7/10], Step [870/938], Loss: 0.2198\n",
      "Epoch [7/10], Step [872/938], Loss: 0.2868\n",
      "Epoch [7/10], Step [874/938], Loss: 0.3769\n",
      "Epoch [7/10], Step [876/938], Loss: 0.1767\n",
      "Epoch [7/10], Step [878/938], Loss: 0.2296\n",
      "Epoch [7/10], Step [880/938], Loss: 0.2606\n",
      "Epoch [7/10], Step [882/938], Loss: 0.1398\n",
      "Epoch [7/10], Step [884/938], Loss: 0.2375\n",
      "Epoch [7/10], Step [886/938], Loss: 0.3372\n",
      "Epoch [7/10], Step [888/938], Loss: 0.4078\n",
      "Epoch [7/10], Step [890/938], Loss: 0.2631\n",
      "Epoch [7/10], Step [892/938], Loss: 0.2880\n",
      "Epoch [7/10], Step [894/938], Loss: 0.3461\n",
      "Epoch [7/10], Step [896/938], Loss: 0.3852\n",
      "Epoch [7/10], Step [898/938], Loss: 0.2798\n",
      "Epoch [7/10], Step [900/938], Loss: 0.2072\n",
      "Epoch [7/10], Step [902/938], Loss: 0.2323\n",
      "Epoch [7/10], Step [904/938], Loss: 0.2743\n",
      "Epoch [7/10], Step [906/938], Loss: 0.1806\n",
      "Epoch [7/10], Step [908/938], Loss: 0.2481\n",
      "Epoch [7/10], Step [910/938], Loss: 0.2662\n",
      "Epoch [7/10], Step [912/938], Loss: 0.3039\n",
      "Epoch [7/10], Step [914/938], Loss: 0.4513\n",
      "Epoch [7/10], Step [916/938], Loss: 0.2243\n",
      "Epoch [7/10], Step [918/938], Loss: 0.4004\n",
      "Epoch [7/10], Step [920/938], Loss: 0.2688\n",
      "Epoch [7/10], Step [922/938], Loss: 0.2662\n",
      "Epoch [7/10], Step [924/938], Loss: 0.2892\n",
      "Epoch [7/10], Step [926/938], Loss: 0.3535\n",
      "Epoch [7/10], Step [928/938], Loss: 0.2103\n",
      "Epoch [7/10], Step [930/938], Loss: 0.2311\n",
      "Epoch [7/10], Step [932/938], Loss: 0.3284\n",
      "Epoch [7/10], Step [934/938], Loss: 0.2229\n",
      "Epoch [7/10], Step [936/938], Loss: 0.2197\n",
      "Epoch [7/10], Step [938/938], Loss: 0.1627\n",
      "Epoch [7/10], Loss: 0.3164\n",
      "Epoch [8/10], Step [2/938], Loss: 0.2687\n",
      "Epoch [8/10], Step [4/938], Loss: 0.3123\n",
      "Epoch [8/10], Step [6/938], Loss: 0.2903\n",
      "Epoch [8/10], Step [8/938], Loss: 0.4613\n",
      "Epoch [8/10], Step [10/938], Loss: 0.2795\n",
      "Epoch [8/10], Step [12/938], Loss: 0.3392\n",
      "Epoch [8/10], Step [14/938], Loss: 0.4137\n",
      "Epoch [8/10], Step [16/938], Loss: 0.2404\n",
      "Epoch [8/10], Step [18/938], Loss: 0.2526\n",
      "Epoch [8/10], Step [20/938], Loss: 0.3981\n",
      "Epoch [8/10], Step [22/938], Loss: 0.3726\n",
      "Epoch [8/10], Step [24/938], Loss: 0.4288\n",
      "Epoch [8/10], Step [26/938], Loss: 0.4574\n",
      "Epoch [8/10], Step [28/938], Loss: 0.3222\n",
      "Epoch [8/10], Step [30/938], Loss: 0.1701\n",
      "Epoch [8/10], Step [32/938], Loss: 0.2875\n",
      "Epoch [8/10], Step [34/938], Loss: 0.4232\n",
      "Epoch [8/10], Step [36/938], Loss: 0.3274\n",
      "Epoch [8/10], Step [38/938], Loss: 0.3106\n",
      "Epoch [8/10], Step [40/938], Loss: 0.2251\n",
      "Epoch [8/10], Step [42/938], Loss: 0.3489\n",
      "Epoch [8/10], Step [44/938], Loss: 0.2070\n",
      "Epoch [8/10], Step [46/938], Loss: 0.1765\n",
      "Epoch [8/10], Step [48/938], Loss: 0.3420\n",
      "Epoch [8/10], Step [50/938], Loss: 0.1961\n",
      "Epoch [8/10], Step [52/938], Loss: 0.2437\n",
      "Epoch [8/10], Step [54/938], Loss: 0.2910\n",
      "Epoch [8/10], Step [56/938], Loss: 0.2929\n",
      "Epoch [8/10], Step [58/938], Loss: 0.3429\n",
      "Epoch [8/10], Step [60/938], Loss: 0.3141\n",
      "Epoch [8/10], Step [62/938], Loss: 0.3039\n",
      "Epoch [8/10], Step [64/938], Loss: 0.2567\n",
      "Epoch [8/10], Step [66/938], Loss: 0.5472\n",
      "Epoch [8/10], Step [68/938], Loss: 0.2715\n",
      "Epoch [8/10], Step [70/938], Loss: 0.3418\n",
      "Epoch [8/10], Step [72/938], Loss: 0.3930\n",
      "Epoch [8/10], Step [74/938], Loss: 0.1446\n",
      "Epoch [8/10], Step [76/938], Loss: 0.4369\n",
      "Epoch [8/10], Step [78/938], Loss: 0.1302\n",
      "Epoch [8/10], Step [80/938], Loss: 0.2345\n",
      "Epoch [8/10], Step [82/938], Loss: 0.2366\n",
      "Epoch [8/10], Step [84/938], Loss: 0.3021\n",
      "Epoch [8/10], Step [86/938], Loss: 0.2875\n",
      "Epoch [8/10], Step [88/938], Loss: 0.3776\n",
      "Epoch [8/10], Step [90/938], Loss: 0.2137\n",
      "Epoch [8/10], Step [92/938], Loss: 0.2509\n",
      "Epoch [8/10], Step [94/938], Loss: 0.1609\n",
      "Epoch [8/10], Step [96/938], Loss: 0.2759\n",
      "Epoch [8/10], Step [98/938], Loss: 0.3007\n",
      "Epoch [8/10], Step [100/938], Loss: 0.4690\n",
      "Epoch [8/10], Step [102/938], Loss: 0.1628\n",
      "Epoch [8/10], Step [104/938], Loss: 0.2949\n",
      "Epoch [8/10], Step [106/938], Loss: 0.3508\n",
      "Epoch [8/10], Step [108/938], Loss: 0.3227\n",
      "Epoch [8/10], Step [110/938], Loss: 0.5912\n",
      "Epoch [8/10], Step [112/938], Loss: 0.3554\n",
      "Epoch [8/10], Step [114/938], Loss: 0.1614\n",
      "Epoch [8/10], Step [116/938], Loss: 0.3824\n",
      "Epoch [8/10], Step [118/938], Loss: 0.3448\n",
      "Epoch [8/10], Step [120/938], Loss: 0.3774\n",
      "Epoch [8/10], Step [122/938], Loss: 0.2393\n",
      "Epoch [8/10], Step [124/938], Loss: 0.3672\n",
      "Epoch [8/10], Step [126/938], Loss: 0.3117\n",
      "Epoch [8/10], Step [128/938], Loss: 0.1627\n",
      "Epoch [8/10], Step [130/938], Loss: 0.1480\n",
      "Epoch [8/10], Step [132/938], Loss: 0.3984\n",
      "Epoch [8/10], Step [134/938], Loss: 0.2455\n",
      "Epoch [8/10], Step [136/938], Loss: 0.3362\n",
      "Epoch [8/10], Step [138/938], Loss: 0.2127\n",
      "Epoch [8/10], Step [140/938], Loss: 0.2490\n",
      "Epoch [8/10], Step [142/938], Loss: 0.3073\n",
      "Epoch [8/10], Step [144/938], Loss: 0.4542\n",
      "Epoch [8/10], Step [146/938], Loss: 0.2743\n",
      "Epoch [8/10], Step [148/938], Loss: 0.1953\n",
      "Epoch [8/10], Step [150/938], Loss: 0.2981\n",
      "Epoch [8/10], Step [152/938], Loss: 0.3741\n",
      "Epoch [8/10], Step [154/938], Loss: 0.3269\n",
      "Epoch [8/10], Step [156/938], Loss: 0.3557\n",
      "Epoch [8/10], Step [158/938], Loss: 0.3071\n",
      "Epoch [8/10], Step [160/938], Loss: 0.2978\n",
      "Epoch [8/10], Step [162/938], Loss: 0.1761\n",
      "Epoch [8/10], Step [164/938], Loss: 0.2509\n",
      "Epoch [8/10], Step [166/938], Loss: 0.4853\n",
      "Epoch [8/10], Step [168/938], Loss: 0.2703\n",
      "Epoch [8/10], Step [170/938], Loss: 0.3663\n",
      "Epoch [8/10], Step [172/938], Loss: 0.2551\n",
      "Epoch [8/10], Step [174/938], Loss: 0.2392\n",
      "Epoch [8/10], Step [176/938], Loss: 0.3062\n",
      "Epoch [8/10], Step [178/938], Loss: 0.4335\n",
      "Epoch [8/10], Step [180/938], Loss: 0.3260\n",
      "Epoch [8/10], Step [182/938], Loss: 0.2967\n",
      "Epoch [8/10], Step [184/938], Loss: 0.2191\n",
      "Epoch [8/10], Step [186/938], Loss: 0.3425\n",
      "Epoch [8/10], Step [188/938], Loss: 0.2014\n",
      "Epoch [8/10], Step [190/938], Loss: 0.2218\n",
      "Epoch [8/10], Step [192/938], Loss: 0.2027\n",
      "Epoch [8/10], Step [194/938], Loss: 0.4842\n",
      "Epoch [8/10], Step [196/938], Loss: 0.3551\n",
      "Epoch [8/10], Step [198/938], Loss: 0.3343\n",
      "Epoch [8/10], Step [200/938], Loss: 0.2502\n",
      "Epoch [8/10], Step [202/938], Loss: 0.1831\n",
      "Epoch [8/10], Step [204/938], Loss: 0.3549\n",
      "Epoch [8/10], Step [206/938], Loss: 0.2896\n",
      "Epoch [8/10], Step [208/938], Loss: 0.2879\n",
      "Epoch [8/10], Step [210/938], Loss: 0.1766\n",
      "Epoch [8/10], Step [212/938], Loss: 0.3297\n",
      "Epoch [8/10], Step [214/938], Loss: 0.2979\n",
      "Epoch [8/10], Step [216/938], Loss: 0.2652\n",
      "Epoch [8/10], Step [218/938], Loss: 0.3916\n",
      "Epoch [8/10], Step [220/938], Loss: 0.2532\n",
      "Epoch [8/10], Step [222/938], Loss: 0.1832\n",
      "Epoch [8/10], Step [224/938], Loss: 0.1868\n",
      "Epoch [8/10], Step [226/938], Loss: 0.1736\n",
      "Epoch [8/10], Step [228/938], Loss: 0.3813\n",
      "Epoch [8/10], Step [230/938], Loss: 0.2710\n",
      "Epoch [8/10], Step [232/938], Loss: 0.1441\n",
      "Epoch [8/10], Step [234/938], Loss: 0.3573\n",
      "Epoch [8/10], Step [236/938], Loss: 0.3220\n",
      "Epoch [8/10], Step [238/938], Loss: 0.3261\n",
      "Epoch [8/10], Step [240/938], Loss: 0.2224\n",
      "Epoch [8/10], Step [242/938], Loss: 0.1815\n",
      "Epoch [8/10], Step [244/938], Loss: 0.3661\n",
      "Epoch [8/10], Step [246/938], Loss: 0.4286\n",
      "Epoch [8/10], Step [248/938], Loss: 0.3415\n",
      "Epoch [8/10], Step [250/938], Loss: 0.3514\n",
      "Epoch [8/10], Step [252/938], Loss: 0.2227\n",
      "Epoch [8/10], Step [254/938], Loss: 0.1580\n",
      "Epoch [8/10], Step [256/938], Loss: 0.2234\n",
      "Epoch [8/10], Step [258/938], Loss: 0.3049\n",
      "Epoch [8/10], Step [260/938], Loss: 0.1493\n",
      "Epoch [8/10], Step [262/938], Loss: 0.5079\n",
      "Epoch [8/10], Step [264/938], Loss: 0.2547\n",
      "Epoch [8/10], Step [266/938], Loss: 0.3537\n",
      "Epoch [8/10], Step [268/938], Loss: 0.2511\n",
      "Epoch [8/10], Step [270/938], Loss: 0.2781\n",
      "Epoch [8/10], Step [272/938], Loss: 0.4315\n",
      "Epoch [8/10], Step [274/938], Loss: 0.1942\n",
      "Epoch [8/10], Step [276/938], Loss: 0.2855\n",
      "Epoch [8/10], Step [278/938], Loss: 0.1921\n",
      "Epoch [8/10], Step [280/938], Loss: 0.2323\n",
      "Epoch [8/10], Step [282/938], Loss: 0.3244\n",
      "Epoch [8/10], Step [284/938], Loss: 0.2648\n",
      "Epoch [8/10], Step [286/938], Loss: 0.3114\n",
      "Epoch [8/10], Step [288/938], Loss: 0.1720\n",
      "Epoch [8/10], Step [290/938], Loss: 0.2795\n",
      "Epoch [8/10], Step [292/938], Loss: 0.2766\n",
      "Epoch [8/10], Step [294/938], Loss: 0.4304\n",
      "Epoch [8/10], Step [296/938], Loss: 0.2738\n",
      "Epoch [8/10], Step [298/938], Loss: 0.2786\n",
      "Epoch [8/10], Step [300/938], Loss: 0.2654\n",
      "Epoch [8/10], Step [302/938], Loss: 0.5282\n",
      "Epoch [8/10], Step [304/938], Loss: 0.2246\n",
      "Epoch [8/10], Step [306/938], Loss: 0.2632\n",
      "Epoch [8/10], Step [308/938], Loss: 0.2996\n",
      "Epoch [8/10], Step [310/938], Loss: 0.2062\n",
      "Epoch [8/10], Step [312/938], Loss: 0.2734\n",
      "Epoch [8/10], Step [314/938], Loss: 0.3063\n",
      "Epoch [8/10], Step [316/938], Loss: 0.2212\n",
      "Epoch [8/10], Step [318/938], Loss: 0.2184\n",
      "Epoch [8/10], Step [320/938], Loss: 0.1503\n",
      "Epoch [8/10], Step [322/938], Loss: 0.2189\n",
      "Epoch [8/10], Step [324/938], Loss: 0.1832\n",
      "Epoch [8/10], Step [326/938], Loss: 0.1903\n",
      "Epoch [8/10], Step [328/938], Loss: 0.2414\n",
      "Epoch [8/10], Step [330/938], Loss: 0.3192\n",
      "Epoch [8/10], Step [332/938], Loss: 0.4316\n",
      "Epoch [8/10], Step [334/938], Loss: 0.2542\n",
      "Epoch [8/10], Step [336/938], Loss: 0.3388\n",
      "Epoch [8/10], Step [338/938], Loss: 0.3665\n",
      "Epoch [8/10], Step [340/938], Loss: 0.3237\n",
      "Epoch [8/10], Step [342/938], Loss: 0.2503\n",
      "Epoch [8/10], Step [344/938], Loss: 0.1890\n",
      "Epoch [8/10], Step [346/938], Loss: 0.1937\n",
      "Epoch [8/10], Step [348/938], Loss: 0.2172\n",
      "Epoch [8/10], Step [350/938], Loss: 0.2110\n",
      "Epoch [8/10], Step [352/938], Loss: 0.6053\n",
      "Epoch [8/10], Step [354/938], Loss: 0.2512\n",
      "Epoch [8/10], Step [356/938], Loss: 0.2684\n",
      "Epoch [8/10], Step [358/938], Loss: 0.3760\n",
      "Epoch [8/10], Step [360/938], Loss: 0.2847\n",
      "Epoch [8/10], Step [362/938], Loss: 0.3323\n",
      "Epoch [8/10], Step [364/938], Loss: 0.2394\n",
      "Epoch [8/10], Step [366/938], Loss: 0.3192\n",
      "Epoch [8/10], Step [368/938], Loss: 0.2673\n",
      "Epoch [8/10], Step [370/938], Loss: 0.4198\n",
      "Epoch [8/10], Step [372/938], Loss: 0.2546\n",
      "Epoch [8/10], Step [374/938], Loss: 0.2662\n",
      "Epoch [8/10], Step [376/938], Loss: 0.2262\n",
      "Epoch [8/10], Step [378/938], Loss: 0.1836\n",
      "Epoch [8/10], Step [380/938], Loss: 0.2709\n",
      "Epoch [8/10], Step [382/938], Loss: 0.2063\n",
      "Epoch [8/10], Step [384/938], Loss: 0.1857\n",
      "Epoch [8/10], Step [386/938], Loss: 0.3372\n",
      "Epoch [8/10], Step [388/938], Loss: 0.1864\n",
      "Epoch [8/10], Step [390/938], Loss: 0.2352\n",
      "Epoch [8/10], Step [392/938], Loss: 0.2513\n",
      "Epoch [8/10], Step [394/938], Loss: 0.4176\n",
      "Epoch [8/10], Step [396/938], Loss: 0.1523\n",
      "Epoch [8/10], Step [398/938], Loss: 0.4993\n",
      "Epoch [8/10], Step [400/938], Loss: 0.2647\n",
      "Epoch [8/10], Step [402/938], Loss: 0.4644\n",
      "Epoch [8/10], Step [404/938], Loss: 0.1249\n",
      "Epoch [8/10], Step [406/938], Loss: 0.4140\n",
      "Epoch [8/10], Step [408/938], Loss: 0.3532\n",
      "Epoch [8/10], Step [410/938], Loss: 0.2343\n",
      "Epoch [8/10], Step [412/938], Loss: 0.1497\n",
      "Epoch [8/10], Step [414/938], Loss: 0.1747\n",
      "Epoch [8/10], Step [416/938], Loss: 0.1474\n",
      "Epoch [8/10], Step [418/938], Loss: 0.3512\n",
      "Epoch [8/10], Step [420/938], Loss: 0.1881\n",
      "Epoch [8/10], Step [422/938], Loss: 0.3733\n",
      "Epoch [8/10], Step [424/938], Loss: 0.2151\n",
      "Epoch [8/10], Step [426/938], Loss: 0.3858\n",
      "Epoch [8/10], Step [428/938], Loss: 0.2304\n",
      "Epoch [8/10], Step [430/938], Loss: 0.3580\n",
      "Epoch [8/10], Step [432/938], Loss: 0.2135\n",
      "Epoch [8/10], Step [434/938], Loss: 0.3572\n",
      "Epoch [8/10], Step [436/938], Loss: 0.2371\n",
      "Epoch [8/10], Step [438/938], Loss: 0.5051\n",
      "Epoch [8/10], Step [440/938], Loss: 0.2067\n",
      "Epoch [8/10], Step [442/938], Loss: 0.2038\n",
      "Epoch [8/10], Step [444/938], Loss: 0.2382\n",
      "Epoch [8/10], Step [446/938], Loss: 0.3385\n",
      "Epoch [8/10], Step [448/938], Loss: 0.1867\n",
      "Epoch [8/10], Step [450/938], Loss: 0.3957\n",
      "Epoch [8/10], Step [452/938], Loss: 0.4381\n",
      "Epoch [8/10], Step [454/938], Loss: 0.1633\n",
      "Epoch [8/10], Step [456/938], Loss: 0.2895\n",
      "Epoch [8/10], Step [458/938], Loss: 0.1713\n",
      "Epoch [8/10], Step [460/938], Loss: 0.5038\n",
      "Epoch [8/10], Step [462/938], Loss: 0.1794\n",
      "Epoch [8/10], Step [464/938], Loss: 0.1598\n",
      "Epoch [8/10], Step [466/938], Loss: 0.2300\n",
      "Epoch [8/10], Step [468/938], Loss: 0.1753\n",
      "Epoch [8/10], Step [470/938], Loss: 0.2678\n",
      "Epoch [8/10], Step [472/938], Loss: 0.4132\n",
      "Epoch [8/10], Step [474/938], Loss: 0.1548\n",
      "Epoch [8/10], Step [476/938], Loss: 0.3462\n",
      "Epoch [8/10], Step [478/938], Loss: 0.2037\n",
      "Epoch [8/10], Step [480/938], Loss: 0.2198\n",
      "Epoch [8/10], Step [482/938], Loss: 0.4708\n",
      "Epoch [8/10], Step [484/938], Loss: 0.2859\n",
      "Epoch [8/10], Step [486/938], Loss: 0.2547\n",
      "Epoch [8/10], Step [488/938], Loss: 0.3048\n",
      "Epoch [8/10], Step [490/938], Loss: 0.4153\n",
      "Epoch [8/10], Step [492/938], Loss: 0.4115\n",
      "Epoch [8/10], Step [494/938], Loss: 0.2571\n",
      "Epoch [8/10], Step [496/938], Loss: 0.1778\n",
      "Epoch [8/10], Step [498/938], Loss: 0.1746\n",
      "Epoch [8/10], Step [500/938], Loss: 0.4579\n",
      "Epoch [8/10], Step [502/938], Loss: 0.2532\n",
      "Epoch [8/10], Step [504/938], Loss: 0.2781\n",
      "Epoch [8/10], Step [506/938], Loss: 0.3252\n",
      "Epoch [8/10], Step [508/938], Loss: 0.3389\n",
      "Epoch [8/10], Step [510/938], Loss: 0.3757\n",
      "Epoch [8/10], Step [512/938], Loss: 0.4374\n",
      "Epoch [8/10], Step [514/938], Loss: 0.1958\n",
      "Epoch [8/10], Step [516/938], Loss: 0.2481\n",
      "Epoch [8/10], Step [518/938], Loss: 0.3282\n",
      "Epoch [8/10], Step [520/938], Loss: 0.5119\n",
      "Epoch [8/10], Step [522/938], Loss: 0.2326\n",
      "Epoch [8/10], Step [524/938], Loss: 0.2904\n",
      "Epoch [8/10], Step [526/938], Loss: 0.2917\n",
      "Epoch [8/10], Step [528/938], Loss: 0.2664\n",
      "Epoch [8/10], Step [530/938], Loss: 0.2729\n",
      "Epoch [8/10], Step [532/938], Loss: 0.2118\n",
      "Epoch [8/10], Step [534/938], Loss: 0.2506\n",
      "Epoch [8/10], Step [536/938], Loss: 0.3111\n",
      "Epoch [8/10], Step [538/938], Loss: 0.2088\n",
      "Epoch [8/10], Step [540/938], Loss: 0.1391\n",
      "Epoch [8/10], Step [542/938], Loss: 0.2365\n",
      "Epoch [8/10], Step [544/938], Loss: 0.1909\n",
      "Epoch [8/10], Step [546/938], Loss: 0.3405\n",
      "Epoch [8/10], Step [548/938], Loss: 0.4074\n",
      "Epoch [8/10], Step [550/938], Loss: 0.3301\n",
      "Epoch [8/10], Step [552/938], Loss: 0.2979\n",
      "Epoch [8/10], Step [554/938], Loss: 0.3757\n",
      "Epoch [8/10], Step [556/938], Loss: 0.1693\n",
      "Epoch [8/10], Step [558/938], Loss: 0.2213\n",
      "Epoch [8/10], Step [560/938], Loss: 0.2975\n",
      "Epoch [8/10], Step [562/938], Loss: 0.2418\n",
      "Epoch [8/10], Step [564/938], Loss: 0.3491\n",
      "Epoch [8/10], Step [566/938], Loss: 0.2786\n",
      "Epoch [8/10], Step [568/938], Loss: 0.1911\n",
      "Epoch [8/10], Step [570/938], Loss: 0.2222\n",
      "Epoch [8/10], Step [572/938], Loss: 0.2974\n",
      "Epoch [8/10], Step [574/938], Loss: 0.2241\n",
      "Epoch [8/10], Step [576/938], Loss: 0.5698\n",
      "Epoch [8/10], Step [578/938], Loss: 0.2104\n",
      "Epoch [8/10], Step [580/938], Loss: 0.3739\n",
      "Epoch [8/10], Step [582/938], Loss: 0.3721\n",
      "Epoch [8/10], Step [584/938], Loss: 0.1726\n",
      "Epoch [8/10], Step [586/938], Loss: 0.3183\n",
      "Epoch [8/10], Step [588/938], Loss: 0.3372\n",
      "Epoch [8/10], Step [590/938], Loss: 0.2523\n",
      "Epoch [8/10], Step [592/938], Loss: 0.2954\n",
      "Epoch [8/10], Step [594/938], Loss: 0.2439\n",
      "Epoch [8/10], Step [596/938], Loss: 0.3204\n",
      "Epoch [8/10], Step [598/938], Loss: 0.2877\n",
      "Epoch [8/10], Step [600/938], Loss: 0.3198\n",
      "Epoch [8/10], Step [602/938], Loss: 0.2043\n",
      "Epoch [8/10], Step [604/938], Loss: 0.2298\n",
      "Epoch [8/10], Step [606/938], Loss: 0.2332\n",
      "Epoch [8/10], Step [608/938], Loss: 0.3265\n",
      "Epoch [8/10], Step [610/938], Loss: 0.2604\n",
      "Epoch [8/10], Step [612/938], Loss: 0.2824\n",
      "Epoch [8/10], Step [614/938], Loss: 0.3910\n",
      "Epoch [8/10], Step [616/938], Loss: 0.3212\n",
      "Epoch [8/10], Step [618/938], Loss: 0.3844\n",
      "Epoch [8/10], Step [620/938], Loss: 0.3037\n",
      "Epoch [8/10], Step [622/938], Loss: 0.3817\n",
      "Epoch [8/10], Step [624/938], Loss: 0.3155\n",
      "Epoch [8/10], Step [626/938], Loss: 0.2238\n",
      "Epoch [8/10], Step [628/938], Loss: 0.2897\n",
      "Epoch [8/10], Step [630/938], Loss: 0.2204\n",
      "Epoch [8/10], Step [632/938], Loss: 0.2141\n",
      "Epoch [8/10], Step [634/938], Loss: 0.2886\n",
      "Epoch [8/10], Step [636/938], Loss: 0.1555\n",
      "Epoch [8/10], Step [638/938], Loss: 0.2658\n",
      "Epoch [8/10], Step [640/938], Loss: 0.2027\n",
      "Epoch [8/10], Step [642/938], Loss: 0.3041\n",
      "Epoch [8/10], Step [644/938], Loss: 0.3435\n",
      "Epoch [8/10], Step [646/938], Loss: 0.1963\n",
      "Epoch [8/10], Step [648/938], Loss: 0.1609\n",
      "Epoch [8/10], Step [650/938], Loss: 0.2939\n",
      "Epoch [8/10], Step [652/938], Loss: 0.2211\n",
      "Epoch [8/10], Step [654/938], Loss: 0.2648\n",
      "Epoch [8/10], Step [656/938], Loss: 0.1711\n",
      "Epoch [8/10], Step [658/938], Loss: 0.2155\n",
      "Epoch [8/10], Step [660/938], Loss: 0.3368\n",
      "Epoch [8/10], Step [662/938], Loss: 0.1939\n",
      "Epoch [8/10], Step [664/938], Loss: 0.2095\n",
      "Epoch [8/10], Step [666/938], Loss: 0.1865\n",
      "Epoch [8/10], Step [668/938], Loss: 0.3274\n",
      "Epoch [8/10], Step [670/938], Loss: 0.3137\n",
      "Epoch [8/10], Step [672/938], Loss: 0.3271\n",
      "Epoch [8/10], Step [674/938], Loss: 0.2019\n",
      "Epoch [8/10], Step [676/938], Loss: 0.2464\n",
      "Epoch [8/10], Step [678/938], Loss: 0.2121\n",
      "Epoch [8/10], Step [680/938], Loss: 0.3585\n",
      "Epoch [8/10], Step [682/938], Loss: 0.4105\n",
      "Epoch [8/10], Step [684/938], Loss: 0.2946\n",
      "Epoch [8/10], Step [686/938], Loss: 0.3312\n",
      "Epoch [8/10], Step [688/938], Loss: 0.2736\n",
      "Epoch [8/10], Step [690/938], Loss: 0.3721\n",
      "Epoch [8/10], Step [692/938], Loss: 0.2414\n",
      "Epoch [8/10], Step [694/938], Loss: 0.3165\n",
      "Epoch [8/10], Step [696/938], Loss: 0.1989\n",
      "Epoch [8/10], Step [698/938], Loss: 0.2318\n",
      "Epoch [8/10], Step [700/938], Loss: 0.3178\n",
      "Epoch [8/10], Step [702/938], Loss: 0.3698\n",
      "Epoch [8/10], Step [704/938], Loss: 0.2166\n",
      "Epoch [8/10], Step [706/938], Loss: 0.2479\n",
      "Epoch [8/10], Step [708/938], Loss: 0.3821\n",
      "Epoch [8/10], Step [710/938], Loss: 0.2581\n",
      "Epoch [8/10], Step [712/938], Loss: 0.3102\n",
      "Epoch [8/10], Step [714/938], Loss: 0.2308\n",
      "Epoch [8/10], Step [716/938], Loss: 0.2811\n",
      "Epoch [8/10], Step [718/938], Loss: 0.3402\n",
      "Epoch [8/10], Step [720/938], Loss: 0.2468\n",
      "Epoch [8/10], Step [722/938], Loss: 0.3024\n",
      "Epoch [8/10], Step [724/938], Loss: 0.2398\n",
      "Epoch [8/10], Step [726/938], Loss: 0.1864\n",
      "Epoch [8/10], Step [728/938], Loss: 0.3026\n",
      "Epoch [8/10], Step [730/938], Loss: 0.2562\n",
      "Epoch [8/10], Step [732/938], Loss: 0.2354\n",
      "Epoch [8/10], Step [734/938], Loss: 0.2635\n",
      "Epoch [8/10], Step [736/938], Loss: 0.1935\n",
      "Epoch [8/10], Step [738/938], Loss: 0.3818\n",
      "Epoch [8/10], Step [740/938], Loss: 0.2792\n",
      "Epoch [8/10], Step [742/938], Loss: 0.1359\n",
      "Epoch [8/10], Step [744/938], Loss: 0.2033\n",
      "Epoch [8/10], Step [746/938], Loss: 0.1729\n",
      "Epoch [8/10], Step [748/938], Loss: 0.3669\n",
      "Epoch [8/10], Step [750/938], Loss: 0.2282\n",
      "Epoch [8/10], Step [752/938], Loss: 0.3479\n",
      "Epoch [8/10], Step [754/938], Loss: 0.3501\n",
      "Epoch [8/10], Step [756/938], Loss: 0.2177\n",
      "Epoch [8/10], Step [758/938], Loss: 0.1933\n",
      "Epoch [8/10], Step [760/938], Loss: 0.4764\n",
      "Epoch [8/10], Step [762/938], Loss: 0.2063\n",
      "Epoch [8/10], Step [764/938], Loss: 0.3517\n",
      "Epoch [8/10], Step [766/938], Loss: 0.2166\n",
      "Epoch [8/10], Step [768/938], Loss: 0.1633\n",
      "Epoch [8/10], Step [770/938], Loss: 0.4042\n",
      "Epoch [8/10], Step [772/938], Loss: 0.1611\n",
      "Epoch [8/10], Step [774/938], Loss: 0.2819\n",
      "Epoch [8/10], Step [776/938], Loss: 0.1896\n",
      "Epoch [8/10], Step [778/938], Loss: 0.4242\n",
      "Epoch [8/10], Step [780/938], Loss: 0.2380\n",
      "Epoch [8/10], Step [782/938], Loss: 0.1620\n",
      "Epoch [8/10], Step [784/938], Loss: 0.2077\n",
      "Epoch [8/10], Step [786/938], Loss: 0.2628\n",
      "Epoch [8/10], Step [788/938], Loss: 0.1983\n",
      "Epoch [8/10], Step [790/938], Loss: 0.2537\n",
      "Epoch [8/10], Step [792/938], Loss: 0.2953\n",
      "Epoch [8/10], Step [794/938], Loss: 0.2879\n",
      "Epoch [8/10], Step [796/938], Loss: 0.3575\n",
      "Epoch [8/10], Step [798/938], Loss: 0.2214\n",
      "Epoch [8/10], Step [800/938], Loss: 0.2225\n",
      "Epoch [8/10], Step [802/938], Loss: 0.3488\n",
      "Epoch [8/10], Step [804/938], Loss: 0.3497\n",
      "Epoch [8/10], Step [806/938], Loss: 0.2050\n",
      "Epoch [8/10], Step [808/938], Loss: 0.2433\n",
      "Epoch [8/10], Step [810/938], Loss: 0.1692\n",
      "Epoch [8/10], Step [812/938], Loss: 0.2047\n",
      "Epoch [8/10], Step [814/938], Loss: 0.2847\n",
      "Epoch [8/10], Step [816/938], Loss: 0.3405\n",
      "Epoch [8/10], Step [818/938], Loss: 0.1994\n",
      "Epoch [8/10], Step [820/938], Loss: 0.2941\n",
      "Epoch [8/10], Step [822/938], Loss: 0.3324\n",
      "Epoch [8/10], Step [824/938], Loss: 0.2635\n",
      "Epoch [8/10], Step [826/938], Loss: 0.1697\n",
      "Epoch [8/10], Step [828/938], Loss: 0.1673\n",
      "Epoch [8/10], Step [830/938], Loss: 0.2953\n",
      "Epoch [8/10], Step [832/938], Loss: 0.3404\n",
      "Epoch [8/10], Step [834/938], Loss: 0.4105\n",
      "Epoch [8/10], Step [836/938], Loss: 0.2952\n",
      "Epoch [8/10], Step [838/938], Loss: 0.1673\n",
      "Epoch [8/10], Step [840/938], Loss: 0.1648\n",
      "Epoch [8/10], Step [842/938], Loss: 0.3390\n",
      "Epoch [8/10], Step [844/938], Loss: 0.1389\n",
      "Epoch [8/10], Step [846/938], Loss: 0.2165\n",
      "Epoch [8/10], Step [848/938], Loss: 0.4156\n",
      "Epoch [8/10], Step [850/938], Loss: 0.1506\n",
      "Epoch [8/10], Step [852/938], Loss: 0.3372\n",
      "Epoch [8/10], Step [854/938], Loss: 0.2893\n",
      "Epoch [8/10], Step [856/938], Loss: 0.1702\n",
      "Epoch [8/10], Step [858/938], Loss: 0.3063\n",
      "Epoch [8/10], Step [860/938], Loss: 0.1932\n",
      "Epoch [8/10], Step [862/938], Loss: 0.3037\n",
      "Epoch [8/10], Step [864/938], Loss: 0.1794\n",
      "Epoch [8/10], Step [866/938], Loss: 0.2356\n",
      "Epoch [8/10], Step [868/938], Loss: 0.2493\n",
      "Epoch [8/10], Step [870/938], Loss: 0.2889\n",
      "Epoch [8/10], Step [872/938], Loss: 0.5867\n",
      "Epoch [8/10], Step [874/938], Loss: 0.2746\n",
      "Epoch [8/10], Step [876/938], Loss: 0.3276\n",
      "Epoch [8/10], Step [878/938], Loss: 0.3020\n",
      "Epoch [8/10], Step [880/938], Loss: 0.3316\n",
      "Epoch [8/10], Step [882/938], Loss: 0.3046\n",
      "Epoch [8/10], Step [884/938], Loss: 0.1759\n",
      "Epoch [8/10], Step [886/938], Loss: 0.2683\n",
      "Epoch [8/10], Step [888/938], Loss: 0.2343\n",
      "Epoch [8/10], Step [890/938], Loss: 0.2259\n",
      "Epoch [8/10], Step [892/938], Loss: 0.2264\n",
      "Epoch [8/10], Step [894/938], Loss: 0.3002\n",
      "Epoch [8/10], Step [896/938], Loss: 0.1651\n",
      "Epoch [8/10], Step [898/938], Loss: 0.3255\n",
      "Epoch [8/10], Step [900/938], Loss: 0.2764\n",
      "Epoch [8/10], Step [902/938], Loss: 0.2032\n",
      "Epoch [8/10], Step [904/938], Loss: 0.4247\n",
      "Epoch [8/10], Step [906/938], Loss: 0.2800\n",
      "Epoch [8/10], Step [908/938], Loss: 0.2456\n",
      "Epoch [8/10], Step [910/938], Loss: 0.1821\n",
      "Epoch [8/10], Step [912/938], Loss: 0.1832\n",
      "Epoch [8/10], Step [914/938], Loss: 0.1778\n",
      "Epoch [8/10], Step [916/938], Loss: 0.2563\n",
      "Epoch [8/10], Step [918/938], Loss: 0.3655\n",
      "Epoch [8/10], Step [920/938], Loss: 0.1420\n",
      "Epoch [8/10], Step [922/938], Loss: 0.2858\n",
      "Epoch [8/10], Step [924/938], Loss: 0.1723\n",
      "Epoch [8/10], Step [926/938], Loss: 0.1758\n",
      "Epoch [8/10], Step [928/938], Loss: 0.5054\n",
      "Epoch [8/10], Step [930/938], Loss: 0.3965\n",
      "Epoch [8/10], Step [932/938], Loss: 0.1661\n",
      "Epoch [8/10], Step [934/938], Loss: 0.2386\n",
      "Epoch [8/10], Step [936/938], Loss: 0.2435\n",
      "Epoch [8/10], Step [938/938], Loss: 0.3439\n",
      "Epoch [8/10], Loss: 0.2802\n",
      "Epoch [9/10], Step [2/938], Loss: 0.2691\n",
      "Epoch [9/10], Step [4/938], Loss: 0.1353\n",
      "Epoch [9/10], Step [6/938], Loss: 0.2529\n",
      "Epoch [9/10], Step [8/938], Loss: 0.2060\n",
      "Epoch [9/10], Step [10/938], Loss: 0.1742\n",
      "Epoch [9/10], Step [12/938], Loss: 0.2781\n",
      "Epoch [9/10], Step [14/938], Loss: 0.4485\n",
      "Epoch [9/10], Step [16/938], Loss: 0.2057\n",
      "Epoch [9/10], Step [18/938], Loss: 0.3153\n",
      "Epoch [9/10], Step [20/938], Loss: 0.2288\n",
      "Epoch [9/10], Step [22/938], Loss: 0.3068\n",
      "Epoch [9/10], Step [24/938], Loss: 0.2784\n",
      "Epoch [9/10], Step [26/938], Loss: 0.1669\n",
      "Epoch [9/10], Step [28/938], Loss: 0.3405\n",
      "Epoch [9/10], Step [30/938], Loss: 0.2549\n",
      "Epoch [9/10], Step [32/938], Loss: 0.2375\n",
      "Epoch [9/10], Step [34/938], Loss: 0.3700\n",
      "Epoch [9/10], Step [36/938], Loss: 0.3339\n",
      "Epoch [9/10], Step [38/938], Loss: 0.2248\n",
      "Epoch [9/10], Step [40/938], Loss: 0.2429\n",
      "Epoch [9/10], Step [42/938], Loss: 0.1863\n",
      "Epoch [9/10], Step [44/938], Loss: 0.2218\n",
      "Epoch [9/10], Step [46/938], Loss: 0.2623\n",
      "Epoch [9/10], Step [48/938], Loss: 0.2419\n",
      "Epoch [9/10], Step [50/938], Loss: 0.2641\n",
      "Epoch [9/10], Step [52/938], Loss: 0.2052\n",
      "Epoch [9/10], Step [54/938], Loss: 0.3144\n",
      "Epoch [9/10], Step [56/938], Loss: 0.1803\n",
      "Epoch [9/10], Step [58/938], Loss: 0.2530\n",
      "Epoch [9/10], Step [60/938], Loss: 0.1395\n",
      "Epoch [9/10], Step [62/938], Loss: 0.2939\n",
      "Epoch [9/10], Step [64/938], Loss: 0.3560\n",
      "Epoch [9/10], Step [66/938], Loss: 0.1164\n",
      "Epoch [9/10], Step [68/938], Loss: 0.4404\n",
      "Epoch [9/10], Step [70/938], Loss: 0.3220\n",
      "Epoch [9/10], Step [72/938], Loss: 0.2888\n",
      "Epoch [9/10], Step [74/938], Loss: 0.3258\n",
      "Epoch [9/10], Step [76/938], Loss: 0.2536\n",
      "Epoch [9/10], Step [78/938], Loss: 0.1515\n",
      "Epoch [9/10], Step [80/938], Loss: 0.2477\n",
      "Epoch [9/10], Step [82/938], Loss: 0.2691\n",
      "Epoch [9/10], Step [84/938], Loss: 0.2393\n",
      "Epoch [9/10], Step [86/938], Loss: 0.2775\n",
      "Epoch [9/10], Step [88/938], Loss: 0.4271\n",
      "Epoch [9/10], Step [90/938], Loss: 0.2488\n",
      "Epoch [9/10], Step [92/938], Loss: 0.2459\n",
      "Epoch [9/10], Step [94/938], Loss: 0.1729\n",
      "Epoch [9/10], Step [96/938], Loss: 0.3472\n",
      "Epoch [9/10], Step [98/938], Loss: 0.2707\n",
      "Epoch [9/10], Step [100/938], Loss: 0.3350\n",
      "Epoch [9/10], Step [102/938], Loss: 0.2492\n",
      "Epoch [9/10], Step [104/938], Loss: 0.3243\n",
      "Epoch [9/10], Step [106/938], Loss: 0.2333\n",
      "Epoch [9/10], Step [108/938], Loss: 0.2403\n",
      "Epoch [9/10], Step [110/938], Loss: 0.1551\n",
      "Epoch [9/10], Step [112/938], Loss: 0.2172\n",
      "Epoch [9/10], Step [114/938], Loss: 0.2175\n",
      "Epoch [9/10], Step [116/938], Loss: 0.2893\n",
      "Epoch [9/10], Step [118/938], Loss: 0.2779\n",
      "Epoch [9/10], Step [120/938], Loss: 0.3103\n",
      "Epoch [9/10], Step [122/938], Loss: 0.3317\n",
      "Epoch [9/10], Step [124/938], Loss: 0.2834\n",
      "Epoch [9/10], Step [126/938], Loss: 0.1659\n",
      "Epoch [9/10], Step [128/938], Loss: 0.2079\n",
      "Epoch [9/10], Step [130/938], Loss: 0.3763\n",
      "Epoch [9/10], Step [132/938], Loss: 0.1852\n",
      "Epoch [9/10], Step [134/938], Loss: 0.2617\n",
      "Epoch [9/10], Step [136/938], Loss: 0.2438\n",
      "Epoch [9/10], Step [138/938], Loss: 0.2299\n",
      "Epoch [9/10], Step [140/938], Loss: 0.3825\n",
      "Epoch [9/10], Step [142/938], Loss: 0.2792\n",
      "Epoch [9/10], Step [144/938], Loss: 0.1726\n",
      "Epoch [9/10], Step [146/938], Loss: 0.2837\n",
      "Epoch [9/10], Step [148/938], Loss: 0.3121\n",
      "Epoch [9/10], Step [150/938], Loss: 0.1858\n",
      "Epoch [9/10], Step [152/938], Loss: 0.2739\n",
      "Epoch [9/10], Step [154/938], Loss: 0.2079\n",
      "Epoch [9/10], Step [156/938], Loss: 0.3000\n",
      "Epoch [9/10], Step [158/938], Loss: 0.3101\n",
      "Epoch [9/10], Step [160/938], Loss: 0.3149\n",
      "Epoch [9/10], Step [162/938], Loss: 0.6111\n",
      "Epoch [9/10], Step [164/938], Loss: 0.1758\n",
      "Epoch [9/10], Step [166/938], Loss: 0.2683\n",
      "Epoch [9/10], Step [168/938], Loss: 0.1925\n",
      "Epoch [9/10], Step [170/938], Loss: 0.2616\n",
      "Epoch [9/10], Step [172/938], Loss: 0.2577\n",
      "Epoch [9/10], Step [174/938], Loss: 0.1536\n",
      "Epoch [9/10], Step [176/938], Loss: 0.2687\n",
      "Epoch [9/10], Step [178/938], Loss: 0.2212\n",
      "Epoch [9/10], Step [180/938], Loss: 0.3736\n",
      "Epoch [9/10], Step [182/938], Loss: 0.2511\n",
      "Epoch [9/10], Step [184/938], Loss: 0.2336\n",
      "Epoch [9/10], Step [186/938], Loss: 0.3227\n",
      "Epoch [9/10], Step [188/938], Loss: 0.2737\n",
      "Epoch [9/10], Step [190/938], Loss: 0.2929\n",
      "Epoch [9/10], Step [192/938], Loss: 0.2069\n",
      "Epoch [9/10], Step [194/938], Loss: 0.4187\n",
      "Epoch [9/10], Step [196/938], Loss: 0.3046\n",
      "Epoch [9/10], Step [198/938], Loss: 0.2370\n",
      "Epoch [9/10], Step [200/938], Loss: 0.4720\n",
      "Epoch [9/10], Step [202/938], Loss: 0.1457\n",
      "Epoch [9/10], Step [204/938], Loss: 0.1276\n",
      "Epoch [9/10], Step [206/938], Loss: 0.3878\n",
      "Epoch [9/10], Step [208/938], Loss: 0.1957\n",
      "Epoch [9/10], Step [210/938], Loss: 0.1930\n",
      "Epoch [9/10], Step [212/938], Loss: 0.2263\n",
      "Epoch [9/10], Step [214/938], Loss: 0.3344\n",
      "Epoch [9/10], Step [216/938], Loss: 0.1807\n",
      "Epoch [9/10], Step [218/938], Loss: 0.3662\n",
      "Epoch [9/10], Step [220/938], Loss: 0.2422\n",
      "Epoch [9/10], Step [222/938], Loss: 0.4411\n",
      "Epoch [9/10], Step [224/938], Loss: 0.2572\n",
      "Epoch [9/10], Step [226/938], Loss: 0.3506\n",
      "Epoch [9/10], Step [228/938], Loss: 0.2034\n",
      "Epoch [9/10], Step [230/938], Loss: 0.3835\n",
      "Epoch [9/10], Step [232/938], Loss: 0.3155\n",
      "Epoch [9/10], Step [234/938], Loss: 0.2074\n",
      "Epoch [9/10], Step [236/938], Loss: 0.1636\n",
      "Epoch [9/10], Step [238/938], Loss: 0.2786\n",
      "Epoch [9/10], Step [240/938], Loss: 0.1903\n",
      "Epoch [9/10], Step [242/938], Loss: 0.2514\n",
      "Epoch [9/10], Step [244/938], Loss: 0.2337\n",
      "Epoch [9/10], Step [246/938], Loss: 0.2991\n",
      "Epoch [9/10], Step [248/938], Loss: 0.2945\n",
      "Epoch [9/10], Step [250/938], Loss: 0.2531\n",
      "Epoch [9/10], Step [252/938], Loss: 0.2889\n",
      "Epoch [9/10], Step [254/938], Loss: 0.3128\n",
      "Epoch [9/10], Step [256/938], Loss: 0.2154\n",
      "Epoch [9/10], Step [258/938], Loss: 0.1417\n",
      "Epoch [9/10], Step [260/938], Loss: 0.4400\n",
      "Epoch [9/10], Step [262/938], Loss: 0.3045\n",
      "Epoch [9/10], Step [264/938], Loss: 0.2697\n",
      "Epoch [9/10], Step [266/938], Loss: 0.3178\n",
      "Epoch [9/10], Step [268/938], Loss: 0.2126\n",
      "Epoch [9/10], Step [270/938], Loss: 0.1792\n",
      "Epoch [9/10], Step [272/938], Loss: 0.2567\n",
      "Epoch [9/10], Step [274/938], Loss: 0.2759\n",
      "Epoch [9/10], Step [276/938], Loss: 0.2674\n",
      "Epoch [9/10], Step [278/938], Loss: 0.3090\n",
      "Epoch [9/10], Step [280/938], Loss: 0.1764\n",
      "Epoch [9/10], Step [282/938], Loss: 0.1857\n",
      "Epoch [9/10], Step [284/938], Loss: 0.4817\n",
      "Epoch [9/10], Step [286/938], Loss: 0.2672\n",
      "Epoch [9/10], Step [288/938], Loss: 0.2187\n",
      "Epoch [9/10], Step [290/938], Loss: 0.3492\n",
      "Epoch [9/10], Step [292/938], Loss: 0.1795\n",
      "Epoch [9/10], Step [294/938], Loss: 0.3404\n",
      "Epoch [9/10], Step [296/938], Loss: 0.2731\n",
      "Epoch [9/10], Step [298/938], Loss: 0.3864\n",
      "Epoch [9/10], Step [300/938], Loss: 0.2942\n",
      "Epoch [9/10], Step [302/938], Loss: 0.1693\n",
      "Epoch [9/10], Step [304/938], Loss: 0.2079\n",
      "Epoch [9/10], Step [306/938], Loss: 0.5315\n",
      "Epoch [9/10], Step [308/938], Loss: 0.2258\n",
      "Epoch [9/10], Step [310/938], Loss: 0.1907\n",
      "Epoch [9/10], Step [312/938], Loss: 0.2907\n",
      "Epoch [9/10], Step [314/938], Loss: 0.1946\n",
      "Epoch [9/10], Step [316/938], Loss: 0.3362\n",
      "Epoch [9/10], Step [318/938], Loss: 0.1934\n",
      "Epoch [9/10], Step [320/938], Loss: 0.2032\n",
      "Epoch [9/10], Step [322/938], Loss: 0.2432\n",
      "Epoch [9/10], Step [324/938], Loss: 0.2918\n",
      "Epoch [9/10], Step [326/938], Loss: 0.4075\n",
      "Epoch [9/10], Step [328/938], Loss: 0.1813\n",
      "Epoch [9/10], Step [330/938], Loss: 0.2233\n",
      "Epoch [9/10], Step [332/938], Loss: 0.1417\n",
      "Epoch [9/10], Step [334/938], Loss: 0.1779\n",
      "Epoch [9/10], Step [336/938], Loss: 0.2329\n",
      "Epoch [9/10], Step [338/938], Loss: 0.2321\n",
      "Epoch [9/10], Step [340/938], Loss: 0.1356\n",
      "Epoch [9/10], Step [342/938], Loss: 0.2210\n",
      "Epoch [9/10], Step [344/938], Loss: 0.3244\n",
      "Epoch [9/10], Step [346/938], Loss: 0.3777\n",
      "Epoch [9/10], Step [348/938], Loss: 0.3946\n",
      "Epoch [9/10], Step [350/938], Loss: 0.2566\n",
      "Epoch [9/10], Step [352/938], Loss: 0.2179\n",
      "Epoch [9/10], Step [354/938], Loss: 0.1391\n",
      "Epoch [9/10], Step [356/938], Loss: 0.2960\n",
      "Epoch [9/10], Step [358/938], Loss: 0.2271\n",
      "Epoch [9/10], Step [360/938], Loss: 0.2025\n",
      "Epoch [9/10], Step [362/938], Loss: 0.2894\n",
      "Epoch [9/10], Step [364/938], Loss: 0.3559\n",
      "Epoch [9/10], Step [366/938], Loss: 0.3240\n",
      "Epoch [9/10], Step [368/938], Loss: 0.2341\n",
      "Epoch [9/10], Step [370/938], Loss: 0.2388\n",
      "Epoch [9/10], Step [372/938], Loss: 0.2655\n",
      "Epoch [9/10], Step [374/938], Loss: 0.5141\n",
      "Epoch [9/10], Step [376/938], Loss: 0.2564\n",
      "Epoch [9/10], Step [378/938], Loss: 0.1753\n",
      "Epoch [9/10], Step [380/938], Loss: 0.1545\n",
      "Epoch [9/10], Step [382/938], Loss: 0.1732\n",
      "Epoch [9/10], Step [384/938], Loss: 0.2988\n",
      "Epoch [9/10], Step [386/938], Loss: 0.2114\n",
      "Epoch [9/10], Step [388/938], Loss: 0.3108\n",
      "Epoch [9/10], Step [390/938], Loss: 0.3200\n",
      "Epoch [9/10], Step [392/938], Loss: 0.2547\n",
      "Epoch [9/10], Step [394/938], Loss: 0.2395\n",
      "Epoch [9/10], Step [396/938], Loss: 0.2811\n",
      "Epoch [9/10], Step [398/938], Loss: 0.4246\n",
      "Epoch [9/10], Step [400/938], Loss: 0.2336\n",
      "Epoch [9/10], Step [402/938], Loss: 0.2048\n",
      "Epoch [9/10], Step [404/938], Loss: 0.3260\n",
      "Epoch [9/10], Step [406/938], Loss: 0.4641\n",
      "Epoch [9/10], Step [408/938], Loss: 0.1612\n",
      "Epoch [9/10], Step [410/938], Loss: 0.3380\n",
      "Epoch [9/10], Step [412/938], Loss: 0.3408\n",
      "Epoch [9/10], Step [414/938], Loss: 0.1944\n",
      "Epoch [9/10], Step [416/938], Loss: 0.1895\n",
      "Epoch [9/10], Step [418/938], Loss: 0.1952\n",
      "Epoch [9/10], Step [420/938], Loss: 0.1922\n",
      "Epoch [9/10], Step [422/938], Loss: 0.2023\n",
      "Epoch [9/10], Step [424/938], Loss: 0.3454\n",
      "Epoch [9/10], Step [426/938], Loss: 0.1631\n",
      "Epoch [9/10], Step [428/938], Loss: 0.2160\n",
      "Epoch [9/10], Step [430/938], Loss: 0.1457\n",
      "Epoch [9/10], Step [432/938], Loss: 0.2197\n",
      "Epoch [9/10], Step [434/938], Loss: 0.2405\n",
      "Epoch [9/10], Step [436/938], Loss: 0.2116\n",
      "Epoch [9/10], Step [438/938], Loss: 0.5845\n",
      "Epoch [9/10], Step [440/938], Loss: 0.2005\n",
      "Epoch [9/10], Step [442/938], Loss: 0.2857\n",
      "Epoch [9/10], Step [444/938], Loss: 0.1787\n",
      "Epoch [9/10], Step [446/938], Loss: 0.1668\n",
      "Epoch [9/10], Step [448/938], Loss: 0.3027\n",
      "Epoch [9/10], Step [450/938], Loss: 0.2453\n",
      "Epoch [9/10], Step [452/938], Loss: 0.2594\n",
      "Epoch [9/10], Step [454/938], Loss: 0.1674\n",
      "Epoch [9/10], Step [456/938], Loss: 0.1988\n",
      "Epoch [9/10], Step [458/938], Loss: 0.3527\n",
      "Epoch [9/10], Step [460/938], Loss: 0.1662\n",
      "Epoch [9/10], Step [462/938], Loss: 0.2452\n",
      "Epoch [9/10], Step [464/938], Loss: 0.2526\n",
      "Epoch [9/10], Step [466/938], Loss: 0.1463\n",
      "Epoch [9/10], Step [468/938], Loss: 0.2459\n",
      "Epoch [9/10], Step [470/938], Loss: 0.2474\n",
      "Epoch [9/10], Step [472/938], Loss: 0.3458\n",
      "Epoch [9/10], Step [474/938], Loss: 0.2575\n",
      "Epoch [9/10], Step [476/938], Loss: 0.2447\n",
      "Epoch [9/10], Step [478/938], Loss: 0.3275\n",
      "Epoch [9/10], Step [480/938], Loss: 0.4359\n",
      "Epoch [9/10], Step [482/938], Loss: 0.2317\n",
      "Epoch [9/10], Step [484/938], Loss: 0.2271\n",
      "Epoch [9/10], Step [486/938], Loss: 0.2477\n",
      "Epoch [9/10], Step [488/938], Loss: 0.2389\n",
      "Epoch [9/10], Step [490/938], Loss: 0.1953\n",
      "Epoch [9/10], Step [492/938], Loss: 0.2240\n",
      "Epoch [9/10], Step [494/938], Loss: 0.2388\n",
      "Epoch [9/10], Step [496/938], Loss: 0.2824\n",
      "Epoch [9/10], Step [498/938], Loss: 0.2431\n",
      "Epoch [9/10], Step [500/938], Loss: 0.1767\n",
      "Epoch [9/10], Step [502/938], Loss: 0.1905\n",
      "Epoch [9/10], Step [504/938], Loss: 0.1837\n",
      "Epoch [9/10], Step [506/938], Loss: 0.1970\n",
      "Epoch [9/10], Step [508/938], Loss: 0.1374\n",
      "Epoch [9/10], Step [510/938], Loss: 0.2118\n",
      "Epoch [9/10], Step [512/938], Loss: 0.3415\n",
      "Epoch [9/10], Step [514/938], Loss: 0.1843\n",
      "Epoch [9/10], Step [516/938], Loss: 0.2493\n",
      "Epoch [9/10], Step [518/938], Loss: 0.2269\n",
      "Epoch [9/10], Step [520/938], Loss: 0.1928\n",
      "Epoch [9/10], Step [522/938], Loss: 0.1179\n",
      "Epoch [9/10], Step [524/938], Loss: 0.2871\n",
      "Epoch [9/10], Step [526/938], Loss: 0.1455\n",
      "Epoch [9/10], Step [528/938], Loss: 0.2162\n",
      "Epoch [9/10], Step [530/938], Loss: 0.1538\n",
      "Epoch [9/10], Step [532/938], Loss: 0.1867\n",
      "Epoch [9/10], Step [534/938], Loss: 0.3129\n",
      "Epoch [9/10], Step [536/938], Loss: 0.1086\n",
      "Epoch [9/10], Step [538/938], Loss: 0.2472\n",
      "Epoch [9/10], Step [540/938], Loss: 0.2271\n",
      "Epoch [9/10], Step [542/938], Loss: 0.2828\n",
      "Epoch [9/10], Step [544/938], Loss: 0.2300\n",
      "Epoch [9/10], Step [546/938], Loss: 0.3950\n",
      "Epoch [9/10], Step [548/938], Loss: 0.1194\n",
      "Epoch [9/10], Step [550/938], Loss: 0.2184\n",
      "Epoch [9/10], Step [552/938], Loss: 0.2612\n",
      "Epoch [9/10], Step [554/938], Loss: 0.1629\n",
      "Epoch [9/10], Step [556/938], Loss: 0.2055\n",
      "Epoch [9/10], Step [558/938], Loss: 0.2449\n",
      "Epoch [9/10], Step [560/938], Loss: 0.3746\n",
      "Epoch [9/10], Step [562/938], Loss: 0.1769\n",
      "Epoch [9/10], Step [564/938], Loss: 0.2007\n",
      "Epoch [9/10], Step [566/938], Loss: 0.2508\n",
      "Epoch [9/10], Step [568/938], Loss: 0.2884\n",
      "Epoch [9/10], Step [570/938], Loss: 0.4460\n",
      "Epoch [9/10], Step [572/938], Loss: 0.2034\n",
      "Epoch [9/10], Step [574/938], Loss: 0.4360\n",
      "Epoch [9/10], Step [576/938], Loss: 0.1297\n",
      "Epoch [9/10], Step [578/938], Loss: 0.2822\n",
      "Epoch [9/10], Step [580/938], Loss: 0.2476\n",
      "Epoch [9/10], Step [582/938], Loss: 0.3016\n",
      "Epoch [9/10], Step [584/938], Loss: 0.2168\n",
      "Epoch [9/10], Step [586/938], Loss: 0.2461\n",
      "Epoch [9/10], Step [588/938], Loss: 0.2596\n",
      "Epoch [9/10], Step [590/938], Loss: 0.3793\n",
      "Epoch [9/10], Step [592/938], Loss: 0.2265\n",
      "Epoch [9/10], Step [594/938], Loss: 0.2017\n",
      "Epoch [9/10], Step [596/938], Loss: 0.2266\n",
      "Epoch [9/10], Step [598/938], Loss: 0.2005\n",
      "Epoch [9/10], Step [600/938], Loss: 0.2058\n",
      "Epoch [9/10], Step [602/938], Loss: 0.1808\n",
      "Epoch [9/10], Step [604/938], Loss: 0.2202\n",
      "Epoch [9/10], Step [606/938], Loss: 0.1493\n",
      "Epoch [9/10], Step [608/938], Loss: 0.2433\n",
      "Epoch [9/10], Step [610/938], Loss: 0.2176\n",
      "Epoch [9/10], Step [612/938], Loss: 0.2590\n",
      "Epoch [9/10], Step [614/938], Loss: 0.1237\n",
      "Epoch [9/10], Step [616/938], Loss: 0.3043\n",
      "Epoch [9/10], Step [618/938], Loss: 0.2220\n",
      "Epoch [9/10], Step [620/938], Loss: 0.2099\n",
      "Epoch [9/10], Step [622/938], Loss: 0.3033\n",
      "Epoch [9/10], Step [624/938], Loss: 0.2936\n",
      "Epoch [9/10], Step [626/938], Loss: 0.3826\n",
      "Epoch [9/10], Step [628/938], Loss: 0.2200\n",
      "Epoch [9/10], Step [630/938], Loss: 0.2608\n",
      "Epoch [9/10], Step [632/938], Loss: 0.1646\n",
      "Epoch [9/10], Step [634/938], Loss: 0.1248\n",
      "Epoch [9/10], Step [636/938], Loss: 0.2314\n",
      "Epoch [9/10], Step [638/938], Loss: 0.2831\n",
      "Epoch [9/10], Step [640/938], Loss: 0.1475\n",
      "Epoch [9/10], Step [642/938], Loss: 0.2929\n",
      "Epoch [9/10], Step [644/938], Loss: 0.2875\n",
      "Epoch [9/10], Step [646/938], Loss: 0.1795\n",
      "Epoch [9/10], Step [648/938], Loss: 0.2866\n",
      "Epoch [9/10], Step [650/938], Loss: 0.3410\n",
      "Epoch [9/10], Step [652/938], Loss: 0.1071\n",
      "Epoch [9/10], Step [654/938], Loss: 0.2210\n",
      "Epoch [9/10], Step [656/938], Loss: 0.2266\n",
      "Epoch [9/10], Step [658/938], Loss: 0.2293\n",
      "Epoch [9/10], Step [660/938], Loss: 0.3862\n",
      "Epoch [9/10], Step [662/938], Loss: 0.2421\n",
      "Epoch [9/10], Step [664/938], Loss: 0.3707\n",
      "Epoch [9/10], Step [666/938], Loss: 0.2701\n",
      "Epoch [9/10], Step [668/938], Loss: 0.2816\n",
      "Epoch [9/10], Step [670/938], Loss: 0.1788\n",
      "Epoch [9/10], Step [672/938], Loss: 0.2080\n",
      "Epoch [9/10], Step [674/938], Loss: 0.3102\n",
      "Epoch [9/10], Step [676/938], Loss: 0.3738\n",
      "Epoch [9/10], Step [678/938], Loss: 0.1350\n",
      "Epoch [9/10], Step [680/938], Loss: 0.0996\n",
      "Epoch [9/10], Step [682/938], Loss: 0.1660\n",
      "Epoch [9/10], Step [684/938], Loss: 0.2425\n",
      "Epoch [9/10], Step [686/938], Loss: 0.1368\n",
      "Epoch [9/10], Step [688/938], Loss: 0.2287\n",
      "Epoch [9/10], Step [690/938], Loss: 0.2653\n",
      "Epoch [9/10], Step [692/938], Loss: 0.1788\n",
      "Epoch [9/10], Step [694/938], Loss: 0.1525\n",
      "Epoch [9/10], Step [696/938], Loss: 0.1391\n",
      "Epoch [9/10], Step [698/938], Loss: 0.2353\n",
      "Epoch [9/10], Step [700/938], Loss: 0.1999\n",
      "Epoch [9/10], Step [702/938], Loss: 0.2813\n",
      "Epoch [9/10], Step [704/938], Loss: 0.2637\n",
      "Epoch [9/10], Step [706/938], Loss: 0.2547\n",
      "Epoch [9/10], Step [708/938], Loss: 0.2141\n",
      "Epoch [9/10], Step [710/938], Loss: 0.2570\n",
      "Epoch [9/10], Step [712/938], Loss: 0.1529\n",
      "Epoch [9/10], Step [714/938], Loss: 0.1861\n",
      "Epoch [9/10], Step [716/938], Loss: 0.1757\n",
      "Epoch [9/10], Step [718/938], Loss: 0.1562\n",
      "Epoch [9/10], Step [720/938], Loss: 0.2345\n",
      "Epoch [9/10], Step [722/938], Loss: 0.4362\n",
      "Epoch [9/10], Step [724/938], Loss: 0.2415\n",
      "Epoch [9/10], Step [726/938], Loss: 0.1950\n",
      "Epoch [9/10], Step [728/938], Loss: 0.1722\n",
      "Epoch [9/10], Step [730/938], Loss: 0.1501\n",
      "Epoch [9/10], Step [732/938], Loss: 0.3062\n",
      "Epoch [9/10], Step [734/938], Loss: 0.1312\n",
      "Epoch [9/10], Step [736/938], Loss: 0.2468\n",
      "Epoch [9/10], Step [738/938], Loss: 0.3543\n",
      "Epoch [9/10], Step [740/938], Loss: 0.1963\n",
      "Epoch [9/10], Step [742/938], Loss: 0.1629\n",
      "Epoch [9/10], Step [744/938], Loss: 0.2013\n",
      "Epoch [9/10], Step [746/938], Loss: 0.2343\n",
      "Epoch [9/10], Step [748/938], Loss: 0.2910\n",
      "Epoch [9/10], Step [750/938], Loss: 0.2527\n",
      "Epoch [9/10], Step [752/938], Loss: 0.2532\n",
      "Epoch [9/10], Step [754/938], Loss: 0.2127\n",
      "Epoch [9/10], Step [756/938], Loss: 0.2122\n",
      "Epoch [9/10], Step [758/938], Loss: 0.1390\n",
      "Epoch [9/10], Step [760/938], Loss: 0.2389\n",
      "Epoch [9/10], Step [762/938], Loss: 0.1257\n",
      "Epoch [9/10], Step [764/938], Loss: 0.1865\n",
      "Epoch [9/10], Step [766/938], Loss: 0.2301\n",
      "Epoch [9/10], Step [768/938], Loss: 0.3139\n",
      "Epoch [9/10], Step [770/938], Loss: 0.3496\n",
      "Epoch [9/10], Step [772/938], Loss: 0.2483\n",
      "Epoch [9/10], Step [774/938], Loss: 0.2251\n",
      "Epoch [9/10], Step [776/938], Loss: 0.3436\n",
      "Epoch [9/10], Step [778/938], Loss: 0.1925\n",
      "Epoch [9/10], Step [780/938], Loss: 0.2726\n",
      "Epoch [9/10], Step [782/938], Loss: 0.2167\n",
      "Epoch [9/10], Step [784/938], Loss: 0.2263\n",
      "Epoch [9/10], Step [786/938], Loss: 0.1095\n",
      "Epoch [9/10], Step [788/938], Loss: 0.1670\n",
      "Epoch [9/10], Step [790/938], Loss: 0.2533\n",
      "Epoch [9/10], Step [792/938], Loss: 0.2776\n",
      "Epoch [9/10], Step [794/938], Loss: 0.2521\n",
      "Epoch [9/10], Step [796/938], Loss: 0.2525\n",
      "Epoch [9/10], Step [798/938], Loss: 0.2984\n",
      "Epoch [9/10], Step [800/938], Loss: 0.1882\n",
      "Epoch [9/10], Step [802/938], Loss: 0.1958\n",
      "Epoch [9/10], Step [804/938], Loss: 0.2280\n",
      "Epoch [9/10], Step [806/938], Loss: 0.1948\n",
      "Epoch [9/10], Step [808/938], Loss: 0.3878\n",
      "Epoch [9/10], Step [810/938], Loss: 0.2074\n",
      "Epoch [9/10], Step [812/938], Loss: 0.2455\n",
      "Epoch [9/10], Step [814/938], Loss: 0.1820\n",
      "Epoch [9/10], Step [816/938], Loss: 0.2254\n",
      "Epoch [9/10], Step [818/938], Loss: 0.1100\n",
      "Epoch [9/10], Step [820/938], Loss: 0.2788\n",
      "Epoch [9/10], Step [822/938], Loss: 0.2640\n",
      "Epoch [9/10], Step [824/938], Loss: 0.0793\n",
      "Epoch [9/10], Step [826/938], Loss: 0.1737\n",
      "Epoch [9/10], Step [828/938], Loss: 0.2713\n",
      "Epoch [9/10], Step [830/938], Loss: 0.1982\n",
      "Epoch [9/10], Step [832/938], Loss: 0.1932\n",
      "Epoch [9/10], Step [834/938], Loss: 0.2381\n",
      "Epoch [9/10], Step [836/938], Loss: 0.2348\n",
      "Epoch [9/10], Step [838/938], Loss: 0.1971\n",
      "Epoch [9/10], Step [840/938], Loss: 0.1304\n",
      "Epoch [9/10], Step [842/938], Loss: 0.3220\n",
      "Epoch [9/10], Step [844/938], Loss: 0.2478\n",
      "Epoch [9/10], Step [846/938], Loss: 0.2039\n",
      "Epoch [9/10], Step [848/938], Loss: 0.1957\n",
      "Epoch [9/10], Step [850/938], Loss: 0.2346\n",
      "Epoch [9/10], Step [852/938], Loss: 0.3115\n",
      "Epoch [9/10], Step [854/938], Loss: 0.2440\n",
      "Epoch [9/10], Step [856/938], Loss: 0.1998\n",
      "Epoch [9/10], Step [858/938], Loss: 0.2325\n",
      "Epoch [9/10], Step [860/938], Loss: 0.1949\n",
      "Epoch [9/10], Step [862/938], Loss: 0.2890\n",
      "Epoch [9/10], Step [864/938], Loss: 0.2228\n",
      "Epoch [9/10], Step [866/938], Loss: 0.2736\n",
      "Epoch [9/10], Step [868/938], Loss: 0.2071\n",
      "Epoch [9/10], Step [870/938], Loss: 0.1582\n",
      "Epoch [9/10], Step [872/938], Loss: 0.3068\n",
      "Epoch [9/10], Step [874/938], Loss: 0.4446\n",
      "Epoch [9/10], Step [876/938], Loss: 0.2172\n",
      "Epoch [9/10], Step [878/938], Loss: 0.2167\n",
      "Epoch [9/10], Step [880/938], Loss: 0.2913\n",
      "Epoch [9/10], Step [882/938], Loss: 0.2168\n",
      "Epoch [9/10], Step [884/938], Loss: 0.2884\n",
      "Epoch [9/10], Step [886/938], Loss: 0.1116\n",
      "Epoch [9/10], Step [888/938], Loss: 0.2130\n",
      "Epoch [9/10], Step [890/938], Loss: 0.2650\n",
      "Epoch [9/10], Step [892/938], Loss: 0.3628\n",
      "Epoch [9/10], Step [894/938], Loss: 0.1460\n",
      "Epoch [9/10], Step [896/938], Loss: 0.2232\n",
      "Epoch [9/10], Step [898/938], Loss: 0.1886\n",
      "Epoch [9/10], Step [900/938], Loss: 0.2817\n",
      "Epoch [9/10], Step [902/938], Loss: 0.2289\n",
      "Epoch [9/10], Step [904/938], Loss: 0.1614\n",
      "Epoch [9/10], Step [906/938], Loss: 0.3059\n",
      "Epoch [9/10], Step [908/938], Loss: 0.1616\n",
      "Epoch [9/10], Step [910/938], Loss: 0.1938\n",
      "Epoch [9/10], Step [912/938], Loss: 0.2475\n",
      "Epoch [9/10], Step [914/938], Loss: 0.2629\n",
      "Epoch [9/10], Step [916/938], Loss: 0.1601\n",
      "Epoch [9/10], Step [918/938], Loss: 0.1995\n",
      "Epoch [9/10], Step [920/938], Loss: 0.1046\n",
      "Epoch [9/10], Step [922/938], Loss: 0.2825\n",
      "Epoch [9/10], Step [924/938], Loss: 0.1814\n",
      "Epoch [9/10], Step [926/938], Loss: 0.3357\n",
      "Epoch [9/10], Step [928/938], Loss: 0.2344\n",
      "Epoch [9/10], Step [930/938], Loss: 0.1293\n",
      "Epoch [9/10], Step [932/938], Loss: 0.2226\n",
      "Epoch [9/10], Step [934/938], Loss: 0.3397\n",
      "Epoch [9/10], Step [936/938], Loss: 0.0984\n",
      "Epoch [9/10], Step [938/938], Loss: 0.1067\n",
      "Epoch [9/10], Loss: 0.2531\n",
      "Epoch [10/10], Step [2/938], Loss: 0.1784\n",
      "Epoch [10/10], Step [4/938], Loss: 0.3381\n",
      "Epoch [10/10], Step [6/938], Loss: 0.2544\n",
      "Epoch [10/10], Step [8/938], Loss: 0.3797\n",
      "Epoch [10/10], Step [10/938], Loss: 0.3486\n",
      "Epoch [10/10], Step [12/938], Loss: 0.2257\n",
      "Epoch [10/10], Step [14/938], Loss: 0.3414\n",
      "Epoch [10/10], Step [16/938], Loss: 0.2295\n",
      "Epoch [10/10], Step [18/938], Loss: 0.2885\n",
      "Epoch [10/10], Step [20/938], Loss: 0.1360\n",
      "Epoch [10/10], Step [22/938], Loss: 0.2215\n",
      "Epoch [10/10], Step [24/938], Loss: 0.3231\n",
      "Epoch [10/10], Step [26/938], Loss: 0.2499\n",
      "Epoch [10/10], Step [28/938], Loss: 0.1778\n",
      "Epoch [10/10], Step [30/938], Loss: 0.2780\n",
      "Epoch [10/10], Step [32/938], Loss: 0.1251\n",
      "Epoch [10/10], Step [34/938], Loss: 0.0852\n",
      "Epoch [10/10], Step [36/938], Loss: 0.1870\n",
      "Epoch [10/10], Step [38/938], Loss: 0.1843\n",
      "Epoch [10/10], Step [40/938], Loss: 0.2286\n",
      "Epoch [10/10], Step [42/938], Loss: 0.1915\n",
      "Epoch [10/10], Step [44/938], Loss: 0.2222\n",
      "Epoch [10/10], Step [46/938], Loss: 0.2394\n",
      "Epoch [10/10], Step [48/938], Loss: 0.2016\n",
      "Epoch [10/10], Step [50/938], Loss: 0.2477\n",
      "Epoch [10/10], Step [52/938], Loss: 0.1699\n",
      "Epoch [10/10], Step [54/938], Loss: 0.2323\n",
      "Epoch [10/10], Step [56/938], Loss: 0.1875\n",
      "Epoch [10/10], Step [58/938], Loss: 0.3292\n",
      "Epoch [10/10], Step [60/938], Loss: 0.2172\n",
      "Epoch [10/10], Step [62/938], Loss: 0.0748\n",
      "Epoch [10/10], Step [64/938], Loss: 0.3688\n",
      "Epoch [10/10], Step [66/938], Loss: 0.1532\n",
      "Epoch [10/10], Step [68/938], Loss: 0.2278\n",
      "Epoch [10/10], Step [70/938], Loss: 0.3229\n",
      "Epoch [10/10], Step [72/938], Loss: 0.3297\n",
      "Epoch [10/10], Step [74/938], Loss: 0.2165\n",
      "Epoch [10/10], Step [76/938], Loss: 0.2025\n",
      "Epoch [10/10], Step [78/938], Loss: 0.1492\n",
      "Epoch [10/10], Step [80/938], Loss: 0.2557\n",
      "Epoch [10/10], Step [82/938], Loss: 0.3488\n",
      "Epoch [10/10], Step [84/938], Loss: 0.2430\n",
      "Epoch [10/10], Step [86/938], Loss: 0.2901\n",
      "Epoch [10/10], Step [88/938], Loss: 0.2526\n",
      "Epoch [10/10], Step [90/938], Loss: 0.2383\n",
      "Epoch [10/10], Step [92/938], Loss: 0.1487\n",
      "Epoch [10/10], Step [94/938], Loss: 0.1405\n",
      "Epoch [10/10], Step [96/938], Loss: 0.3456\n",
      "Epoch [10/10], Step [98/938], Loss: 0.1864\n",
      "Epoch [10/10], Step [100/938], Loss: 0.2502\n",
      "Epoch [10/10], Step [102/938], Loss: 0.2664\n",
      "Epoch [10/10], Step [104/938], Loss: 0.1701\n",
      "Epoch [10/10], Step [106/938], Loss: 0.3820\n",
      "Epoch [10/10], Step [108/938], Loss: 0.3047\n",
      "Epoch [10/10], Step [110/938], Loss: 0.2327\n",
      "Epoch [10/10], Step [112/938], Loss: 0.2095\n",
      "Epoch [10/10], Step [114/938], Loss: 0.4495\n",
      "Epoch [10/10], Step [116/938], Loss: 0.2580\n",
      "Epoch [10/10], Step [118/938], Loss: 0.2666\n",
      "Epoch [10/10], Step [120/938], Loss: 0.1759\n",
      "Epoch [10/10], Step [122/938], Loss: 0.4989\n",
      "Epoch [10/10], Step [124/938], Loss: 0.2255\n",
      "Epoch [10/10], Step [126/938], Loss: 0.1816\n",
      "Epoch [10/10], Step [128/938], Loss: 0.1882\n",
      "Epoch [10/10], Step [130/938], Loss: 0.2498\n",
      "Epoch [10/10], Step [132/938], Loss: 0.1849\n",
      "Epoch [10/10], Step [134/938], Loss: 0.3759\n",
      "Epoch [10/10], Step [136/938], Loss: 0.1620\n",
      "Epoch [10/10], Step [138/938], Loss: 0.1947\n",
      "Epoch [10/10], Step [140/938], Loss: 0.5883\n",
      "Epoch [10/10], Step [142/938], Loss: 0.3128\n",
      "Epoch [10/10], Step [144/938], Loss: 0.2110\n",
      "Epoch [10/10], Step [146/938], Loss: 0.2415\n",
      "Epoch [10/10], Step [148/938], Loss: 0.2551\n",
      "Epoch [10/10], Step [150/938], Loss: 0.2600\n",
      "Epoch [10/10], Step [152/938], Loss: 0.0904\n",
      "Epoch [10/10], Step [154/938], Loss: 0.2212\n",
      "Epoch [10/10], Step [156/938], Loss: 0.2394\n",
      "Epoch [10/10], Step [158/938], Loss: 0.3925\n",
      "Epoch [10/10], Step [160/938], Loss: 0.3218\n",
      "Epoch [10/10], Step [162/938], Loss: 0.2429\n",
      "Epoch [10/10], Step [164/938], Loss: 0.2789\n",
      "Epoch [10/10], Step [166/938], Loss: 0.1400\n",
      "Epoch [10/10], Step [168/938], Loss: 0.2477\n",
      "Epoch [10/10], Step [170/938], Loss: 0.2644\n",
      "Epoch [10/10], Step [172/938], Loss: 0.2868\n",
      "Epoch [10/10], Step [174/938], Loss: 0.2969\n",
      "Epoch [10/10], Step [176/938], Loss: 0.1889\n",
      "Epoch [10/10], Step [178/938], Loss: 0.1383\n",
      "Epoch [10/10], Step [180/938], Loss: 0.2281\n",
      "Epoch [10/10], Step [182/938], Loss: 0.2891\n",
      "Epoch [10/10], Step [184/938], Loss: 0.4049\n",
      "Epoch [10/10], Step [186/938], Loss: 0.2337\n",
      "Epoch [10/10], Step [188/938], Loss: 0.2710\n",
      "Epoch [10/10], Step [190/938], Loss: 0.2678\n",
      "Epoch [10/10], Step [192/938], Loss: 0.2804\n",
      "Epoch [10/10], Step [194/938], Loss: 0.3325\n",
      "Epoch [10/10], Step [196/938], Loss: 0.1524\n",
      "Epoch [10/10], Step [198/938], Loss: 0.1770\n",
      "Epoch [10/10], Step [200/938], Loss: 0.3623\n",
      "Epoch [10/10], Step [202/938], Loss: 0.4320\n",
      "Epoch [10/10], Step [204/938], Loss: 0.2132\n",
      "Epoch [10/10], Step [206/938], Loss: 0.1172\n",
      "Epoch [10/10], Step [208/938], Loss: 0.3865\n",
      "Epoch [10/10], Step [210/938], Loss: 0.2998\n",
      "Epoch [10/10], Step [212/938], Loss: 0.2529\n",
      "Epoch [10/10], Step [214/938], Loss: 0.1289\n",
      "Epoch [10/10], Step [216/938], Loss: 0.1691\n",
      "Epoch [10/10], Step [218/938], Loss: 0.3214\n",
      "Epoch [10/10], Step [220/938], Loss: 0.3026\n",
      "Epoch [10/10], Step [222/938], Loss: 0.1707\n",
      "Epoch [10/10], Step [224/938], Loss: 0.3076\n",
      "Epoch [10/10], Step [226/938], Loss: 0.5764\n",
      "Epoch [10/10], Step [228/938], Loss: 0.2166\n",
      "Epoch [10/10], Step [230/938], Loss: 0.1449\n",
      "Epoch [10/10], Step [232/938], Loss: 0.2395\n",
      "Epoch [10/10], Step [234/938], Loss: 0.1083\n",
      "Epoch [10/10], Step [236/938], Loss: 0.1522\n",
      "Epoch [10/10], Step [238/938], Loss: 0.1959\n",
      "Epoch [10/10], Step [240/938], Loss: 0.1028\n",
      "Epoch [10/10], Step [242/938], Loss: 0.1754\n",
      "Epoch [10/10], Step [244/938], Loss: 0.1978\n",
      "Epoch [10/10], Step [246/938], Loss: 0.2776\n",
      "Epoch [10/10], Step [248/938], Loss: 0.1425\n",
      "Epoch [10/10], Step [250/938], Loss: 0.2097\n",
      "Epoch [10/10], Step [252/938], Loss: 0.2274\n",
      "Epoch [10/10], Step [254/938], Loss: 0.4470\n",
      "Epoch [10/10], Step [256/938], Loss: 0.4271\n",
      "Epoch [10/10], Step [258/938], Loss: 0.1933\n",
      "Epoch [10/10], Step [260/938], Loss: 0.3364\n",
      "Epoch [10/10], Step [262/938], Loss: 0.1524\n",
      "Epoch [10/10], Step [264/938], Loss: 0.1602\n",
      "Epoch [10/10], Step [266/938], Loss: 0.1629\n",
      "Epoch [10/10], Step [268/938], Loss: 0.4519\n",
      "Epoch [10/10], Step [270/938], Loss: 0.2249\n",
      "Epoch [10/10], Step [272/938], Loss: 0.2555\n",
      "Epoch [10/10], Step [274/938], Loss: 0.0825\n",
      "Epoch [10/10], Step [276/938], Loss: 0.1313\n",
      "Epoch [10/10], Step [278/938], Loss: 0.0973\n",
      "Epoch [10/10], Step [280/938], Loss: 0.1724\n",
      "Epoch [10/10], Step [282/938], Loss: 0.1436\n",
      "Epoch [10/10], Step [284/938], Loss: 0.2298\n",
      "Epoch [10/10], Step [286/938], Loss: 0.2671\n",
      "Epoch [10/10], Step [288/938], Loss: 0.4649\n",
      "Epoch [10/10], Step [290/938], Loss: 0.1070\n",
      "Epoch [10/10], Step [292/938], Loss: 0.1957\n",
      "Epoch [10/10], Step [294/938], Loss: 0.1739\n",
      "Epoch [10/10], Step [296/938], Loss: 0.1560\n",
      "Epoch [10/10], Step [298/938], Loss: 0.4276\n",
      "Epoch [10/10], Step [300/938], Loss: 0.2653\n",
      "Epoch [10/10], Step [302/938], Loss: 0.2715\n",
      "Epoch [10/10], Step [304/938], Loss: 0.1486\n",
      "Epoch [10/10], Step [306/938], Loss: 0.2290\n",
      "Epoch [10/10], Step [308/938], Loss: 0.1738\n",
      "Epoch [10/10], Step [310/938], Loss: 0.2782\n",
      "Epoch [10/10], Step [312/938], Loss: 0.2936\n",
      "Epoch [10/10], Step [314/938], Loss: 0.1685\n",
      "Epoch [10/10], Step [316/938], Loss: 0.3404\n",
      "Epoch [10/10], Step [318/938], Loss: 0.4065\n",
      "Epoch [10/10], Step [320/938], Loss: 0.1551\n",
      "Epoch [10/10], Step [322/938], Loss: 0.2180\n",
      "Epoch [10/10], Step [324/938], Loss: 0.3251\n",
      "Epoch [10/10], Step [326/938], Loss: 0.2119\n",
      "Epoch [10/10], Step [328/938], Loss: 0.1245\n",
      "Epoch [10/10], Step [330/938], Loss: 0.1740\n",
      "Epoch [10/10], Step [332/938], Loss: 0.2446\n",
      "Epoch [10/10], Step [334/938], Loss: 0.1207\n",
      "Epoch [10/10], Step [336/938], Loss: 0.2765\n",
      "Epoch [10/10], Step [338/938], Loss: 0.2994\n",
      "Epoch [10/10], Step [340/938], Loss: 0.1955\n",
      "Epoch [10/10], Step [342/938], Loss: 0.2107\n",
      "Epoch [10/10], Step [344/938], Loss: 0.2512\n",
      "Epoch [10/10], Step [346/938], Loss: 0.2698\n",
      "Epoch [10/10], Step [348/938], Loss: 0.1943\n",
      "Epoch [10/10], Step [350/938], Loss: 0.1999\n",
      "Epoch [10/10], Step [352/938], Loss: 0.1972\n",
      "Epoch [10/10], Step [354/938], Loss: 0.2745\n",
      "Epoch [10/10], Step [356/938], Loss: 0.3107\n",
      "Epoch [10/10], Step [358/938], Loss: 0.2076\n",
      "Epoch [10/10], Step [360/938], Loss: 0.2596\n",
      "Epoch [10/10], Step [362/938], Loss: 0.2876\n",
      "Epoch [10/10], Step [364/938], Loss: 0.1776\n",
      "Epoch [10/10], Step [366/938], Loss: 0.4541\n",
      "Epoch [10/10], Step [368/938], Loss: 0.3375\n",
      "Epoch [10/10], Step [370/938], Loss: 0.1968\n",
      "Epoch [10/10], Step [372/938], Loss: 0.2808\n",
      "Epoch [10/10], Step [374/938], Loss: 0.3566\n",
      "Epoch [10/10], Step [376/938], Loss: 0.4617\n",
      "Epoch [10/10], Step [378/938], Loss: 0.2256\n",
      "Epoch [10/10], Step [380/938], Loss: 0.1905\n",
      "Epoch [10/10], Step [382/938], Loss: 0.1294\n",
      "Epoch [10/10], Step [384/938], Loss: 0.2700\n",
      "Epoch [10/10], Step [386/938], Loss: 0.2391\n",
      "Epoch [10/10], Step [388/938], Loss: 0.2974\n",
      "Epoch [10/10], Step [390/938], Loss: 0.1915\n",
      "Epoch [10/10], Step [392/938], Loss: 0.2392\n",
      "Epoch [10/10], Step [394/938], Loss: 0.1900\n",
      "Epoch [10/10], Step [396/938], Loss: 0.2396\n",
      "Epoch [10/10], Step [398/938], Loss: 0.2023\n",
      "Epoch [10/10], Step [400/938], Loss: 0.2005\n",
      "Epoch [10/10], Step [402/938], Loss: 0.2730\n",
      "Epoch [10/10], Step [404/938], Loss: 0.2607\n",
      "Epoch [10/10], Step [406/938], Loss: 0.2263\n",
      "Epoch [10/10], Step [408/938], Loss: 0.2986\n",
      "Epoch [10/10], Step [410/938], Loss: 0.2640\n",
      "Epoch [10/10], Step [412/938], Loss: 0.2251\n",
      "Epoch [10/10], Step [414/938], Loss: 0.1953\n",
      "Epoch [10/10], Step [416/938], Loss: 0.2864\n",
      "Epoch [10/10], Step [418/938], Loss: 0.1218\n",
      "Epoch [10/10], Step [420/938], Loss: 0.2216\n",
      "Epoch [10/10], Step [422/938], Loss: 0.1786\n",
      "Epoch [10/10], Step [424/938], Loss: 0.1903\n",
      "Epoch [10/10], Step [426/938], Loss: 0.1623\n",
      "Epoch [10/10], Step [428/938], Loss: 0.1852\n",
      "Epoch [10/10], Step [430/938], Loss: 0.1925\n",
      "Epoch [10/10], Step [432/938], Loss: 0.2660\n",
      "Epoch [10/10], Step [434/938], Loss: 0.1073\n",
      "Epoch [10/10], Step [436/938], Loss: 0.2161\n",
      "Epoch [10/10], Step [438/938], Loss: 0.1961\n",
      "Epoch [10/10], Step [440/938], Loss: 0.2148\n",
      "Epoch [10/10], Step [442/938], Loss: 0.3467\n",
      "Epoch [10/10], Step [444/938], Loss: 0.1692\n",
      "Epoch [10/10], Step [446/938], Loss: 0.1292\n",
      "Epoch [10/10], Step [448/938], Loss: 0.2639\n",
      "Epoch [10/10], Step [450/938], Loss: 0.1766\n",
      "Epoch [10/10], Step [452/938], Loss: 0.1887\n",
      "Epoch [10/10], Step [454/938], Loss: 0.2348\n",
      "Epoch [10/10], Step [456/938], Loss: 0.2434\n",
      "Epoch [10/10], Step [458/938], Loss: 0.2970\n",
      "Epoch [10/10], Step [460/938], Loss: 0.2545\n",
      "Epoch [10/10], Step [462/938], Loss: 0.3109\n",
      "Epoch [10/10], Step [464/938], Loss: 0.2249\n",
      "Epoch [10/10], Step [466/938], Loss: 0.2179\n",
      "Epoch [10/10], Step [468/938], Loss: 0.1794\n",
      "Epoch [10/10], Step [470/938], Loss: 0.2537\n",
      "Epoch [10/10], Step [472/938], Loss: 0.3149\n",
      "Epoch [10/10], Step [474/938], Loss: 0.2681\n",
      "Epoch [10/10], Step [476/938], Loss: 0.1691\n",
      "Epoch [10/10], Step [478/938], Loss: 0.1760\n",
      "Epoch [10/10], Step [480/938], Loss: 0.3713\n",
      "Epoch [10/10], Step [482/938], Loss: 0.3264\n",
      "Epoch [10/10], Step [484/938], Loss: 0.2954\n",
      "Epoch [10/10], Step [486/938], Loss: 0.2089\n",
      "Epoch [10/10], Step [488/938], Loss: 0.3265\n",
      "Epoch [10/10], Step [490/938], Loss: 0.1963\n",
      "Epoch [10/10], Step [492/938], Loss: 0.2135\n",
      "Epoch [10/10], Step [494/938], Loss: 0.1484\n",
      "Epoch [10/10], Step [496/938], Loss: 0.1953\n",
      "Epoch [10/10], Step [498/938], Loss: 0.1551\n",
      "Epoch [10/10], Step [500/938], Loss: 0.2203\n",
      "Epoch [10/10], Step [502/938], Loss: 0.2538\n",
      "Epoch [10/10], Step [504/938], Loss: 0.3970\n",
      "Epoch [10/10], Step [506/938], Loss: 0.2177\n",
      "Epoch [10/10], Step [508/938], Loss: 0.1604\n",
      "Epoch [10/10], Step [510/938], Loss: 0.4311\n",
      "Epoch [10/10], Step [512/938], Loss: 0.1969\n",
      "Epoch [10/10], Step [514/938], Loss: 0.2465\n",
      "Epoch [10/10], Step [516/938], Loss: 0.2033\n",
      "Epoch [10/10], Step [518/938], Loss: 0.1041\n",
      "Epoch [10/10], Step [520/938], Loss: 0.3131\n",
      "Epoch [10/10], Step [522/938], Loss: 0.1809\n",
      "Epoch [10/10], Step [524/938], Loss: 0.2491\n",
      "Epoch [10/10], Step [526/938], Loss: 0.1882\n",
      "Epoch [10/10], Step [528/938], Loss: 0.1350\n",
      "Epoch [10/10], Step [530/938], Loss: 0.2434\n",
      "Epoch [10/10], Step [532/938], Loss: 0.3416\n",
      "Epoch [10/10], Step [534/938], Loss: 0.1912\n",
      "Epoch [10/10], Step [536/938], Loss: 0.3363\n",
      "Epoch [10/10], Step [538/938], Loss: 0.2372\n",
      "Epoch [10/10], Step [540/938], Loss: 0.1794\n",
      "Epoch [10/10], Step [542/938], Loss: 0.4378\n",
      "Epoch [10/10], Step [544/938], Loss: 0.2419\n",
      "Epoch [10/10], Step [546/938], Loss: 0.2560\n",
      "Epoch [10/10], Step [548/938], Loss: 0.1494\n",
      "Epoch [10/10], Step [550/938], Loss: 0.2249\n",
      "Epoch [10/10], Step [552/938], Loss: 0.2629\n",
      "Epoch [10/10], Step [554/938], Loss: 0.1653\n",
      "Epoch [10/10], Step [556/938], Loss: 0.1803\n",
      "Epoch [10/10], Step [558/938], Loss: 0.1214\n",
      "Epoch [10/10], Step [560/938], Loss: 0.2316\n",
      "Epoch [10/10], Step [562/938], Loss: 0.2231\n",
      "Epoch [10/10], Step [564/938], Loss: 0.1223\n",
      "Epoch [10/10], Step [566/938], Loss: 0.1883\n",
      "Epoch [10/10], Step [568/938], Loss: 0.2140\n",
      "Epoch [10/10], Step [570/938], Loss: 0.1635\n",
      "Epoch [10/10], Step [572/938], Loss: 0.3062\n",
      "Epoch [10/10], Step [574/938], Loss: 0.1953\n",
      "Epoch [10/10], Step [576/938], Loss: 0.2870\n",
      "Epoch [10/10], Step [578/938], Loss: 0.1389\n",
      "Epoch [10/10], Step [580/938], Loss: 0.2477\n",
      "Epoch [10/10], Step [582/938], Loss: 0.2475\n",
      "Epoch [10/10], Step [584/938], Loss: 0.1982\n",
      "Epoch [10/10], Step [586/938], Loss: 0.1207\n",
      "Epoch [10/10], Step [588/938], Loss: 0.3083\n",
      "Epoch [10/10], Step [590/938], Loss: 0.2589\n",
      "Epoch [10/10], Step [592/938], Loss: 0.1720\n",
      "Epoch [10/10], Step [594/938], Loss: 0.1910\n",
      "Epoch [10/10], Step [596/938], Loss: 0.2025\n",
      "Epoch [10/10], Step [598/938], Loss: 0.1721\n",
      "Epoch [10/10], Step [600/938], Loss: 0.1981\n",
      "Epoch [10/10], Step [602/938], Loss: 0.1272\n",
      "Epoch [10/10], Step [604/938], Loss: 0.4178\n",
      "Epoch [10/10], Step [606/938], Loss: 0.1071\n",
      "Epoch [10/10], Step [608/938], Loss: 0.1570\n",
      "Epoch [10/10], Step [610/938], Loss: 0.2767\n",
      "Epoch [10/10], Step [612/938], Loss: 0.1510\n",
      "Epoch [10/10], Step [614/938], Loss: 0.3483\n",
      "Epoch [10/10], Step [616/938], Loss: 0.1405\n",
      "Epoch [10/10], Step [618/938], Loss: 0.1474\n",
      "Epoch [10/10], Step [620/938], Loss: 0.2801\n",
      "Epoch [10/10], Step [622/938], Loss: 0.1203\n",
      "Epoch [10/10], Step [624/938], Loss: 0.3655\n",
      "Epoch [10/10], Step [626/938], Loss: 0.1750\n",
      "Epoch [10/10], Step [628/938], Loss: 0.1246\n",
      "Epoch [10/10], Step [630/938], Loss: 0.2400\n",
      "Epoch [10/10], Step [632/938], Loss: 0.1787\n",
      "Epoch [10/10], Step [634/938], Loss: 0.0824\n",
      "Epoch [10/10], Step [636/938], Loss: 0.1243\n",
      "Epoch [10/10], Step [638/938], Loss: 0.3252\n",
      "Epoch [10/10], Step [640/938], Loss: 0.2746\n",
      "Epoch [10/10], Step [642/938], Loss: 0.1144\n",
      "Epoch [10/10], Step [644/938], Loss: 0.1241\n",
      "Epoch [10/10], Step [646/938], Loss: 0.2733\n",
      "Epoch [10/10], Step [648/938], Loss: 0.3674\n",
      "Epoch [10/10], Step [650/938], Loss: 0.1884\n",
      "Epoch [10/10], Step [652/938], Loss: 0.3718\n",
      "Epoch [10/10], Step [654/938], Loss: 0.1650\n",
      "Epoch [10/10], Step [656/938], Loss: 0.2055\n",
      "Epoch [10/10], Step [658/938], Loss: 0.2621\n",
      "Epoch [10/10], Step [660/938], Loss: 0.1398\n",
      "Epoch [10/10], Step [662/938], Loss: 0.1991\n",
      "Epoch [10/10], Step [664/938], Loss: 0.2204\n",
      "Epoch [10/10], Step [666/938], Loss: 0.2121\n",
      "Epoch [10/10], Step [668/938], Loss: 0.1632\n",
      "Epoch [10/10], Step [670/938], Loss: 0.1955\n",
      "Epoch [10/10], Step [672/938], Loss: 0.2404\n",
      "Epoch [10/10], Step [674/938], Loss: 0.2575\n",
      "Epoch [10/10], Step [676/938], Loss: 0.2674\n",
      "Epoch [10/10], Step [678/938], Loss: 0.1885\n",
      "Epoch [10/10], Step [680/938], Loss: 0.2266\n",
      "Epoch [10/10], Step [682/938], Loss: 0.2138\n",
      "Epoch [10/10], Step [684/938], Loss: 0.2272\n",
      "Epoch [10/10], Step [686/938], Loss: 0.1669\n",
      "Epoch [10/10], Step [688/938], Loss: 0.1075\n",
      "Epoch [10/10], Step [690/938], Loss: 0.3342\n",
      "Epoch [10/10], Step [692/938], Loss: 0.3775\n",
      "Epoch [10/10], Step [694/938], Loss: 0.2372\n",
      "Epoch [10/10], Step [696/938], Loss: 0.2124\n",
      "Epoch [10/10], Step [698/938], Loss: 0.2874\n",
      "Epoch [10/10], Step [700/938], Loss: 0.2798\n",
      "Epoch [10/10], Step [702/938], Loss: 0.2501\n",
      "Epoch [10/10], Step [704/938], Loss: 0.2059\n",
      "Epoch [10/10], Step [706/938], Loss: 0.1988\n",
      "Epoch [10/10], Step [708/938], Loss: 0.3634\n",
      "Epoch [10/10], Step [710/938], Loss: 0.1764\n",
      "Epoch [10/10], Step [712/938], Loss: 0.1576\n",
      "Epoch [10/10], Step [714/938], Loss: 0.2672\n",
      "Epoch [10/10], Step [716/938], Loss: 0.1793\n",
      "Epoch [10/10], Step [718/938], Loss: 0.2049\n",
      "Epoch [10/10], Step [720/938], Loss: 0.2924\n",
      "Epoch [10/10], Step [722/938], Loss: 0.4265\n",
      "Epoch [10/10], Step [724/938], Loss: 0.1865\n",
      "Epoch [10/10], Step [726/938], Loss: 0.1250\n",
      "Epoch [10/10], Step [728/938], Loss: 0.1072\n",
      "Epoch [10/10], Step [730/938], Loss: 0.3667\n",
      "Epoch [10/10], Step [732/938], Loss: 0.2448\n",
      "Epoch [10/10], Step [734/938], Loss: 0.1369\n",
      "Epoch [10/10], Step [736/938], Loss: 0.1162\n",
      "Epoch [10/10], Step [738/938], Loss: 0.2045\n",
      "Epoch [10/10], Step [740/938], Loss: 0.2332\n",
      "Epoch [10/10], Step [742/938], Loss: 0.2630\n",
      "Epoch [10/10], Step [744/938], Loss: 0.1745\n",
      "Epoch [10/10], Step [746/938], Loss: 0.1356\n",
      "Epoch [10/10], Step [748/938], Loss: 0.4413\n",
      "Epoch [10/10], Step [750/938], Loss: 0.2720\n",
      "Epoch [10/10], Step [752/938], Loss: 0.0884\n",
      "Epoch [10/10], Step [754/938], Loss: 0.1176\n",
      "Epoch [10/10], Step [756/938], Loss: 0.2384\n",
      "Epoch [10/10], Step [758/938], Loss: 0.1496\n",
      "Epoch [10/10], Step [760/938], Loss: 0.1773\n",
      "Epoch [10/10], Step [762/938], Loss: 0.1318\n",
      "Epoch [10/10], Step [764/938], Loss: 0.2013\n",
      "Epoch [10/10], Step [766/938], Loss: 0.3018\n",
      "Epoch [10/10], Step [768/938], Loss: 0.1855\n",
      "Epoch [10/10], Step [770/938], Loss: 0.3333\n",
      "Epoch [10/10], Step [772/938], Loss: 0.2759\n",
      "Epoch [10/10], Step [774/938], Loss: 0.0552\n",
      "Epoch [10/10], Step [776/938], Loss: 0.3332\n",
      "Epoch [10/10], Step [778/938], Loss: 0.4053\n",
      "Epoch [10/10], Step [780/938], Loss: 0.1486\n",
      "Epoch [10/10], Step [782/938], Loss: 0.1649\n",
      "Epoch [10/10], Step [784/938], Loss: 0.1757\n",
      "Epoch [10/10], Step [786/938], Loss: 0.2100\n",
      "Epoch [10/10], Step [788/938], Loss: 0.2334\n",
      "Epoch [10/10], Step [790/938], Loss: 0.1640\n",
      "Epoch [10/10], Step [792/938], Loss: 0.2361\n",
      "Epoch [10/10], Step [794/938], Loss: 0.2905\n",
      "Epoch [10/10], Step [796/938], Loss: 0.3559\n",
      "Epoch [10/10], Step [798/938], Loss: 0.2047\n",
      "Epoch [10/10], Step [800/938], Loss: 0.1696\n",
      "Epoch [10/10], Step [802/938], Loss: 0.2695\n",
      "Epoch [10/10], Step [804/938], Loss: 0.1796\n",
      "Epoch [10/10], Step [806/938], Loss: 0.1123\n",
      "Epoch [10/10], Step [808/938], Loss: 0.2756\n",
      "Epoch [10/10], Step [810/938], Loss: 0.2309\n",
      "Epoch [10/10], Step [812/938], Loss: 0.1555\n",
      "Epoch [10/10], Step [814/938], Loss: 0.2325\n",
      "Epoch [10/10], Step [816/938], Loss: 0.1958\n",
      "Epoch [10/10], Step [818/938], Loss: 0.2133\n",
      "Epoch [10/10], Step [820/938], Loss: 0.1022\n",
      "Epoch [10/10], Step [822/938], Loss: 0.2368\n",
      "Epoch [10/10], Step [824/938], Loss: 0.2207\n",
      "Epoch [10/10], Step [826/938], Loss: 0.1122\n",
      "Epoch [10/10], Step [828/938], Loss: 0.1031\n",
      "Epoch [10/10], Step [830/938], Loss: 0.2223\n",
      "Epoch [10/10], Step [832/938], Loss: 0.1248\n",
      "Epoch [10/10], Step [834/938], Loss: 0.2813\n",
      "Epoch [10/10], Step [836/938], Loss: 0.1891\n",
      "Epoch [10/10], Step [838/938], Loss: 0.2804\n",
      "Epoch [10/10], Step [840/938], Loss: 0.1498\n",
      "Epoch [10/10], Step [842/938], Loss: 0.1692\n",
      "Epoch [10/10], Step [844/938], Loss: 0.1347\n",
      "Epoch [10/10], Step [846/938], Loss: 0.2768\n",
      "Epoch [10/10], Step [848/938], Loss: 0.3247\n",
      "Epoch [10/10], Step [850/938], Loss: 0.2906\n",
      "Epoch [10/10], Step [852/938], Loss: 0.2184\n",
      "Epoch [10/10], Step [854/938], Loss: 0.2602\n",
      "Epoch [10/10], Step [856/938], Loss: 0.2299\n",
      "Epoch [10/10], Step [858/938], Loss: 0.2404\n",
      "Epoch [10/10], Step [860/938], Loss: 0.2377\n",
      "Epoch [10/10], Step [862/938], Loss: 0.2113\n",
      "Epoch [10/10], Step [864/938], Loss: 0.1825\n",
      "Epoch [10/10], Step [866/938], Loss: 0.1787\n",
      "Epoch [10/10], Step [868/938], Loss: 0.2256\n",
      "Epoch [10/10], Step [870/938], Loss: 0.2292\n",
      "Epoch [10/10], Step [872/938], Loss: 0.1905\n",
      "Epoch [10/10], Step [874/938], Loss: 0.2455\n",
      "Epoch [10/10], Step [876/938], Loss: 0.1807\n",
      "Epoch [10/10], Step [878/938], Loss: 0.2615\n",
      "Epoch [10/10], Step [880/938], Loss: 0.2621\n",
      "Epoch [10/10], Step [882/938], Loss: 0.1486\n",
      "Epoch [10/10], Step [884/938], Loss: 0.1833\n",
      "Epoch [10/10], Step [886/938], Loss: 0.1722\n",
      "Epoch [10/10], Step [888/938], Loss: 0.2214\n",
      "Epoch [10/10], Step [890/938], Loss: 0.2079\n",
      "Epoch [10/10], Step [892/938], Loss: 0.2189\n",
      "Epoch [10/10], Step [894/938], Loss: 0.1711\n",
      "Epoch [10/10], Step [896/938], Loss: 0.3620\n",
      "Epoch [10/10], Step [898/938], Loss: 0.2417\n",
      "Epoch [10/10], Step [900/938], Loss: 0.1709\n",
      "Epoch [10/10], Step [902/938], Loss: 0.1032\n",
      "Epoch [10/10], Step [904/938], Loss: 0.1490\n",
      "Epoch [10/10], Step [906/938], Loss: 0.2599\n",
      "Epoch [10/10], Step [908/938], Loss: 0.0906\n",
      "Epoch [10/10], Step [910/938], Loss: 0.2062\n",
      "Epoch [10/10], Step [912/938], Loss: 0.3800\n",
      "Epoch [10/10], Step [914/938], Loss: 0.2802\n",
      "Epoch [10/10], Step [916/938], Loss: 0.3678\n",
      "Epoch [10/10], Step [918/938], Loss: 0.3809\n",
      "Epoch [10/10], Step [920/938], Loss: 0.1511\n",
      "Epoch [10/10], Step [922/938], Loss: 0.2227\n",
      "Epoch [10/10], Step [924/938], Loss: 0.2231\n",
      "Epoch [10/10], Step [926/938], Loss: 0.1499\n",
      "Epoch [10/10], Step [928/938], Loss: 0.1257\n",
      "Epoch [10/10], Step [930/938], Loss: 0.3105\n",
      "Epoch [10/10], Step [932/938], Loss: 0.2321\n",
      "Epoch [10/10], Step [934/938], Loss: 0.2021\n",
      "Epoch [10/10], Step [936/938], Loss: 0.1607\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0457\n",
      "Epoch [10/10], Loss: 0.2314\n",
      "Training is finished!\n",
      "Accuracy: 94.02%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model with SGD\")\n",
    "sgd_model = SimpleCNN().to(device)\n",
    "sgd_train_loss = train_model('SGD', sgd_model, train_loader, criterion, num_epochs)\n",
    "accuracy_sgd = evaluate_model(sgd_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2vHgS76sUker",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vHgS76sUker",
    "outputId": "90bbde61-0e85-448f-e191-4c6e17bb11e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with Adam\n",
      "Epoch [1/10], Step [2/938], Loss: 2.3197\n",
      "Epoch [1/10], Step [4/938], Loss: 2.2401\n",
      "Epoch [1/10], Step [6/938], Loss: 2.1576\n",
      "Epoch [1/10], Step [8/938], Loss: 2.0099\n",
      "Epoch [1/10], Step [10/938], Loss: 1.7569\n",
      "Epoch [1/10], Step [12/938], Loss: 1.5821\n",
      "Epoch [1/10], Step [14/938], Loss: 1.4750\n",
      "Epoch [1/10], Step [16/938], Loss: 1.1886\n",
      "Epoch [1/10], Step [18/938], Loss: 1.1125\n",
      "Epoch [1/10], Step [20/938], Loss: 0.9535\n",
      "Epoch [1/10], Step [22/938], Loss: 0.8162\n",
      "Epoch [1/10], Step [24/938], Loss: 0.8198\n",
      "Epoch [1/10], Step [26/938], Loss: 0.7208\n",
      "Epoch [1/10], Step [28/938], Loss: 0.5666\n",
      "Epoch [1/10], Step [30/938], Loss: 0.8223\n",
      "Epoch [1/10], Step [32/938], Loss: 0.6091\n",
      "Epoch [1/10], Step [34/938], Loss: 0.6378\n",
      "Epoch [1/10], Step [36/938], Loss: 0.4972\n",
      "Epoch [1/10], Step [38/938], Loss: 0.5832\n",
      "Epoch [1/10], Step [40/938], Loss: 0.3101\n",
      "Epoch [1/10], Step [42/938], Loss: 0.4493\n",
      "Epoch [1/10], Step [44/938], Loss: 0.4397\n",
      "Epoch [1/10], Step [46/938], Loss: 0.4518\n",
      "Epoch [1/10], Step [48/938], Loss: 0.3450\n",
      "Epoch [1/10], Step [50/938], Loss: 0.3863\n",
      "Epoch [1/10], Step [52/938], Loss: 0.3153\n",
      "Epoch [1/10], Step [54/938], Loss: 0.2989\n",
      "Epoch [1/10], Step [56/938], Loss: 0.2063\n",
      "Epoch [1/10], Step [58/938], Loss: 0.4510\n",
      "Epoch [1/10], Step [60/938], Loss: 0.2189\n",
      "Epoch [1/10], Step [62/938], Loss: 0.3250\n",
      "Epoch [1/10], Step [64/938], Loss: 0.3889\n",
      "Epoch [1/10], Step [66/938], Loss: 0.2190\n",
      "Epoch [1/10], Step [68/938], Loss: 0.2739\n",
      "Epoch [1/10], Step [70/938], Loss: 0.2935\n",
      "Epoch [1/10], Step [72/938], Loss: 0.2180\n",
      "Epoch [1/10], Step [74/938], Loss: 0.2225\n",
      "Epoch [1/10], Step [76/938], Loss: 0.2666\n",
      "Epoch [1/10], Step [78/938], Loss: 0.3870\n",
      "Epoch [1/10], Step [80/938], Loss: 0.2862\n",
      "Epoch [1/10], Step [82/938], Loss: 0.3658\n",
      "Epoch [1/10], Step [84/938], Loss: 0.3054\n",
      "Epoch [1/10], Step [86/938], Loss: 0.2433\n",
      "Epoch [1/10], Step [88/938], Loss: 0.3277\n",
      "Epoch [1/10], Step [90/938], Loss: 0.1744\n",
      "Epoch [1/10], Step [92/938], Loss: 0.2562\n",
      "Epoch [1/10], Step [94/938], Loss: 0.1539\n",
      "Epoch [1/10], Step [96/938], Loss: 0.1502\n",
      "Epoch [1/10], Step [98/938], Loss: 0.1413\n",
      "Epoch [1/10], Step [100/938], Loss: 0.2315\n",
      "Epoch [1/10], Step [102/938], Loss: 0.1809\n",
      "Epoch [1/10], Step [104/938], Loss: 0.1849\n",
      "Epoch [1/10], Step [106/938], Loss: 0.1812\n",
      "Epoch [1/10], Step [108/938], Loss: 0.2202\n",
      "Epoch [1/10], Step [110/938], Loss: 0.2384\n",
      "Epoch [1/10], Step [112/938], Loss: 0.1257\n",
      "Epoch [1/10], Step [114/938], Loss: 0.1412\n",
      "Epoch [1/10], Step [116/938], Loss: 0.1810\n",
      "Epoch [1/10], Step [118/938], Loss: 0.1895\n",
      "Epoch [1/10], Step [120/938], Loss: 0.2575\n",
      "Epoch [1/10], Step [122/938], Loss: 0.2281\n",
      "Epoch [1/10], Step [124/938], Loss: 0.2317\n",
      "Epoch [1/10], Step [126/938], Loss: 0.1115\n",
      "Epoch [1/10], Step [128/938], Loss: 0.2081\n",
      "Epoch [1/10], Step [130/938], Loss: 0.1739\n",
      "Epoch [1/10], Step [132/938], Loss: 0.1900\n",
      "Epoch [1/10], Step [134/938], Loss: 0.1570\n",
      "Epoch [1/10], Step [136/938], Loss: 0.2329\n",
      "Epoch [1/10], Step [138/938], Loss: 0.1075\n",
      "Epoch [1/10], Step [140/938], Loss: 0.1367\n",
      "Epoch [1/10], Step [142/938], Loss: 0.1448\n",
      "Epoch [1/10], Step [144/938], Loss: 0.1477\n",
      "Epoch [1/10], Step [146/938], Loss: 0.2053\n",
      "Epoch [1/10], Step [148/938], Loss: 0.1834\n",
      "Epoch [1/10], Step [150/938], Loss: 0.3675\n",
      "Epoch [1/10], Step [152/938], Loss: 0.1332\n",
      "Epoch [1/10], Step [154/938], Loss: 0.2332\n",
      "Epoch [1/10], Step [156/938], Loss: 0.2288\n",
      "Epoch [1/10], Step [158/938], Loss: 0.1757\n",
      "Epoch [1/10], Step [160/938], Loss: 0.1013\n",
      "Epoch [1/10], Step [162/938], Loss: 0.1193\n",
      "Epoch [1/10], Step [164/938], Loss: 0.1046\n",
      "Epoch [1/10], Step [166/938], Loss: 0.0849\n",
      "Epoch [1/10], Step [168/938], Loss: 0.2126\n",
      "Epoch [1/10], Step [170/938], Loss: 0.1757\n",
      "Epoch [1/10], Step [172/938], Loss: 0.2531\n",
      "Epoch [1/10], Step [174/938], Loss: 0.1249\n",
      "Epoch [1/10], Step [176/938], Loss: 0.1894\n",
      "Epoch [1/10], Step [178/938], Loss: 0.0974\n",
      "Epoch [1/10], Step [180/938], Loss: 0.1525\n",
      "Epoch [1/10], Step [182/938], Loss: 0.0984\n",
      "Epoch [1/10], Step [184/938], Loss: 0.1450\n",
      "Epoch [1/10], Step [186/938], Loss: 0.2686\n",
      "Epoch [1/10], Step [188/938], Loss: 0.1342\n",
      "Epoch [1/10], Step [190/938], Loss: 0.0464\n",
      "Epoch [1/10], Step [192/938], Loss: 0.2874\n",
      "Epoch [1/10], Step [194/938], Loss: 0.1928\n",
      "Epoch [1/10], Step [196/938], Loss: 0.1001\n",
      "Epoch [1/10], Step [198/938], Loss: 0.0793\n",
      "Epoch [1/10], Step [200/938], Loss: 0.1161\n",
      "Epoch [1/10], Step [202/938], Loss: 0.1824\n",
      "Epoch [1/10], Step [204/938], Loss: 0.1794\n",
      "Epoch [1/10], Step [206/938], Loss: 0.1180\n",
      "Epoch [1/10], Step [208/938], Loss: 0.3792\n",
      "Epoch [1/10], Step [210/938], Loss: 0.2113\n",
      "Epoch [1/10], Step [212/938], Loss: 0.1324\n",
      "Epoch [1/10], Step [214/938], Loss: 0.1666\n",
      "Epoch [1/10], Step [216/938], Loss: 0.2504\n",
      "Epoch [1/10], Step [218/938], Loss: 0.1091\n",
      "Epoch [1/10], Step [220/938], Loss: 0.1777\n",
      "Epoch [1/10], Step [222/938], Loss: 0.2193\n",
      "Epoch [1/10], Step [224/938], Loss: 0.0847\n",
      "Epoch [1/10], Step [226/938], Loss: 0.1034\n",
      "Epoch [1/10], Step [228/938], Loss: 0.1966\n",
      "Epoch [1/10], Step [230/938], Loss: 0.0821\n",
      "Epoch [1/10], Step [232/938], Loss: 0.0489\n",
      "Epoch [1/10], Step [234/938], Loss: 0.0565\n",
      "Epoch [1/10], Step [236/938], Loss: 0.0688\n",
      "Epoch [1/10], Step [238/938], Loss: 0.0657\n",
      "Epoch [1/10], Step [240/938], Loss: 0.1516\n",
      "Epoch [1/10], Step [242/938], Loss: 0.1678\n",
      "Epoch [1/10], Step [244/938], Loss: 0.1875\n",
      "Epoch [1/10], Step [246/938], Loss: 0.1497\n",
      "Epoch [1/10], Step [248/938], Loss: 0.1048\n",
      "Epoch [1/10], Step [250/938], Loss: 0.1691\n",
      "Epoch [1/10], Step [252/938], Loss: 0.1347\n",
      "Epoch [1/10], Step [254/938], Loss: 0.0604\n",
      "Epoch [1/10], Step [256/938], Loss: 0.0588\n",
      "Epoch [1/10], Step [258/938], Loss: 0.0217\n",
      "Epoch [1/10], Step [260/938], Loss: 0.0186\n",
      "Epoch [1/10], Step [262/938], Loss: 0.2063\n",
      "Epoch [1/10], Step [264/938], Loss: 0.0192\n",
      "Epoch [1/10], Step [266/938], Loss: 0.0170\n",
      "Epoch [1/10], Step [268/938], Loss: 0.0501\n",
      "Epoch [1/10], Step [270/938], Loss: 0.0919\n",
      "Epoch [1/10], Step [272/938], Loss: 0.0369\n",
      "Epoch [1/10], Step [274/938], Loss: 0.1183\n",
      "Epoch [1/10], Step [276/938], Loss: 0.1526\n",
      "Epoch [1/10], Step [278/938], Loss: 0.0273\n",
      "Epoch [1/10], Step [280/938], Loss: 0.1369\n",
      "Epoch [1/10], Step [282/938], Loss: 0.1058\n",
      "Epoch [1/10], Step [284/938], Loss: 0.1236\n",
      "Epoch [1/10], Step [286/938], Loss: 0.4236\n",
      "Epoch [1/10], Step [288/938], Loss: 0.0598\n",
      "Epoch [1/10], Step [290/938], Loss: 0.1116\n",
      "Epoch [1/10], Step [292/938], Loss: 0.2777\n",
      "Epoch [1/10], Step [294/938], Loss: 0.0738\n",
      "Epoch [1/10], Step [296/938], Loss: 0.1040\n",
      "Epoch [1/10], Step [298/938], Loss: 0.1170\n",
      "Epoch [1/10], Step [300/938], Loss: 0.0436\n",
      "Epoch [1/10], Step [302/938], Loss: 0.0283\n",
      "Epoch [1/10], Step [304/938], Loss: 0.0853\n",
      "Epoch [1/10], Step [306/938], Loss: 0.1337\n",
      "Epoch [1/10], Step [308/938], Loss: 0.1023\n",
      "Epoch [1/10], Step [310/938], Loss: 0.0248\n",
      "Epoch [1/10], Step [312/938], Loss: 0.1930\n",
      "Epoch [1/10], Step [314/938], Loss: 0.1105\n",
      "Epoch [1/10], Step [316/938], Loss: 0.2151\n",
      "Epoch [1/10], Step [318/938], Loss: 0.1724\n",
      "Epoch [1/10], Step [320/938], Loss: 0.0783\n",
      "Epoch [1/10], Step [322/938], Loss: 0.0435\n",
      "Epoch [1/10], Step [324/938], Loss: 0.0793\n",
      "Epoch [1/10], Step [326/938], Loss: 0.0408\n",
      "Epoch [1/10], Step [328/938], Loss: 0.1071\n",
      "Epoch [1/10], Step [330/938], Loss: 0.0110\n",
      "Epoch [1/10], Step [332/938], Loss: 0.1270\n",
      "Epoch [1/10], Step [334/938], Loss: 0.0721\n",
      "Epoch [1/10], Step [336/938], Loss: 0.0561\n",
      "Epoch [1/10], Step [338/938], Loss: 0.0855\n",
      "Epoch [1/10], Step [340/938], Loss: 0.0389\n",
      "Epoch [1/10], Step [342/938], Loss: 0.0980\n",
      "Epoch [1/10], Step [344/938], Loss: 0.0701\n",
      "Epoch [1/10], Step [346/938], Loss: 0.1528\n",
      "Epoch [1/10], Step [348/938], Loss: 0.0748\n",
      "Epoch [1/10], Step [350/938], Loss: 0.0200\n",
      "Epoch [1/10], Step [352/938], Loss: 0.0367\n",
      "Epoch [1/10], Step [354/938], Loss: 0.0302\n",
      "Epoch [1/10], Step [356/938], Loss: 0.0937\n",
      "Epoch [1/10], Step [358/938], Loss: 0.1288\n",
      "Epoch [1/10], Step [360/938], Loss: 0.0544\n",
      "Epoch [1/10], Step [362/938], Loss: 0.0305\n",
      "Epoch [1/10], Step [364/938], Loss: 0.0353\n",
      "Epoch [1/10], Step [366/938], Loss: 0.1233\n",
      "Epoch [1/10], Step [368/938], Loss: 0.0290\n",
      "Epoch [1/10], Step [370/938], Loss: 0.0973\n",
      "Epoch [1/10], Step [372/938], Loss: 0.1999\n",
      "Epoch [1/10], Step [374/938], Loss: 0.0660\n",
      "Epoch [1/10], Step [376/938], Loss: 0.0348\n",
      "Epoch [1/10], Step [378/938], Loss: 0.0330\n",
      "Epoch [1/10], Step [380/938], Loss: 0.0899\n",
      "Epoch [1/10], Step [382/938], Loss: 0.0469\n",
      "Epoch [1/10], Step [384/938], Loss: 0.0337\n",
      "Epoch [1/10], Step [386/938], Loss: 0.0292\n",
      "Epoch [1/10], Step [388/938], Loss: 0.0125\n",
      "Epoch [1/10], Step [390/938], Loss: 0.0386\n",
      "Epoch [1/10], Step [392/938], Loss: 0.1112\n",
      "Epoch [1/10], Step [394/938], Loss: 0.1202\n",
      "Epoch [1/10], Step [396/938], Loss: 0.0534\n",
      "Epoch [1/10], Step [398/938], Loss: 0.0671\n",
      "Epoch [1/10], Step [400/938], Loss: 0.0289\n",
      "Epoch [1/10], Step [402/938], Loss: 0.0746\n",
      "Epoch [1/10], Step [404/938], Loss: 0.1592\n",
      "Epoch [1/10], Step [406/938], Loss: 0.0738\n",
      "Epoch [1/10], Step [408/938], Loss: 0.0433\n",
      "Epoch [1/10], Step [410/938], Loss: 0.0367\n",
      "Epoch [1/10], Step [412/938], Loss: 0.1179\n",
      "Epoch [1/10], Step [414/938], Loss: 0.0591\n",
      "Epoch [1/10], Step [416/938], Loss: 0.0608\n",
      "Epoch [1/10], Step [418/938], Loss: 0.0645\n",
      "Epoch [1/10], Step [420/938], Loss: 0.0292\n",
      "Epoch [1/10], Step [422/938], Loss: 0.0776\n",
      "Epoch [1/10], Step [424/938], Loss: 0.2286\n",
      "Epoch [1/10], Step [426/938], Loss: 0.0277\n",
      "Epoch [1/10], Step [428/938], Loss: 0.0207\n",
      "Epoch [1/10], Step [430/938], Loss: 0.0851\n",
      "Epoch [1/10], Step [432/938], Loss: 0.0354\n",
      "Epoch [1/10], Step [434/938], Loss: 0.1604\n",
      "Epoch [1/10], Step [436/938], Loss: 0.0465\n",
      "Epoch [1/10], Step [438/938], Loss: 0.0277\n",
      "Epoch [1/10], Step [440/938], Loss: 0.0655\n",
      "Epoch [1/10], Step [442/938], Loss: 0.0131\n",
      "Epoch [1/10], Step [444/938], Loss: 0.0571\n",
      "Epoch [1/10], Step [446/938], Loss: 0.1244\n",
      "Epoch [1/10], Step [448/938], Loss: 0.0778\n",
      "Epoch [1/10], Step [450/938], Loss: 0.1076\n",
      "Epoch [1/10], Step [452/938], Loss: 0.0228\n",
      "Epoch [1/10], Step [454/938], Loss: 0.1312\n",
      "Epoch [1/10], Step [456/938], Loss: 0.2188\n",
      "Epoch [1/10], Step [458/938], Loss: 0.1861\n",
      "Epoch [1/10], Step [460/938], Loss: 0.0551\n",
      "Epoch [1/10], Step [462/938], Loss: 0.1373\n",
      "Epoch [1/10], Step [464/938], Loss: 0.0957\n",
      "Epoch [1/10], Step [466/938], Loss: 0.1629\n",
      "Epoch [1/10], Step [468/938], Loss: 0.0354\n",
      "Epoch [1/10], Step [470/938], Loss: 0.1431\n",
      "Epoch [1/10], Step [472/938], Loss: 0.1251\n",
      "Epoch [1/10], Step [474/938], Loss: 0.0726\n",
      "Epoch [1/10], Step [476/938], Loss: 0.0068\n",
      "Epoch [1/10], Step [478/938], Loss: 0.0376\n",
      "Epoch [1/10], Step [480/938], Loss: 0.0575\n",
      "Epoch [1/10], Step [482/938], Loss: 0.0924\n",
      "Epoch [1/10], Step [484/938], Loss: 0.0670\n",
      "Epoch [1/10], Step [486/938], Loss: 0.1411\n",
      "Epoch [1/10], Step [488/938], Loss: 0.0299\n",
      "Epoch [1/10], Step [490/938], Loss: 0.1139\n",
      "Epoch [1/10], Step [492/938], Loss: 0.1365\n",
      "Epoch [1/10], Step [494/938], Loss: 0.0140\n",
      "Epoch [1/10], Step [496/938], Loss: 0.0819\n",
      "Epoch [1/10], Step [498/938], Loss: 0.0441\n",
      "Epoch [1/10], Step [500/938], Loss: 0.1625\n",
      "Epoch [1/10], Step [502/938], Loss: 0.0296\n",
      "Epoch [1/10], Step [504/938], Loss: 0.0766\n",
      "Epoch [1/10], Step [506/938], Loss: 0.0671\n",
      "Epoch [1/10], Step [508/938], Loss: 0.1228\n",
      "Epoch [1/10], Step [510/938], Loss: 0.1841\n",
      "Epoch [1/10], Step [512/938], Loss: 0.0517\n",
      "Epoch [1/10], Step [514/938], Loss: 0.0682\n",
      "Epoch [1/10], Step [516/938], Loss: 0.0164\n",
      "Epoch [1/10], Step [518/938], Loss: 0.0350\n",
      "Epoch [1/10], Step [520/938], Loss: 0.0256\n",
      "Epoch [1/10], Step [522/938], Loss: 0.0356\n",
      "Epoch [1/10], Step [524/938], Loss: 0.0210\n",
      "Epoch [1/10], Step [526/938], Loss: 0.0436\n",
      "Epoch [1/10], Step [528/938], Loss: 0.0800\n",
      "Epoch [1/10], Step [530/938], Loss: 0.0428\n",
      "Epoch [1/10], Step [532/938], Loss: 0.0234\n",
      "Epoch [1/10], Step [534/938], Loss: 0.1237\n",
      "Epoch [1/10], Step [536/938], Loss: 0.0142\n",
      "Epoch [1/10], Step [538/938], Loss: 0.0526\n",
      "Epoch [1/10], Step [540/938], Loss: 0.0921\n",
      "Epoch [1/10], Step [542/938], Loss: 0.0850\n",
      "Epoch [1/10], Step [544/938], Loss: 0.0211\n",
      "Epoch [1/10], Step [546/938], Loss: 0.0530\n",
      "Epoch [1/10], Step [548/938], Loss: 0.0292\n",
      "Epoch [1/10], Step [550/938], Loss: 0.0441\n",
      "Epoch [1/10], Step [552/938], Loss: 0.0227\n",
      "Epoch [1/10], Step [554/938], Loss: 0.0258\n",
      "Epoch [1/10], Step [556/938], Loss: 0.0195\n",
      "Epoch [1/10], Step [558/938], Loss: 0.0908\n",
      "Epoch [1/10], Step [560/938], Loss: 0.0635\n",
      "Epoch [1/10], Step [562/938], Loss: 0.0555\n",
      "Epoch [1/10], Step [564/938], Loss: 0.0271\n",
      "Epoch [1/10], Step [566/938], Loss: 0.0143\n",
      "Epoch [1/10], Step [568/938], Loss: 0.0842\n",
      "Epoch [1/10], Step [570/938], Loss: 0.0103\n",
      "Epoch [1/10], Step [572/938], Loss: 0.0512\n",
      "Epoch [1/10], Step [574/938], Loss: 0.0685\n",
      "Epoch [1/10], Step [576/938], Loss: 0.0154\n",
      "Epoch [1/10], Step [578/938], Loss: 0.1586\n",
      "Epoch [1/10], Step [580/938], Loss: 0.1742\n",
      "Epoch [1/10], Step [582/938], Loss: 0.0251\n",
      "Epoch [1/10], Step [584/938], Loss: 0.1688\n",
      "Epoch [1/10], Step [586/938], Loss: 0.0664\n",
      "Epoch [1/10], Step [588/938], Loss: 0.0447\n",
      "Epoch [1/10], Step [590/938], Loss: 0.1305\n",
      "Epoch [1/10], Step [592/938], Loss: 0.1028\n",
      "Epoch [1/10], Step [594/938], Loss: 0.0242\n",
      "Epoch [1/10], Step [596/938], Loss: 0.1479\n",
      "Epoch [1/10], Step [598/938], Loss: 0.0470\n",
      "Epoch [1/10], Step [600/938], Loss: 0.1114\n",
      "Epoch [1/10], Step [602/938], Loss: 0.0235\n",
      "Epoch [1/10], Step [604/938], Loss: 0.0468\n",
      "Epoch [1/10], Step [606/938], Loss: 0.0360\n",
      "Epoch [1/10], Step [608/938], Loss: 0.1300\n",
      "Epoch [1/10], Step [610/938], Loss: 0.1174\n",
      "Epoch [1/10], Step [612/938], Loss: 0.1139\n",
      "Epoch [1/10], Step [614/938], Loss: 0.0071\n",
      "Epoch [1/10], Step [616/938], Loss: 0.0139\n",
      "Epoch [1/10], Step [618/938], Loss: 0.0498\n",
      "Epoch [1/10], Step [620/938], Loss: 0.0161\n",
      "Epoch [1/10], Step [622/938], Loss: 0.0969\n",
      "Epoch [1/10], Step [624/938], Loss: 0.0590\n",
      "Epoch [1/10], Step [626/938], Loss: 0.0511\n",
      "Epoch [1/10], Step [628/938], Loss: 0.0389\n",
      "Epoch [1/10], Step [630/938], Loss: 0.0483\n",
      "Epoch [1/10], Step [632/938], Loss: 0.1298\n",
      "Epoch [1/10], Step [634/938], Loss: 0.0448\n",
      "Epoch [1/10], Step [636/938], Loss: 0.0178\n",
      "Epoch [1/10], Step [638/938], Loss: 0.0639\n",
      "Epoch [1/10], Step [640/938], Loss: 0.0715\n",
      "Epoch [1/10], Step [642/938], Loss: 0.1366\n",
      "Epoch [1/10], Step [644/938], Loss: 0.0296\n",
      "Epoch [1/10], Step [646/938], Loss: 0.0811\n",
      "Epoch [1/10], Step [648/938], Loss: 0.0849\n",
      "Epoch [1/10], Step [650/938], Loss: 0.0052\n",
      "Epoch [1/10], Step [652/938], Loss: 0.0628\n",
      "Epoch [1/10], Step [654/938], Loss: 0.0130\n",
      "Epoch [1/10], Step [656/938], Loss: 0.0142\n",
      "Epoch [1/10], Step [658/938], Loss: 0.1023\n",
      "Epoch [1/10], Step [660/938], Loss: 0.2183\n",
      "Epoch [1/10], Step [662/938], Loss: 0.0310\n",
      "Epoch [1/10], Step [664/938], Loss: 0.0498\n",
      "Epoch [1/10], Step [666/938], Loss: 0.1084\n",
      "Epoch [1/10], Step [668/938], Loss: 0.0160\n",
      "Epoch [1/10], Step [670/938], Loss: 0.0086\n",
      "Epoch [1/10], Step [672/938], Loss: 0.1753\n",
      "Epoch [1/10], Step [674/938], Loss: 0.0401\n",
      "Epoch [1/10], Step [676/938], Loss: 0.0360\n",
      "Epoch [1/10], Step [678/938], Loss: 0.0074\n",
      "Epoch [1/10], Step [680/938], Loss: 0.0146\n",
      "Epoch [1/10], Step [682/938], Loss: 0.0239\n",
      "Epoch [1/10], Step [684/938], Loss: 0.1089\n",
      "Epoch [1/10], Step [686/938], Loss: 0.0246\n",
      "Epoch [1/10], Step [688/938], Loss: 0.0390\n",
      "Epoch [1/10], Step [690/938], Loss: 0.0962\n",
      "Epoch [1/10], Step [692/938], Loss: 0.0954\n",
      "Epoch [1/10], Step [694/938], Loss: 0.1449\n",
      "Epoch [1/10], Step [696/938], Loss: 0.1169\n",
      "Epoch [1/10], Step [698/938], Loss: 0.0838\n",
      "Epoch [1/10], Step [700/938], Loss: 0.0271\n",
      "Epoch [1/10], Step [702/938], Loss: 0.0276\n",
      "Epoch [1/10], Step [704/938], Loss: 0.0257\n",
      "Epoch [1/10], Step [706/938], Loss: 0.0127\n",
      "Epoch [1/10], Step [708/938], Loss: 0.0465\n",
      "Epoch [1/10], Step [710/938], Loss: 0.1055\n",
      "Epoch [1/10], Step [712/938], Loss: 0.0397\n",
      "Epoch [1/10], Step [714/938], Loss: 0.0962\n",
      "Epoch [1/10], Step [716/938], Loss: 0.0760\n",
      "Epoch [1/10], Step [718/938], Loss: 0.0548\n",
      "Epoch [1/10], Step [720/938], Loss: 0.2576\n",
      "Epoch [1/10], Step [722/938], Loss: 0.2157\n",
      "Epoch [1/10], Step [724/938], Loss: 0.0457\n",
      "Epoch [1/10], Step [726/938], Loss: 0.1744\n",
      "Epoch [1/10], Step [728/938], Loss: 0.1702\n",
      "Epoch [1/10], Step [730/938], Loss: 0.1105\n",
      "Epoch [1/10], Step [732/938], Loss: 0.0398\n",
      "Epoch [1/10], Step [734/938], Loss: 0.1746\n",
      "Epoch [1/10], Step [736/938], Loss: 0.0662\n",
      "Epoch [1/10], Step [738/938], Loss: 0.0235\n",
      "Epoch [1/10], Step [740/938], Loss: 0.0824\n",
      "Epoch [1/10], Step [742/938], Loss: 0.0474\n",
      "Epoch [1/10], Step [744/938], Loss: 0.0567\n",
      "Epoch [1/10], Step [746/938], Loss: 0.1019\n",
      "Epoch [1/10], Step [748/938], Loss: 0.0364\n",
      "Epoch [1/10], Step [750/938], Loss: 0.0319\n",
      "Epoch [1/10], Step [752/938], Loss: 0.0703\n",
      "Epoch [1/10], Step [754/938], Loss: 0.0093\n",
      "Epoch [1/10], Step [756/938], Loss: 0.0533\n",
      "Epoch [1/10], Step [758/938], Loss: 0.0412\n",
      "Epoch [1/10], Step [760/938], Loss: 0.0253\n",
      "Epoch [1/10], Step [762/938], Loss: 0.1664\n",
      "Epoch [1/10], Step [764/938], Loss: 0.0403\n",
      "Epoch [1/10], Step [766/938], Loss: 0.0380\n",
      "Epoch [1/10], Step [768/938], Loss: 0.0531\n",
      "Epoch [1/10], Step [770/938], Loss: 0.0368\n",
      "Epoch [1/10], Step [772/938], Loss: 0.2093\n",
      "Epoch [1/10], Step [774/938], Loss: 0.0460\n",
      "Epoch [1/10], Step [776/938], Loss: 0.0158\n",
      "Epoch [1/10], Step [778/938], Loss: 0.0898\n",
      "Epoch [1/10], Step [780/938], Loss: 0.0401\n",
      "Epoch [1/10], Step [782/938], Loss: 0.0830\n",
      "Epoch [1/10], Step [784/938], Loss: 0.0509\n",
      "Epoch [1/10], Step [786/938], Loss: 0.0352\n",
      "Epoch [1/10], Step [788/938], Loss: 0.0738\n",
      "Epoch [1/10], Step [790/938], Loss: 0.0214\n",
      "Epoch [1/10], Step [792/938], Loss: 0.0194\n",
      "Epoch [1/10], Step [794/938], Loss: 0.1133\n",
      "Epoch [1/10], Step [796/938], Loss: 0.2078\n",
      "Epoch [1/10], Step [798/938], Loss: 0.0128\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0898\n",
      "Epoch [1/10], Step [802/938], Loss: 0.0410\n",
      "Epoch [1/10], Step [804/938], Loss: 0.0836\n",
      "Epoch [1/10], Step [806/938], Loss: 0.0210\n",
      "Epoch [1/10], Step [808/938], Loss: 0.0255\n",
      "Epoch [1/10], Step [810/938], Loss: 0.0175\n",
      "Epoch [1/10], Step [812/938], Loss: 0.0129\n",
      "Epoch [1/10], Step [814/938], Loss: 0.1869\n",
      "Epoch [1/10], Step [816/938], Loss: 0.0368\n",
      "Epoch [1/10], Step [818/938], Loss: 0.0519\n",
      "Epoch [1/10], Step [820/938], Loss: 0.0892\n",
      "Epoch [1/10], Step [822/938], Loss: 0.0261\n",
      "Epoch [1/10], Step [824/938], Loss: 0.0647\n",
      "Epoch [1/10], Step [826/938], Loss: 0.0531\n",
      "Epoch [1/10], Step [828/938], Loss: 0.2444\n",
      "Epoch [1/10], Step [830/938], Loss: 0.1061\n",
      "Epoch [1/10], Step [832/938], Loss: 0.1403\n",
      "Epoch [1/10], Step [834/938], Loss: 0.0364\n",
      "Epoch [1/10], Step [836/938], Loss: 0.0100\n",
      "Epoch [1/10], Step [838/938], Loss: 0.0180\n",
      "Epoch [1/10], Step [840/938], Loss: 0.1000\n",
      "Epoch [1/10], Step [842/938], Loss: 0.0794\n",
      "Epoch [1/10], Step [844/938], Loss: 0.0171\n",
      "Epoch [1/10], Step [846/938], Loss: 0.0619\n",
      "Epoch [1/10], Step [848/938], Loss: 0.0086\n",
      "Epoch [1/10], Step [850/938], Loss: 0.0458\n",
      "Epoch [1/10], Step [852/938], Loss: 0.0827\n",
      "Epoch [1/10], Step [854/938], Loss: 0.0076\n",
      "Epoch [1/10], Step [856/938], Loss: 0.0158\n",
      "Epoch [1/10], Step [858/938], Loss: 0.0520\n",
      "Epoch [1/10], Step [860/938], Loss: 0.0260\n",
      "Epoch [1/10], Step [862/938], Loss: 0.0891\n",
      "Epoch [1/10], Step [864/938], Loss: 0.0300\n",
      "Epoch [1/10], Step [866/938], Loss: 0.2351\n",
      "Epoch [1/10], Step [868/938], Loss: 0.0722\n",
      "Epoch [1/10], Step [870/938], Loss: 0.0570\n",
      "Epoch [1/10], Step [872/938], Loss: 0.0098\n",
      "Epoch [1/10], Step [874/938], Loss: 0.0126\n",
      "Epoch [1/10], Step [876/938], Loss: 0.0355\n",
      "Epoch [1/10], Step [878/938], Loss: 0.0630\n",
      "Epoch [1/10], Step [880/938], Loss: 0.1046\n",
      "Epoch [1/10], Step [882/938], Loss: 0.0831\n",
      "Epoch [1/10], Step [884/938], Loss: 0.0686\n",
      "Epoch [1/10], Step [886/938], Loss: 0.0759\n",
      "Epoch [1/10], Step [888/938], Loss: 0.0605\n",
      "Epoch [1/10], Step [890/938], Loss: 0.0833\n",
      "Epoch [1/10], Step [892/938], Loss: 0.0114\n",
      "Epoch [1/10], Step [894/938], Loss: 0.1260\n",
      "Epoch [1/10], Step [896/938], Loss: 0.0141\n",
      "Epoch [1/10], Step [898/938], Loss: 0.1130\n",
      "Epoch [1/10], Step [900/938], Loss: 0.0587\n",
      "Epoch [1/10], Step [902/938], Loss: 0.0171\n",
      "Epoch [1/10], Step [904/938], Loss: 0.1261\n",
      "Epoch [1/10], Step [906/938], Loss: 0.0693\n",
      "Epoch [1/10], Step [908/938], Loss: 0.0327\n",
      "Epoch [1/10], Step [910/938], Loss: 0.0126\n",
      "Epoch [1/10], Step [912/938], Loss: 0.0668\n",
      "Epoch [1/10], Step [914/938], Loss: 0.0842\n",
      "Epoch [1/10], Step [916/938], Loss: 0.0332\n",
      "Epoch [1/10], Step [918/938], Loss: 0.0867\n",
      "Epoch [1/10], Step [920/938], Loss: 0.0115\n",
      "Epoch [1/10], Step [922/938], Loss: 0.0819\n",
      "Epoch [1/10], Step [924/938], Loss: 0.0520\n",
      "Epoch [1/10], Step [926/938], Loss: 0.0191\n",
      "Epoch [1/10], Step [928/938], Loss: 0.0199\n",
      "Epoch [1/10], Step [930/938], Loss: 0.0205\n",
      "Epoch [1/10], Step [932/938], Loss: 0.0066\n",
      "Epoch [1/10], Step [934/938], Loss: 0.0783\n",
      "Epoch [1/10], Step [936/938], Loss: 0.0570\n",
      "Epoch [1/10], Step [938/938], Loss: 0.0388\n",
      "Epoch [1/10], Loss: 0.1499\n",
      "Epoch [2/10], Step [2/938], Loss: 0.0585\n",
      "Epoch [2/10], Step [4/938], Loss: 0.0111\n",
      "Epoch [2/10], Step [6/938], Loss: 0.0085\n",
      "Epoch [2/10], Step [8/938], Loss: 0.0208\n",
      "Epoch [2/10], Step [10/938], Loss: 0.1218\n",
      "Epoch [2/10], Step [12/938], Loss: 0.0576\n",
      "Epoch [2/10], Step [14/938], Loss: 0.1795\n",
      "Epoch [2/10], Step [16/938], Loss: 0.0306\n",
      "Epoch [2/10], Step [18/938], Loss: 0.0853\n",
      "Epoch [2/10], Step [20/938], Loss: 0.0433\n",
      "Epoch [2/10], Step [22/938], Loss: 0.0343\n",
      "Epoch [2/10], Step [24/938], Loss: 0.0082\n",
      "Epoch [2/10], Step [26/938], Loss: 0.0031\n",
      "Epoch [2/10], Step [28/938], Loss: 0.0093\n",
      "Epoch [2/10], Step [30/938], Loss: 0.1186\n",
      "Epoch [2/10], Step [32/938], Loss: 0.0426\n",
      "Epoch [2/10], Step [34/938], Loss: 0.0789\n",
      "Epoch [2/10], Step [36/938], Loss: 0.0104\n",
      "Epoch [2/10], Step [38/938], Loss: 0.0096\n",
      "Epoch [2/10], Step [40/938], Loss: 0.0480\n",
      "Epoch [2/10], Step [42/938], Loss: 0.0088\n",
      "Epoch [2/10], Step [44/938], Loss: 0.0229\n",
      "Epoch [2/10], Step [46/938], Loss: 0.0559\n",
      "Epoch [2/10], Step [48/938], Loss: 0.0268\n",
      "Epoch [2/10], Step [50/938], Loss: 0.0162\n",
      "Epoch [2/10], Step [52/938], Loss: 0.0582\n",
      "Epoch [2/10], Step [54/938], Loss: 0.0627\n",
      "Epoch [2/10], Step [56/938], Loss: 0.0640\n",
      "Epoch [2/10], Step [58/938], Loss: 0.0376\n",
      "Epoch [2/10], Step [60/938], Loss: 0.0029\n",
      "Epoch [2/10], Step [62/938], Loss: 0.0207\n",
      "Epoch [2/10], Step [64/938], Loss: 0.0018\n",
      "Epoch [2/10], Step [66/938], Loss: 0.0038\n",
      "Epoch [2/10], Step [68/938], Loss: 0.0664\n",
      "Epoch [2/10], Step [70/938], Loss: 0.0540\n",
      "Epoch [2/10], Step [72/938], Loss: 0.0244\n",
      "Epoch [2/10], Step [74/938], Loss: 0.0238\n",
      "Epoch [2/10], Step [76/938], Loss: 0.0803\n",
      "Epoch [2/10], Step [78/938], Loss: 0.0777\n",
      "Epoch [2/10], Step [80/938], Loss: 0.0126\n",
      "Epoch [2/10], Step [82/938], Loss: 0.0998\n",
      "Epoch [2/10], Step [84/938], Loss: 0.0320\n",
      "Epoch [2/10], Step [86/938], Loss: 0.1112\n",
      "Epoch [2/10], Step [88/938], Loss: 0.0642\n",
      "Epoch [2/10], Step [90/938], Loss: 0.0874\n",
      "Epoch [2/10], Step [92/938], Loss: 0.1599\n",
      "Epoch [2/10], Step [94/938], Loss: 0.0318\n",
      "Epoch [2/10], Step [96/938], Loss: 0.0686\n",
      "Epoch [2/10], Step [98/938], Loss: 0.0391\n",
      "Epoch [2/10], Step [100/938], Loss: 0.0312\n",
      "Epoch [2/10], Step [102/938], Loss: 0.0236\n",
      "Epoch [2/10], Step [104/938], Loss: 0.0546\n",
      "Epoch [2/10], Step [106/938], Loss: 0.0081\n",
      "Epoch [2/10], Step [108/938], Loss: 0.0455\n",
      "Epoch [2/10], Step [110/938], Loss: 0.0212\n",
      "Epoch [2/10], Step [112/938], Loss: 0.0292\n",
      "Epoch [2/10], Step [114/938], Loss: 0.0685\n",
      "Epoch [2/10], Step [116/938], Loss: 0.0394\n",
      "Epoch [2/10], Step [118/938], Loss: 0.0490\n",
      "Epoch [2/10], Step [120/938], Loss: 0.0585\n",
      "Epoch [2/10], Step [122/938], Loss: 0.1165\n",
      "Epoch [2/10], Step [124/938], Loss: 0.0327\n",
      "Epoch [2/10], Step [126/938], Loss: 0.0141\n",
      "Epoch [2/10], Step [128/938], Loss: 0.0161\n",
      "Epoch [2/10], Step [130/938], Loss: 0.0226\n",
      "Epoch [2/10], Step [132/938], Loss: 0.0049\n",
      "Epoch [2/10], Step [134/938], Loss: 0.0203\n",
      "Epoch [2/10], Step [136/938], Loss: 0.0056\n",
      "Epoch [2/10], Step [138/938], Loss: 0.0534\n",
      "Epoch [2/10], Step [140/938], Loss: 0.0585\n",
      "Epoch [2/10], Step [142/938], Loss: 0.1108\n",
      "Epoch [2/10], Step [144/938], Loss: 0.0366\n",
      "Epoch [2/10], Step [146/938], Loss: 0.0173\n",
      "Epoch [2/10], Step [148/938], Loss: 0.0872\n",
      "Epoch [2/10], Step [150/938], Loss: 0.0575\n",
      "Epoch [2/10], Step [152/938], Loss: 0.0104\n",
      "Epoch [2/10], Step [154/938], Loss: 0.0659\n",
      "Epoch [2/10], Step [156/938], Loss: 0.0176\n",
      "Epoch [2/10], Step [158/938], Loss: 0.0116\n",
      "Epoch [2/10], Step [160/938], Loss: 0.0073\n",
      "Epoch [2/10], Step [162/938], Loss: 0.0485\n",
      "Epoch [2/10], Step [164/938], Loss: 0.0034\n",
      "Epoch [2/10], Step [166/938], Loss: 0.0193\n",
      "Epoch [2/10], Step [168/938], Loss: 0.0134\n",
      "Epoch [2/10], Step [170/938], Loss: 0.1021\n",
      "Epoch [2/10], Step [172/938], Loss: 0.0595\n",
      "Epoch [2/10], Step [174/938], Loss: 0.0546\n",
      "Epoch [2/10], Step [176/938], Loss: 0.0066\n",
      "Epoch [2/10], Step [178/938], Loss: 0.0520\n",
      "Epoch [2/10], Step [180/938], Loss: 0.0542\n",
      "Epoch [2/10], Step [182/938], Loss: 0.0018\n",
      "Epoch [2/10], Step [184/938], Loss: 0.0225\n",
      "Epoch [2/10], Step [186/938], Loss: 0.0459\n",
      "Epoch [2/10], Step [188/938], Loss: 0.0347\n",
      "Epoch [2/10], Step [190/938], Loss: 0.0070\n",
      "Epoch [2/10], Step [192/938], Loss: 0.0172\n",
      "Epoch [2/10], Step [194/938], Loss: 0.0370\n",
      "Epoch [2/10], Step [196/938], Loss: 0.0233\n",
      "Epoch [2/10], Step [198/938], Loss: 0.0369\n",
      "Epoch [2/10], Step [200/938], Loss: 0.0116\n",
      "Epoch [2/10], Step [202/938], Loss: 0.0029\n",
      "Epoch [2/10], Step [204/938], Loss: 0.0253\n",
      "Epoch [2/10], Step [206/938], Loss: 0.0411\n",
      "Epoch [2/10], Step [208/938], Loss: 0.0249\n",
      "Epoch [2/10], Step [210/938], Loss: 0.0318\n",
      "Epoch [2/10], Step [212/938], Loss: 0.0022\n",
      "Epoch [2/10], Step [214/938], Loss: 0.0408\n",
      "Epoch [2/10], Step [216/938], Loss: 0.0556\n",
      "Epoch [2/10], Step [218/938], Loss: 0.0320\n",
      "Epoch [2/10], Step [220/938], Loss: 0.0187\n",
      "Epoch [2/10], Step [222/938], Loss: 0.0733\n",
      "Epoch [2/10], Step [224/938], Loss: 0.0675\n",
      "Epoch [2/10], Step [226/938], Loss: 0.1186\n",
      "Epoch [2/10], Step [228/938], Loss: 0.0959\n",
      "Epoch [2/10], Step [230/938], Loss: 0.0234\n",
      "Epoch [2/10], Step [232/938], Loss: 0.0506\n",
      "Epoch [2/10], Step [234/938], Loss: 0.1038\n",
      "Epoch [2/10], Step [236/938], Loss: 0.0030\n",
      "Epoch [2/10], Step [238/938], Loss: 0.0137\n",
      "Epoch [2/10], Step [240/938], Loss: 0.0483\n",
      "Epoch [2/10], Step [242/938], Loss: 0.0076\n",
      "Epoch [2/10], Step [244/938], Loss: 0.0282\n",
      "Epoch [2/10], Step [246/938], Loss: 0.1297\n",
      "Epoch [2/10], Step [248/938], Loss: 0.1298\n",
      "Epoch [2/10], Step [250/938], Loss: 0.0315\n",
      "Epoch [2/10], Step [252/938], Loss: 0.0099\n",
      "Epoch [2/10], Step [254/938], Loss: 0.0771\n",
      "Epoch [2/10], Step [256/938], Loss: 0.1918\n",
      "Epoch [2/10], Step [258/938], Loss: 0.0204\n",
      "Epoch [2/10], Step [260/938], Loss: 0.0114\n",
      "Epoch [2/10], Step [262/938], Loss: 0.0253\n",
      "Epoch [2/10], Step [264/938], Loss: 0.0594\n",
      "Epoch [2/10], Step [266/938], Loss: 0.0158\n",
      "Epoch [2/10], Step [268/938], Loss: 0.0127\n",
      "Epoch [2/10], Step [270/938], Loss: 0.0475\n",
      "Epoch [2/10], Step [272/938], Loss: 0.0305\n",
      "Epoch [2/10], Step [274/938], Loss: 0.0309\n",
      "Epoch [2/10], Step [276/938], Loss: 0.0165\n",
      "Epoch [2/10], Step [278/938], Loss: 0.0645\n",
      "Epoch [2/10], Step [280/938], Loss: 0.0567\n",
      "Epoch [2/10], Step [282/938], Loss: 0.0596\n",
      "Epoch [2/10], Step [284/938], Loss: 0.0477\n",
      "Epoch [2/10], Step [286/938], Loss: 0.0384\n",
      "Epoch [2/10], Step [288/938], Loss: 0.0265\n",
      "Epoch [2/10], Step [290/938], Loss: 0.1057\n",
      "Epoch [2/10], Step [292/938], Loss: 0.0134\n",
      "Epoch [2/10], Step [294/938], Loss: 0.0675\n",
      "Epoch [2/10], Step [296/938], Loss: 0.0221\n",
      "Epoch [2/10], Step [298/938], Loss: 0.1079\n",
      "Epoch [2/10], Step [300/938], Loss: 0.0767\n",
      "Epoch [2/10], Step [302/938], Loss: 0.0163\n",
      "Epoch [2/10], Step [304/938], Loss: 0.0951\n",
      "Epoch [2/10], Step [306/938], Loss: 0.1456\n",
      "Epoch [2/10], Step [308/938], Loss: 0.0477\n",
      "Epoch [2/10], Step [310/938], Loss: 0.0562\n",
      "Epoch [2/10], Step [312/938], Loss: 0.1258\n",
      "Epoch [2/10], Step [314/938], Loss: 0.0521\n",
      "Epoch [2/10], Step [316/938], Loss: 0.1057\n",
      "Epoch [2/10], Step [318/938], Loss: 0.0215\n",
      "Epoch [2/10], Step [320/938], Loss: 0.0433\n",
      "Epoch [2/10], Step [322/938], Loss: 0.0770\n",
      "Epoch [2/10], Step [324/938], Loss: 0.0564\n",
      "Epoch [2/10], Step [326/938], Loss: 0.0597\n",
      "Epoch [2/10], Step [328/938], Loss: 0.0511\n",
      "Epoch [2/10], Step [330/938], Loss: 0.0196\n",
      "Epoch [2/10], Step [332/938], Loss: 0.0505\n",
      "Epoch [2/10], Step [334/938], Loss: 0.0089\n",
      "Epoch [2/10], Step [336/938], Loss: 0.0295\n",
      "Epoch [2/10], Step [338/938], Loss: 0.0904\n",
      "Epoch [2/10], Step [340/938], Loss: 0.0052\n",
      "Epoch [2/10], Step [342/938], Loss: 0.0100\n",
      "Epoch [2/10], Step [344/938], Loss: 0.0196\n",
      "Epoch [2/10], Step [346/938], Loss: 0.0069\n",
      "Epoch [2/10], Step [348/938], Loss: 0.1696\n",
      "Epoch [2/10], Step [350/938], Loss: 0.0412\n",
      "Epoch [2/10], Step [352/938], Loss: 0.1313\n",
      "Epoch [2/10], Step [354/938], Loss: 0.0161\n",
      "Epoch [2/10], Step [356/938], Loss: 0.0812\n",
      "Epoch [2/10], Step [358/938], Loss: 0.0048\n",
      "Epoch [2/10], Step [360/938], Loss: 0.0061\n",
      "Epoch [2/10], Step [362/938], Loss: 0.0114\n",
      "Epoch [2/10], Step [364/938], Loss: 0.1094\n",
      "Epoch [2/10], Step [366/938], Loss: 0.0324\n",
      "Epoch [2/10], Step [368/938], Loss: 0.0303\n",
      "Epoch [2/10], Step [370/938], Loss: 0.0188\n",
      "Epoch [2/10], Step [372/938], Loss: 0.0205\n",
      "Epoch [2/10], Step [374/938], Loss: 0.1154\n",
      "Epoch [2/10], Step [376/938], Loss: 0.0504\n",
      "Epoch [2/10], Step [378/938], Loss: 0.0879\n",
      "Epoch [2/10], Step [380/938], Loss: 0.0156\n",
      "Epoch [2/10], Step [382/938], Loss: 0.0388\n",
      "Epoch [2/10], Step [384/938], Loss: 0.0377\n",
      "Epoch [2/10], Step [386/938], Loss: 0.0075\n",
      "Epoch [2/10], Step [388/938], Loss: 0.0490\n",
      "Epoch [2/10], Step [390/938], Loss: 0.0169\n",
      "Epoch [2/10], Step [392/938], Loss: 0.1887\n",
      "Epoch [2/10], Step [394/938], Loss: 0.0129\n",
      "Epoch [2/10], Step [396/938], Loss: 0.0233\n",
      "Epoch [2/10], Step [398/938], Loss: 0.0079\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1738\n",
      "Epoch [2/10], Step [402/938], Loss: 0.0250\n",
      "Epoch [2/10], Step [404/938], Loss: 0.0309\n",
      "Epoch [2/10], Step [406/938], Loss: 0.0360\n",
      "Epoch [2/10], Step [408/938], Loss: 0.1621\n",
      "Epoch [2/10], Step [410/938], Loss: 0.0298\n",
      "Epoch [2/10], Step [412/938], Loss: 0.0259\n",
      "Epoch [2/10], Step [414/938], Loss: 0.0691\n",
      "Epoch [2/10], Step [416/938], Loss: 0.0152\n",
      "Epoch [2/10], Step [418/938], Loss: 0.1628\n",
      "Epoch [2/10], Step [420/938], Loss: 0.0189\n",
      "Epoch [2/10], Step [422/938], Loss: 0.0403\n",
      "Epoch [2/10], Step [424/938], Loss: 0.0165\n",
      "Epoch [2/10], Step [426/938], Loss: 0.1049\n",
      "Epoch [2/10], Step [428/938], Loss: 0.0268\n",
      "Epoch [2/10], Step [430/938], Loss: 0.0878\n",
      "Epoch [2/10], Step [432/938], Loss: 0.0515\n",
      "Epoch [2/10], Step [434/938], Loss: 0.0716\n",
      "Epoch [2/10], Step [436/938], Loss: 0.1003\n",
      "Epoch [2/10], Step [438/938], Loss: 0.0492\n",
      "Epoch [2/10], Step [440/938], Loss: 0.0231\n",
      "Epoch [2/10], Step [442/938], Loss: 0.0203\n",
      "Epoch [2/10], Step [444/938], Loss: 0.0129\n",
      "Epoch [2/10], Step [446/938], Loss: 0.0172\n",
      "Epoch [2/10], Step [448/938], Loss: 0.0407\n",
      "Epoch [2/10], Step [450/938], Loss: 0.0548\n",
      "Epoch [2/10], Step [452/938], Loss: 0.0565\n",
      "Epoch [2/10], Step [454/938], Loss: 0.0036\n",
      "Epoch [2/10], Step [456/938], Loss: 0.0630\n",
      "Epoch [2/10], Step [458/938], Loss: 0.0487\n",
      "Epoch [2/10], Step [460/938], Loss: 0.0467\n",
      "Epoch [2/10], Step [462/938], Loss: 0.0893\n",
      "Epoch [2/10], Step [464/938], Loss: 0.0103\n",
      "Epoch [2/10], Step [466/938], Loss: 0.0073\n",
      "Epoch [2/10], Step [468/938], Loss: 0.0300\n",
      "Epoch [2/10], Step [470/938], Loss: 0.0725\n",
      "Epoch [2/10], Step [472/938], Loss: 0.0231\n",
      "Epoch [2/10], Step [474/938], Loss: 0.0673\n",
      "Epoch [2/10], Step [476/938], Loss: 0.0348\n",
      "Epoch [2/10], Step [478/938], Loss: 0.0029\n",
      "Epoch [2/10], Step [480/938], Loss: 0.0209\n",
      "Epoch [2/10], Step [482/938], Loss: 0.0090\n",
      "Epoch [2/10], Step [484/938], Loss: 0.0351\n",
      "Epoch [2/10], Step [486/938], Loss: 0.1167\n",
      "Epoch [2/10], Step [488/938], Loss: 0.1175\n",
      "Epoch [2/10], Step [490/938], Loss: 0.0348\n",
      "Epoch [2/10], Step [492/938], Loss: 0.0205\n",
      "Epoch [2/10], Step [494/938], Loss: 0.1124\n",
      "Epoch [2/10], Step [496/938], Loss: 0.0263\n",
      "Epoch [2/10], Step [498/938], Loss: 0.0042\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0140\n",
      "Epoch [2/10], Step [502/938], Loss: 0.0376\n",
      "Epoch [2/10], Step [504/938], Loss: 0.0460\n",
      "Epoch [2/10], Step [506/938], Loss: 0.0109\n",
      "Epoch [2/10], Step [508/938], Loss: 0.0195\n",
      "Epoch [2/10], Step [510/938], Loss: 0.0098\n",
      "Epoch [2/10], Step [512/938], Loss: 0.0099\n",
      "Epoch [2/10], Step [514/938], Loss: 0.0388\n",
      "Epoch [2/10], Step [516/938], Loss: 0.0898\n",
      "Epoch [2/10], Step [518/938], Loss: 0.0263\n",
      "Epoch [2/10], Step [520/938], Loss: 0.0612\n",
      "Epoch [2/10], Step [522/938], Loss: 0.0255\n",
      "Epoch [2/10], Step [524/938], Loss: 0.0335\n",
      "Epoch [2/10], Step [526/938], Loss: 0.0213\n",
      "Epoch [2/10], Step [528/938], Loss: 0.0607\n",
      "Epoch [2/10], Step [530/938], Loss: 0.0253\n",
      "Epoch [2/10], Step [532/938], Loss: 0.0074\n",
      "Epoch [2/10], Step [534/938], Loss: 0.0129\n",
      "Epoch [2/10], Step [536/938], Loss: 0.0265\n",
      "Epoch [2/10], Step [538/938], Loss: 0.0264\n",
      "Epoch [2/10], Step [540/938], Loss: 0.1717\n",
      "Epoch [2/10], Step [542/938], Loss: 0.1178\n",
      "Epoch [2/10], Step [544/938], Loss: 0.0081\n",
      "Epoch [2/10], Step [546/938], Loss: 0.0937\n",
      "Epoch [2/10], Step [548/938], Loss: 0.0272\n",
      "Epoch [2/10], Step [550/938], Loss: 0.0298\n",
      "Epoch [2/10], Step [552/938], Loss: 0.0288\n",
      "Epoch [2/10], Step [554/938], Loss: 0.0438\n",
      "Epoch [2/10], Step [556/938], Loss: 0.0072\n",
      "Epoch [2/10], Step [558/938], Loss: 0.1268\n",
      "Epoch [2/10], Step [560/938], Loss: 0.0025\n",
      "Epoch [2/10], Step [562/938], Loss: 0.0181\n",
      "Epoch [2/10], Step [564/938], Loss: 0.0030\n",
      "Epoch [2/10], Step [566/938], Loss: 0.0051\n",
      "Epoch [2/10], Step [568/938], Loss: 0.0173\n",
      "Epoch [2/10], Step [570/938], Loss: 0.0511\n",
      "Epoch [2/10], Step [572/938], Loss: 0.0469\n",
      "Epoch [2/10], Step [574/938], Loss: 0.2383\n",
      "Epoch [2/10], Step [576/938], Loss: 0.0132\n",
      "Epoch [2/10], Step [578/938], Loss: 0.1130\n",
      "Epoch [2/10], Step [580/938], Loss: 0.0127\n",
      "Epoch [2/10], Step [582/938], Loss: 0.0077\n",
      "Epoch [2/10], Step [584/938], Loss: 0.0119\n",
      "Epoch [2/10], Step [586/938], Loss: 0.0361\n",
      "Epoch [2/10], Step [588/938], Loss: 0.0392\n",
      "Epoch [2/10], Step [590/938], Loss: 0.0301\n",
      "Epoch [2/10], Step [592/938], Loss: 0.0059\n",
      "Epoch [2/10], Step [594/938], Loss: 0.0526\n",
      "Epoch [2/10], Step [596/938], Loss: 0.0734\n",
      "Epoch [2/10], Step [598/938], Loss: 0.0656\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0681\n",
      "Epoch [2/10], Step [602/938], Loss: 0.0164\n",
      "Epoch [2/10], Step [604/938], Loss: 0.0144\n",
      "Epoch [2/10], Step [606/938], Loss: 0.0445\n",
      "Epoch [2/10], Step [608/938], Loss: 0.0687\n",
      "Epoch [2/10], Step [610/938], Loss: 0.0248\n",
      "Epoch [2/10], Step [612/938], Loss: 0.2958\n",
      "Epoch [2/10], Step [614/938], Loss: 0.1029\n",
      "Epoch [2/10], Step [616/938], Loss: 0.0052\n",
      "Epoch [2/10], Step [618/938], Loss: 0.0463\n",
      "Epoch [2/10], Step [620/938], Loss: 0.1694\n",
      "Epoch [2/10], Step [622/938], Loss: 0.0110\n",
      "Epoch [2/10], Step [624/938], Loss: 0.0048\n",
      "Epoch [2/10], Step [626/938], Loss: 0.0271\n",
      "Epoch [2/10], Step [628/938], Loss: 0.0606\n",
      "Epoch [2/10], Step [630/938], Loss: 0.0580\n",
      "Epoch [2/10], Step [632/938], Loss: 0.0033\n",
      "Epoch [2/10], Step [634/938], Loss: 0.0199\n",
      "Epoch [2/10], Step [636/938], Loss: 0.0243\n",
      "Epoch [2/10], Step [638/938], Loss: 0.0354\n",
      "Epoch [2/10], Step [640/938], Loss: 0.0142\n",
      "Epoch [2/10], Step [642/938], Loss: 0.0054\n",
      "Epoch [2/10], Step [644/938], Loss: 0.0219\n",
      "Epoch [2/10], Step [646/938], Loss: 0.0088\n",
      "Epoch [2/10], Step [648/938], Loss: 0.0347\n",
      "Epoch [2/10], Step [650/938], Loss: 0.0185\n",
      "Epoch [2/10], Step [652/938], Loss: 0.0269\n",
      "Epoch [2/10], Step [654/938], Loss: 0.0723\n",
      "Epoch [2/10], Step [656/938], Loss: 0.0463\n",
      "Epoch [2/10], Step [658/938], Loss: 0.0210\n",
      "Epoch [2/10], Step [660/938], Loss: 0.0115\n",
      "Epoch [2/10], Step [662/938], Loss: 0.0253\n",
      "Epoch [2/10], Step [664/938], Loss: 0.1546\n",
      "Epoch [2/10], Step [666/938], Loss: 0.0271\n",
      "Epoch [2/10], Step [668/938], Loss: 0.0183\n",
      "Epoch [2/10], Step [670/938], Loss: 0.0686\n",
      "Epoch [2/10], Step [672/938], Loss: 0.0189\n",
      "Epoch [2/10], Step [674/938], Loss: 0.0228\n",
      "Epoch [2/10], Step [676/938], Loss: 0.0206\n",
      "Epoch [2/10], Step [678/938], Loss: 0.0044\n",
      "Epoch [2/10], Step [680/938], Loss: 0.0274\n",
      "Epoch [2/10], Step [682/938], Loss: 0.0239\n",
      "Epoch [2/10], Step [684/938], Loss: 0.0063\n",
      "Epoch [2/10], Step [686/938], Loss: 0.0318\n",
      "Epoch [2/10], Step [688/938], Loss: 0.0178\n",
      "Epoch [2/10], Step [690/938], Loss: 0.0095\n",
      "Epoch [2/10], Step [692/938], Loss: 0.0043\n",
      "Epoch [2/10], Step [694/938], Loss: 0.0733\n",
      "Epoch [2/10], Step [696/938], Loss: 0.0250\n",
      "Epoch [2/10], Step [698/938], Loss: 0.0120\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0079\n",
      "Epoch [2/10], Step [702/938], Loss: 0.0427\n",
      "Epoch [2/10], Step [704/938], Loss: 0.0270\n",
      "Epoch [2/10], Step [706/938], Loss: 0.0232\n",
      "Epoch [2/10], Step [708/938], Loss: 0.0110\n",
      "Epoch [2/10], Step [710/938], Loss: 0.0273\n",
      "Epoch [2/10], Step [712/938], Loss: 0.0006\n",
      "Epoch [2/10], Step [714/938], Loss: 0.0115\n",
      "Epoch [2/10], Step [716/938], Loss: 0.0598\n",
      "Epoch [2/10], Step [718/938], Loss: 0.0195\n",
      "Epoch [2/10], Step [720/938], Loss: 0.0124\n",
      "Epoch [2/10], Step [722/938], Loss: 0.0126\n",
      "Epoch [2/10], Step [724/938], Loss: 0.0052\n",
      "Epoch [2/10], Step [726/938], Loss: 0.0028\n",
      "Epoch [2/10], Step [728/938], Loss: 0.1349\n",
      "Epoch [2/10], Step [730/938], Loss: 0.0526\n",
      "Epoch [2/10], Step [732/938], Loss: 0.0967\n",
      "Epoch [2/10], Step [734/938], Loss: 0.0211\n",
      "Epoch [2/10], Step [736/938], Loss: 0.1107\n",
      "Epoch [2/10], Step [738/938], Loss: 0.0228\n",
      "Epoch [2/10], Step [740/938], Loss: 0.0089\n",
      "Epoch [2/10], Step [742/938], Loss: 0.0399\n",
      "Epoch [2/10], Step [744/938], Loss: 0.0095\n",
      "Epoch [2/10], Step [746/938], Loss: 0.0065\n",
      "Epoch [2/10], Step [748/938], Loss: 0.0886\n",
      "Epoch [2/10], Step [750/938], Loss: 0.0410\n",
      "Epoch [2/10], Step [752/938], Loss: 0.0225\n",
      "Epoch [2/10], Step [754/938], Loss: 0.0134\n",
      "Epoch [2/10], Step [756/938], Loss: 0.0047\n",
      "Epoch [2/10], Step [758/938], Loss: 0.1325\n",
      "Epoch [2/10], Step [760/938], Loss: 0.0458\n",
      "Epoch [2/10], Step [762/938], Loss: 0.0334\n",
      "Epoch [2/10], Step [764/938], Loss: 0.0235\n",
      "Epoch [2/10], Step [766/938], Loss: 0.0023\n",
      "Epoch [2/10], Step [768/938], Loss: 0.0047\n",
      "Epoch [2/10], Step [770/938], Loss: 0.0566\n",
      "Epoch [2/10], Step [772/938], Loss: 0.0009\n",
      "Epoch [2/10], Step [774/938], Loss: 0.1196\n",
      "Epoch [2/10], Step [776/938], Loss: 0.0183\n",
      "Epoch [2/10], Step [778/938], Loss: 0.0364\n",
      "Epoch [2/10], Step [780/938], Loss: 0.0164\n",
      "Epoch [2/10], Step [782/938], Loss: 0.0120\n",
      "Epoch [2/10], Step [784/938], Loss: 0.0037\n",
      "Epoch [2/10], Step [786/938], Loss: 0.0223\n",
      "Epoch [2/10], Step [788/938], Loss: 0.0262\n",
      "Epoch [2/10], Step [790/938], Loss: 0.0426\n",
      "Epoch [2/10], Step [792/938], Loss: 0.0046\n",
      "Epoch [2/10], Step [794/938], Loss: 0.0015\n",
      "Epoch [2/10], Step [796/938], Loss: 0.0049\n",
      "Epoch [2/10], Step [798/938], Loss: 0.0443\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0638\n",
      "Epoch [2/10], Step [802/938], Loss: 0.0665\n",
      "Epoch [2/10], Step [804/938], Loss: 0.0028\n",
      "Epoch [2/10], Step [806/938], Loss: 0.1241\n",
      "Epoch [2/10], Step [808/938], Loss: 0.0015\n",
      "Epoch [2/10], Step [810/938], Loss: 0.0774\n",
      "Epoch [2/10], Step [812/938], Loss: 0.0854\n",
      "Epoch [2/10], Step [814/938], Loss: 0.0817\n",
      "Epoch [2/10], Step [816/938], Loss: 0.0604\n",
      "Epoch [2/10], Step [818/938], Loss: 0.0452\n",
      "Epoch [2/10], Step [820/938], Loss: 0.0155\n",
      "Epoch [2/10], Step [822/938], Loss: 0.0446\n",
      "Epoch [2/10], Step [824/938], Loss: 0.0184\n",
      "Epoch [2/10], Step [826/938], Loss: 0.0197\n",
      "Epoch [2/10], Step [828/938], Loss: 0.0279\n",
      "Epoch [2/10], Step [830/938], Loss: 0.0157\n",
      "Epoch [2/10], Step [832/938], Loss: 0.0069\n",
      "Epoch [2/10], Step [834/938], Loss: 0.0323\n",
      "Epoch [2/10], Step [836/938], Loss: 0.0152\n",
      "Epoch [2/10], Step [838/938], Loss: 0.0264\n",
      "Epoch [2/10], Step [840/938], Loss: 0.0054\n",
      "Epoch [2/10], Step [842/938], Loss: 0.0050\n",
      "Epoch [2/10], Step [844/938], Loss: 0.0073\n",
      "Epoch [2/10], Step [846/938], Loss: 0.0062\n",
      "Epoch [2/10], Step [848/938], Loss: 0.0832\n",
      "Epoch [2/10], Step [850/938], Loss: 0.0038\n",
      "Epoch [2/10], Step [852/938], Loss: 0.0533\n",
      "Epoch [2/10], Step [854/938], Loss: 0.0035\n",
      "Epoch [2/10], Step [856/938], Loss: 0.0234\n",
      "Epoch [2/10], Step [858/938], Loss: 0.0028\n",
      "Epoch [2/10], Step [860/938], Loss: 0.0042\n",
      "Epoch [2/10], Step [862/938], Loss: 0.0529\n",
      "Epoch [2/10], Step [864/938], Loss: 0.0186\n",
      "Epoch [2/10], Step [866/938], Loss: 0.1607\n",
      "Epoch [2/10], Step [868/938], Loss: 0.0058\n",
      "Epoch [2/10], Step [870/938], Loss: 0.0210\n",
      "Epoch [2/10], Step [872/938], Loss: 0.0019\n",
      "Epoch [2/10], Step [874/938], Loss: 0.0099\n",
      "Epoch [2/10], Step [876/938], Loss: 0.0098\n",
      "Epoch [2/10], Step [878/938], Loss: 0.0272\n",
      "Epoch [2/10], Step [880/938], Loss: 0.0057\n",
      "Epoch [2/10], Step [882/938], Loss: 0.0475\n",
      "Epoch [2/10], Step [884/938], Loss: 0.0172\n",
      "Epoch [2/10], Step [886/938], Loss: 0.0018\n",
      "Epoch [2/10], Step [888/938], Loss: 0.0679\n",
      "Epoch [2/10], Step [890/938], Loss: 0.0254\n",
      "Epoch [2/10], Step [892/938], Loss: 0.0029\n",
      "Epoch [2/10], Step [894/938], Loss: 0.0328\n",
      "Epoch [2/10], Step [896/938], Loss: 0.0226\n",
      "Epoch [2/10], Step [898/938], Loss: 0.0154\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0124\n",
      "Epoch [2/10], Step [902/938], Loss: 0.0281\n",
      "Epoch [2/10], Step [904/938], Loss: 0.0039\n",
      "Epoch [2/10], Step [906/938], Loss: 0.0032\n",
      "Epoch [2/10], Step [908/938], Loss: 0.0150\n",
      "Epoch [2/10], Step [910/938], Loss: 0.0589\n",
      "Epoch [2/10], Step [912/938], Loss: 0.0205\n",
      "Epoch [2/10], Step [914/938], Loss: 0.0027\n",
      "Epoch [2/10], Step [916/938], Loss: 0.0168\n",
      "Epoch [2/10], Step [918/938], Loss: 0.0307\n",
      "Epoch [2/10], Step [920/938], Loss: 0.0211\n",
      "Epoch [2/10], Step [922/938], Loss: 0.0596\n",
      "Epoch [2/10], Step [924/938], Loss: 0.0270\n",
      "Epoch [2/10], Step [926/938], Loss: 0.0043\n",
      "Epoch [2/10], Step [928/938], Loss: 0.1013\n",
      "Epoch [2/10], Step [930/938], Loss: 0.0467\n",
      "Epoch [2/10], Step [932/938], Loss: 0.0055\n",
      "Epoch [2/10], Step [934/938], Loss: 0.0805\n",
      "Epoch [2/10], Step [936/938], Loss: 0.0834\n",
      "Epoch [2/10], Step [938/938], Loss: 0.0016\n",
      "Epoch [2/10], Loss: 0.0424\n",
      "Epoch [3/10], Step [2/938], Loss: 0.0332\n",
      "Epoch [3/10], Step [4/938], Loss: 0.0169\n",
      "Epoch [3/10], Step [6/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [8/938], Loss: 0.0320\n",
      "Epoch [3/10], Step [10/938], Loss: 0.0066\n",
      "Epoch [3/10], Step [12/938], Loss: 0.0035\n",
      "Epoch [3/10], Step [14/938], Loss: 0.0074\n",
      "Epoch [3/10], Step [16/938], Loss: 0.0335\n",
      "Epoch [3/10], Step [18/938], Loss: 0.1270\n",
      "Epoch [3/10], Step [20/938], Loss: 0.0069\n",
      "Epoch [3/10], Step [22/938], Loss: 0.0150\n",
      "Epoch [3/10], Step [24/938], Loss: 0.0210\n",
      "Epoch [3/10], Step [26/938], Loss: 0.0309\n",
      "Epoch [3/10], Step [28/938], Loss: 0.0052\n",
      "Epoch [3/10], Step [30/938], Loss: 0.0240\n",
      "Epoch [3/10], Step [32/938], Loss: 0.0133\n",
      "Epoch [3/10], Step [34/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [36/938], Loss: 0.0139\n",
      "Epoch [3/10], Step [38/938], Loss: 0.0505\n",
      "Epoch [3/10], Step [40/938], Loss: 0.0515\n",
      "Epoch [3/10], Step [42/938], Loss: 0.0267\n",
      "Epoch [3/10], Step [44/938], Loss: 0.0037\n",
      "Epoch [3/10], Step [46/938], Loss: 0.0872\n",
      "Epoch [3/10], Step [48/938], Loss: 0.0104\n",
      "Epoch [3/10], Step [50/938], Loss: 0.0201\n",
      "Epoch [3/10], Step [52/938], Loss: 0.0179\n",
      "Epoch [3/10], Step [54/938], Loss: 0.0151\n",
      "Epoch [3/10], Step [56/938], Loss: 0.0688\n",
      "Epoch [3/10], Step [58/938], Loss: 0.0234\n",
      "Epoch [3/10], Step [60/938], Loss: 0.0767\n",
      "Epoch [3/10], Step [62/938], Loss: 0.0151\n",
      "Epoch [3/10], Step [64/938], Loss: 0.0151\n",
      "Epoch [3/10], Step [66/938], Loss: 0.0675\n",
      "Epoch [3/10], Step [68/938], Loss: 0.0208\n",
      "Epoch [3/10], Step [70/938], Loss: 0.0051\n",
      "Epoch [3/10], Step [72/938], Loss: 0.0102\n",
      "Epoch [3/10], Step [74/938], Loss: 0.0040\n",
      "Epoch [3/10], Step [76/938], Loss: 0.0146\n",
      "Epoch [3/10], Step [78/938], Loss: 0.0046\n",
      "Epoch [3/10], Step [80/938], Loss: 0.0841\n",
      "Epoch [3/10], Step [82/938], Loss: 0.0041\n",
      "Epoch [3/10], Step [84/938], Loss: 0.0593\n",
      "Epoch [3/10], Step [86/938], Loss: 0.0086\n",
      "Epoch [3/10], Step [88/938], Loss: 0.0498\n",
      "Epoch [3/10], Step [90/938], Loss: 0.0174\n",
      "Epoch [3/10], Step [92/938], Loss: 0.0095\n",
      "Epoch [3/10], Step [94/938], Loss: 0.1469\n",
      "Epoch [3/10], Step [96/938], Loss: 0.0115\n",
      "Epoch [3/10], Step [98/938], Loss: 0.0361\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0379\n",
      "Epoch [3/10], Step [102/938], Loss: 0.0366\n",
      "Epoch [3/10], Step [104/938], Loss: 0.0177\n",
      "Epoch [3/10], Step [106/938], Loss: 0.0524\n",
      "Epoch [3/10], Step [108/938], Loss: 0.0218\n",
      "Epoch [3/10], Step [110/938], Loss: 0.0330\n",
      "Epoch [3/10], Step [112/938], Loss: 0.0125\n",
      "Epoch [3/10], Step [114/938], Loss: 0.0973\n",
      "Epoch [3/10], Step [116/938], Loss: 0.0294\n",
      "Epoch [3/10], Step [118/938], Loss: 0.0016\n",
      "Epoch [3/10], Step [120/938], Loss: 0.0497\n",
      "Epoch [3/10], Step [122/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [124/938], Loss: 0.0038\n",
      "Epoch [3/10], Step [126/938], Loss: 0.0531\n",
      "Epoch [3/10], Step [128/938], Loss: 0.0290\n",
      "Epoch [3/10], Step [130/938], Loss: 0.0088\n",
      "Epoch [3/10], Step [132/938], Loss: 0.0324\n",
      "Epoch [3/10], Step [134/938], Loss: 0.0068\n",
      "Epoch [3/10], Step [136/938], Loss: 0.0085\n",
      "Epoch [3/10], Step [138/938], Loss: 0.0055\n",
      "Epoch [3/10], Step [140/938], Loss: 0.0941\n",
      "Epoch [3/10], Step [142/938], Loss: 0.0058\n",
      "Epoch [3/10], Step [144/938], Loss: 0.0050\n",
      "Epoch [3/10], Step [146/938], Loss: 0.0745\n",
      "Epoch [3/10], Step [148/938], Loss: 0.0180\n",
      "Epoch [3/10], Step [150/938], Loss: 0.0438\n",
      "Epoch [3/10], Step [152/938], Loss: 0.0010\n",
      "Epoch [3/10], Step [154/938], Loss: 0.0027\n",
      "Epoch [3/10], Step [156/938], Loss: 0.0031\n",
      "Epoch [3/10], Step [158/938], Loss: 0.0291\n",
      "Epoch [3/10], Step [160/938], Loss: 0.0242\n",
      "Epoch [3/10], Step [162/938], Loss: 0.0422\n",
      "Epoch [3/10], Step [164/938], Loss: 0.0026\n",
      "Epoch [3/10], Step [166/938], Loss: 0.0103\n",
      "Epoch [3/10], Step [168/938], Loss: 0.0142\n",
      "Epoch [3/10], Step [170/938], Loss: 0.1292\n",
      "Epoch [3/10], Step [172/938], Loss: 0.0024\n",
      "Epoch [3/10], Step [174/938], Loss: 0.0080\n",
      "Epoch [3/10], Step [176/938], Loss: 0.0315\n",
      "Epoch [3/10], Step [178/938], Loss: 0.0894\n",
      "Epoch [3/10], Step [180/938], Loss: 0.1244\n",
      "Epoch [3/10], Step [182/938], Loss: 0.0256\n",
      "Epoch [3/10], Step [184/938], Loss: 0.0110\n",
      "Epoch [3/10], Step [186/938], Loss: 0.0131\n",
      "Epoch [3/10], Step [188/938], Loss: 0.0042\n",
      "Epoch [3/10], Step [190/938], Loss: 0.0069\n",
      "Epoch [3/10], Step [192/938], Loss: 0.0737\n",
      "Epoch [3/10], Step [194/938], Loss: 0.0242\n",
      "Epoch [3/10], Step [196/938], Loss: 0.0125\n",
      "Epoch [3/10], Step [198/938], Loss: 0.0084\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0063\n",
      "Epoch [3/10], Step [202/938], Loss: 0.0895\n",
      "Epoch [3/10], Step [204/938], Loss: 0.0066\n",
      "Epoch [3/10], Step [206/938], Loss: 0.0028\n",
      "Epoch [3/10], Step [208/938], Loss: 0.0449\n",
      "Epoch [3/10], Step [210/938], Loss: 0.0552\n",
      "Epoch [3/10], Step [212/938], Loss: 0.0066\n",
      "Epoch [3/10], Step [214/938], Loss: 0.0609\n",
      "Epoch [3/10], Step [216/938], Loss: 0.1277\n",
      "Epoch [3/10], Step [218/938], Loss: 0.0293\n",
      "Epoch [3/10], Step [220/938], Loss: 0.0473\n",
      "Epoch [3/10], Step [222/938], Loss: 0.0059\n",
      "Epoch [3/10], Step [224/938], Loss: 0.0053\n",
      "Epoch [3/10], Step [226/938], Loss: 0.0770\n",
      "Epoch [3/10], Step [228/938], Loss: 0.0032\n",
      "Epoch [3/10], Step [230/938], Loss: 0.0133\n",
      "Epoch [3/10], Step [232/938], Loss: 0.0109\n",
      "Epoch [3/10], Step [234/938], Loss: 0.0225\n",
      "Epoch [3/10], Step [236/938], Loss: 0.0104\n",
      "Epoch [3/10], Step [238/938], Loss: 0.0213\n",
      "Epoch [3/10], Step [240/938], Loss: 0.0114\n",
      "Epoch [3/10], Step [242/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [244/938], Loss: 0.0105\n",
      "Epoch [3/10], Step [246/938], Loss: 0.0138\n",
      "Epoch [3/10], Step [248/938], Loss: 0.0102\n",
      "Epoch [3/10], Step [250/938], Loss: 0.0007\n",
      "Epoch [3/10], Step [252/938], Loss: 0.0251\n",
      "Epoch [3/10], Step [254/938], Loss: 0.0010\n",
      "Epoch [3/10], Step [256/938], Loss: 0.0983\n",
      "Epoch [3/10], Step [258/938], Loss: 0.0260\n",
      "Epoch [3/10], Step [260/938], Loss: 0.0035\n",
      "Epoch [3/10], Step [262/938], Loss: 0.0334\n",
      "Epoch [3/10], Step [264/938], Loss: 0.1046\n",
      "Epoch [3/10], Step [266/938], Loss: 0.0422\n",
      "Epoch [3/10], Step [268/938], Loss: 0.0628\n",
      "Epoch [3/10], Step [270/938], Loss: 0.0159\n",
      "Epoch [3/10], Step [272/938], Loss: 0.0313\n",
      "Epoch [3/10], Step [274/938], Loss: 0.0950\n",
      "Epoch [3/10], Step [276/938], Loss: 0.0886\n",
      "Epoch [3/10], Step [278/938], Loss: 0.0081\n",
      "Epoch [3/10], Step [280/938], Loss: 0.0037\n",
      "Epoch [3/10], Step [282/938], Loss: 0.0167\n",
      "Epoch [3/10], Step [284/938], Loss: 0.0141\n",
      "Epoch [3/10], Step [286/938], Loss: 0.0155\n",
      "Epoch [3/10], Step [288/938], Loss: 0.0197\n",
      "Epoch [3/10], Step [290/938], Loss: 0.0318\n",
      "Epoch [3/10], Step [292/938], Loss: 0.0124\n",
      "Epoch [3/10], Step [294/938], Loss: 0.0237\n",
      "Epoch [3/10], Step [296/938], Loss: 0.0354\n",
      "Epoch [3/10], Step [298/938], Loss: 0.0061\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0746\n",
      "Epoch [3/10], Step [302/938], Loss: 0.0249\n",
      "Epoch [3/10], Step [304/938], Loss: 0.0732\n",
      "Epoch [3/10], Step [306/938], Loss: 0.0069\n",
      "Epoch [3/10], Step [308/938], Loss: 0.0109\n",
      "Epoch [3/10], Step [310/938], Loss: 0.0076\n",
      "Epoch [3/10], Step [312/938], Loss: 0.0009\n",
      "Epoch [3/10], Step [314/938], Loss: 0.0309\n",
      "Epoch [3/10], Step [316/938], Loss: 0.0396\n",
      "Epoch [3/10], Step [318/938], Loss: 0.0051\n",
      "Epoch [3/10], Step [320/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [322/938], Loss: 0.0398\n",
      "Epoch [3/10], Step [324/938], Loss: 0.1546\n",
      "Epoch [3/10], Step [326/938], Loss: 0.0124\n",
      "Epoch [3/10], Step [328/938], Loss: 0.0118\n",
      "Epoch [3/10], Step [330/938], Loss: 0.0861\n",
      "Epoch [3/10], Step [332/938], Loss: 0.0992\n",
      "Epoch [3/10], Step [334/938], Loss: 0.0046\n",
      "Epoch [3/10], Step [336/938], Loss: 0.0088\n",
      "Epoch [3/10], Step [338/938], Loss: 0.0089\n",
      "Epoch [3/10], Step [340/938], Loss: 0.0207\n",
      "Epoch [3/10], Step [342/938], Loss: 0.0403\n",
      "Epoch [3/10], Step [344/938], Loss: 0.0429\n",
      "Epoch [3/10], Step [346/938], Loss: 0.0078\n",
      "Epoch [3/10], Step [348/938], Loss: 0.1880\n",
      "Epoch [3/10], Step [350/938], Loss: 0.0643\n",
      "Epoch [3/10], Step [352/938], Loss: 0.0773\n",
      "Epoch [3/10], Step [354/938], Loss: 0.0649\n",
      "Epoch [3/10], Step [356/938], Loss: 0.0091\n",
      "Epoch [3/10], Step [358/938], Loss: 0.0115\n",
      "Epoch [3/10], Step [360/938], Loss: 0.0423\n",
      "Epoch [3/10], Step [362/938], Loss: 0.0145\n",
      "Epoch [3/10], Step [364/938], Loss: 0.0712\n",
      "Epoch [3/10], Step [366/938], Loss: 0.0095\n",
      "Epoch [3/10], Step [368/938], Loss: 0.0023\n",
      "Epoch [3/10], Step [370/938], Loss: 0.0068\n",
      "Epoch [3/10], Step [372/938], Loss: 0.0096\n",
      "Epoch [3/10], Step [374/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [376/938], Loss: 0.0130\n",
      "Epoch [3/10], Step [378/938], Loss: 0.0030\n",
      "Epoch [3/10], Step [380/938], Loss: 0.0225\n",
      "Epoch [3/10], Step [382/938], Loss: 0.0130\n",
      "Epoch [3/10], Step [384/938], Loss: 0.0297\n",
      "Epoch [3/10], Step [386/938], Loss: 0.0142\n",
      "Epoch [3/10], Step [388/938], Loss: 0.0456\n",
      "Epoch [3/10], Step [390/938], Loss: 0.0981\n",
      "Epoch [3/10], Step [392/938], Loss: 0.0487\n",
      "Epoch [3/10], Step [394/938], Loss: 0.0316\n",
      "Epoch [3/10], Step [396/938], Loss: 0.0792\n",
      "Epoch [3/10], Step [398/938], Loss: 0.0165\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0312\n",
      "Epoch [3/10], Step [402/938], Loss: 0.0047\n",
      "Epoch [3/10], Step [404/938], Loss: 0.0095\n",
      "Epoch [3/10], Step [406/938], Loss: 0.0411\n",
      "Epoch [3/10], Step [408/938], Loss: 0.0100\n",
      "Epoch [3/10], Step [410/938], Loss: 0.0364\n",
      "Epoch [3/10], Step [412/938], Loss: 0.0901\n",
      "Epoch [3/10], Step [414/938], Loss: 0.0427\n",
      "Epoch [3/10], Step [416/938], Loss: 0.0372\n",
      "Epoch [3/10], Step [418/938], Loss: 0.0261\n",
      "Epoch [3/10], Step [420/938], Loss: 0.0009\n",
      "Epoch [3/10], Step [422/938], Loss: 0.0079\n",
      "Epoch [3/10], Step [424/938], Loss: 0.0064\n",
      "Epoch [3/10], Step [426/938], Loss: 0.0502\n",
      "Epoch [3/10], Step [428/938], Loss: 0.0070\n",
      "Epoch [3/10], Step [430/938], Loss: 0.0068\n",
      "Epoch [3/10], Step [432/938], Loss: 0.0192\n",
      "Epoch [3/10], Step [434/938], Loss: 0.0201\n",
      "Epoch [3/10], Step [436/938], Loss: 0.0194\n",
      "Epoch [3/10], Step [438/938], Loss: 0.0032\n",
      "Epoch [3/10], Step [440/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [442/938], Loss: 0.0326\n",
      "Epoch [3/10], Step [444/938], Loss: 0.0403\n",
      "Epoch [3/10], Step [446/938], Loss: 0.0061\n",
      "Epoch [3/10], Step [448/938], Loss: 0.0255\n",
      "Epoch [3/10], Step [450/938], Loss: 0.0322\n",
      "Epoch [3/10], Step [452/938], Loss: 0.0149\n",
      "Epoch [3/10], Step [454/938], Loss: 0.0184\n",
      "Epoch [3/10], Step [456/938], Loss: 0.0441\n",
      "Epoch [3/10], Step [458/938], Loss: 0.0013\n",
      "Epoch [3/10], Step [460/938], Loss: 0.0046\n",
      "Epoch [3/10], Step [462/938], Loss: 0.0038\n",
      "Epoch [3/10], Step [464/938], Loss: 0.0886\n",
      "Epoch [3/10], Step [466/938], Loss: 0.0008\n",
      "Epoch [3/10], Step [468/938], Loss: 0.0760\n",
      "Epoch [3/10], Step [470/938], Loss: 0.0029\n",
      "Epoch [3/10], Step [472/938], Loss: 0.0016\n",
      "Epoch [3/10], Step [474/938], Loss: 0.0063\n",
      "Epoch [3/10], Step [476/938], Loss: 0.0009\n",
      "Epoch [3/10], Step [478/938], Loss: 0.0096\n",
      "Epoch [3/10], Step [480/938], Loss: 0.0016\n",
      "Epoch [3/10], Step [482/938], Loss: 0.0788\n",
      "Epoch [3/10], Step [484/938], Loss: 0.0111\n",
      "Epoch [3/10], Step [486/938], Loss: 0.0003\n",
      "Epoch [3/10], Step [488/938], Loss: 0.0007\n",
      "Epoch [3/10], Step [490/938], Loss: 0.0100\n",
      "Epoch [3/10], Step [492/938], Loss: 0.0603\n",
      "Epoch [3/10], Step [494/938], Loss: 0.0897\n",
      "Epoch [3/10], Step [496/938], Loss: 0.0145\n",
      "Epoch [3/10], Step [498/938], Loss: 0.0204\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0051\n",
      "Epoch [3/10], Step [502/938], Loss: 0.0395\n",
      "Epoch [3/10], Step [504/938], Loss: 0.0010\n",
      "Epoch [3/10], Step [506/938], Loss: 0.0419\n",
      "Epoch [3/10], Step [508/938], Loss: 0.0057\n",
      "Epoch [3/10], Step [510/938], Loss: 0.0055\n",
      "Epoch [3/10], Step [512/938], Loss: 0.0501\n",
      "Epoch [3/10], Step [514/938], Loss: 0.0256\n",
      "Epoch [3/10], Step [516/938], Loss: 0.0223\n",
      "Epoch [3/10], Step [518/938], Loss: 0.0134\n",
      "Epoch [3/10], Step [520/938], Loss: 0.0699\n",
      "Epoch [3/10], Step [522/938], Loss: 0.0025\n",
      "Epoch [3/10], Step [524/938], Loss: 0.0009\n",
      "Epoch [3/10], Step [526/938], Loss: 0.0596\n",
      "Epoch [3/10], Step [528/938], Loss: 0.1072\n",
      "Epoch [3/10], Step [530/938], Loss: 0.0073\n",
      "Epoch [3/10], Step [532/938], Loss: 0.0223\n",
      "Epoch [3/10], Step [534/938], Loss: 0.1716\n",
      "Epoch [3/10], Step [536/938], Loss: 0.0229\n",
      "Epoch [3/10], Step [538/938], Loss: 0.0063\n",
      "Epoch [3/10], Step [540/938], Loss: 0.1242\n",
      "Epoch [3/10], Step [542/938], Loss: 0.0344\n",
      "Epoch [3/10], Step [544/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [546/938], Loss: 0.0063\n",
      "Epoch [3/10], Step [548/938], Loss: 0.0055\n",
      "Epoch [3/10], Step [550/938], Loss: 0.1004\n",
      "Epoch [3/10], Step [552/938], Loss: 0.0106\n",
      "Epoch [3/10], Step [554/938], Loss: 0.0071\n",
      "Epoch [3/10], Step [556/938], Loss: 0.0026\n",
      "Epoch [3/10], Step [558/938], Loss: 0.0220\n",
      "Epoch [3/10], Step [560/938], Loss: 0.0239\n",
      "Epoch [3/10], Step [562/938], Loss: 0.1074\n",
      "Epoch [3/10], Step [564/938], Loss: 0.0671\n",
      "Epoch [3/10], Step [566/938], Loss: 0.0066\n",
      "Epoch [3/10], Step [568/938], Loss: 0.0406\n",
      "Epoch [3/10], Step [570/938], Loss: 0.0234\n",
      "Epoch [3/10], Step [572/938], Loss: 0.0616\n",
      "Epoch [3/10], Step [574/938], Loss: 0.1222\n",
      "Epoch [3/10], Step [576/938], Loss: 0.0149\n",
      "Epoch [3/10], Step [578/938], Loss: 0.0452\n",
      "Epoch [3/10], Step [580/938], Loss: 0.0013\n",
      "Epoch [3/10], Step [582/938], Loss: 0.0079\n",
      "Epoch [3/10], Step [584/938], Loss: 0.0546\n",
      "Epoch [3/10], Step [586/938], Loss: 0.0312\n",
      "Epoch [3/10], Step [588/938], Loss: 0.0481\n",
      "Epoch [3/10], Step [590/938], Loss: 0.0596\n",
      "Epoch [3/10], Step [592/938], Loss: 0.0085\n",
      "Epoch [3/10], Step [594/938], Loss: 0.0074\n",
      "Epoch [3/10], Step [596/938], Loss: 0.0024\n",
      "Epoch [3/10], Step [598/938], Loss: 0.0241\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0020\n",
      "Epoch [3/10], Step [602/938], Loss: 0.0581\n",
      "Epoch [3/10], Step [604/938], Loss: 0.4510\n",
      "Epoch [3/10], Step [606/938], Loss: 0.0021\n",
      "Epoch [3/10], Step [608/938], Loss: 0.0051\n",
      "Epoch [3/10], Step [610/938], Loss: 0.0204\n",
      "Epoch [3/10], Step [612/938], Loss: 0.0860\n",
      "Epoch [3/10], Step [614/938], Loss: 0.0273\n",
      "Epoch [3/10], Step [616/938], Loss: 0.0542\n",
      "Epoch [3/10], Step [618/938], Loss: 0.0037\n",
      "Epoch [3/10], Step [620/938], Loss: 0.0061\n",
      "Epoch [3/10], Step [622/938], Loss: 0.0010\n",
      "Epoch [3/10], Step [624/938], Loss: 0.0172\n",
      "Epoch [3/10], Step [626/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [628/938], Loss: 0.0013\n",
      "Epoch [3/10], Step [630/938], Loss: 0.0043\n",
      "Epoch [3/10], Step [632/938], Loss: 0.0379\n",
      "Epoch [3/10], Step [634/938], Loss: 0.0272\n",
      "Epoch [3/10], Step [636/938], Loss: 0.0930\n",
      "Epoch [3/10], Step [638/938], Loss: 0.0311\n",
      "Epoch [3/10], Step [640/938], Loss: 0.0100\n",
      "Epoch [3/10], Step [642/938], Loss: 0.0041\n",
      "Epoch [3/10], Step [644/938], Loss: 0.0148\n",
      "Epoch [3/10], Step [646/938], Loss: 0.0408\n",
      "Epoch [3/10], Step [648/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [650/938], Loss: 0.0135\n",
      "Epoch [3/10], Step [652/938], Loss: 0.0100\n",
      "Epoch [3/10], Step [654/938], Loss: 0.0025\n",
      "Epoch [3/10], Step [656/938], Loss: 0.0069\n",
      "Epoch [3/10], Step [658/938], Loss: 0.0332\n",
      "Epoch [3/10], Step [660/938], Loss: 0.0323\n",
      "Epoch [3/10], Step [662/938], Loss: 0.0019\n",
      "Epoch [3/10], Step [664/938], Loss: 0.1944\n",
      "Epoch [3/10], Step [666/938], Loss: 0.0212\n",
      "Epoch [3/10], Step [668/938], Loss: 0.0257\n",
      "Epoch [3/10], Step [670/938], Loss: 0.0408\n",
      "Epoch [3/10], Step [672/938], Loss: 0.0051\n",
      "Epoch [3/10], Step [674/938], Loss: 0.0163\n",
      "Epoch [3/10], Step [676/938], Loss: 0.0278\n",
      "Epoch [3/10], Step [678/938], Loss: 0.0147\n",
      "Epoch [3/10], Step [680/938], Loss: 0.0122\n",
      "Epoch [3/10], Step [682/938], Loss: 0.0053\n",
      "Epoch [3/10], Step [684/938], Loss: 0.0517\n",
      "Epoch [3/10], Step [686/938], Loss: 0.0118\n",
      "Epoch [3/10], Step [688/938], Loss: 0.0462\n",
      "Epoch [3/10], Step [690/938], Loss: 0.0354\n",
      "Epoch [3/10], Step [692/938], Loss: 0.0221\n",
      "Epoch [3/10], Step [694/938], Loss: 0.0023\n",
      "Epoch [3/10], Step [696/938], Loss: 0.0254\n",
      "Epoch [3/10], Step [698/938], Loss: 0.0170\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0335\n",
      "Epoch [3/10], Step [702/938], Loss: 0.0098\n",
      "Epoch [3/10], Step [704/938], Loss: 0.0020\n",
      "Epoch [3/10], Step [706/938], Loss: 0.0210\n",
      "Epoch [3/10], Step [708/938], Loss: 0.0109\n",
      "Epoch [3/10], Step [710/938], Loss: 0.0370\n",
      "Epoch [3/10], Step [712/938], Loss: 0.1356\n",
      "Epoch [3/10], Step [714/938], Loss: 0.0226\n",
      "Epoch [3/10], Step [716/938], Loss: 0.0010\n",
      "Epoch [3/10], Step [718/938], Loss: 0.0018\n",
      "Epoch [3/10], Step [720/938], Loss: 0.0324\n",
      "Epoch [3/10], Step [722/938], Loss: 0.0762\n",
      "Epoch [3/10], Step [724/938], Loss: 0.0332\n",
      "Epoch [3/10], Step [726/938], Loss: 0.0256\n",
      "Epoch [3/10], Step [728/938], Loss: 0.0661\n",
      "Epoch [3/10], Step [730/938], Loss: 0.0040\n",
      "Epoch [3/10], Step [732/938], Loss: 0.0455\n",
      "Epoch [3/10], Step [734/938], Loss: 0.0515\n",
      "Epoch [3/10], Step [736/938], Loss: 0.0450\n",
      "Epoch [3/10], Step [738/938], Loss: 0.0112\n",
      "Epoch [3/10], Step [740/938], Loss: 0.0045\n",
      "Epoch [3/10], Step [742/938], Loss: 0.0732\n",
      "Epoch [3/10], Step [744/938], Loss: 0.0427\n",
      "Epoch [3/10], Step [746/938], Loss: 0.0021\n",
      "Epoch [3/10], Step [748/938], Loss: 0.0130\n",
      "Epoch [3/10], Step [750/938], Loss: 0.1551\n",
      "Epoch [3/10], Step [752/938], Loss: 0.0391\n",
      "Epoch [3/10], Step [754/938], Loss: 0.0475\n",
      "Epoch [3/10], Step [756/938], Loss: 0.1138\n",
      "Epoch [3/10], Step [758/938], Loss: 0.0177\n",
      "Epoch [3/10], Step [760/938], Loss: 0.0843\n",
      "Epoch [3/10], Step [762/938], Loss: 0.0187\n",
      "Epoch [3/10], Step [764/938], Loss: 0.1010\n",
      "Epoch [3/10], Step [766/938], Loss: 0.0048\n",
      "Epoch [3/10], Step [768/938], Loss: 0.1762\n",
      "Epoch [3/10], Step [770/938], Loss: 0.0081\n",
      "Epoch [3/10], Step [772/938], Loss: 0.0048\n",
      "Epoch [3/10], Step [774/938], Loss: 0.0235\n",
      "Epoch [3/10], Step [776/938], Loss: 0.0493\n",
      "Epoch [3/10], Step [778/938], Loss: 0.0022\n",
      "Epoch [3/10], Step [780/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [782/938], Loss: 0.0178\n",
      "Epoch [3/10], Step [784/938], Loss: 0.0422\n",
      "Epoch [3/10], Step [786/938], Loss: 0.0197\n",
      "Epoch [3/10], Step [788/938], Loss: 0.0254\n",
      "Epoch [3/10], Step [790/938], Loss: 0.0272\n",
      "Epoch [3/10], Step [792/938], Loss: 0.0075\n",
      "Epoch [3/10], Step [794/938], Loss: 0.0081\n",
      "Epoch [3/10], Step [796/938], Loss: 0.0494\n",
      "Epoch [3/10], Step [798/938], Loss: 0.0105\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0104\n",
      "Epoch [3/10], Step [802/938], Loss: 0.0142\n",
      "Epoch [3/10], Step [804/938], Loss: 0.0053\n",
      "Epoch [3/10], Step [806/938], Loss: 0.0059\n",
      "Epoch [3/10], Step [808/938], Loss: 0.0333\n",
      "Epoch [3/10], Step [810/938], Loss: 0.0061\n",
      "Epoch [3/10], Step [812/938], Loss: 0.0334\n",
      "Epoch [3/10], Step [814/938], Loss: 0.0613\n",
      "Epoch [3/10], Step [816/938], Loss: 0.0191\n",
      "Epoch [3/10], Step [818/938], Loss: 0.0178\n",
      "Epoch [3/10], Step [820/938], Loss: 0.0290\n",
      "Epoch [3/10], Step [822/938], Loss: 0.0161\n",
      "Epoch [3/10], Step [824/938], Loss: 0.0622\n",
      "Epoch [3/10], Step [826/938], Loss: 0.0070\n",
      "Epoch [3/10], Step [828/938], Loss: 0.0595\n",
      "Epoch [3/10], Step [830/938], Loss: 0.0245\n",
      "Epoch [3/10], Step [832/938], Loss: 0.0267\n",
      "Epoch [3/10], Step [834/938], Loss: 0.0209\n",
      "Epoch [3/10], Step [836/938], Loss: 0.1453\n",
      "Epoch [3/10], Step [838/938], Loss: 0.0437\n",
      "Epoch [3/10], Step [840/938], Loss: 0.0022\n",
      "Epoch [3/10], Step [842/938], Loss: 0.0414\n",
      "Epoch [3/10], Step [844/938], Loss: 0.0984\n",
      "Epoch [3/10], Step [846/938], Loss: 0.0172\n",
      "Epoch [3/10], Step [848/938], Loss: 0.0126\n",
      "Epoch [3/10], Step [850/938], Loss: 0.0119\n",
      "Epoch [3/10], Step [852/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [854/938], Loss: 0.0539\n",
      "Epoch [3/10], Step [856/938], Loss: 0.0129\n",
      "Epoch [3/10], Step [858/938], Loss: 0.0603\n",
      "Epoch [3/10], Step [860/938], Loss: 0.0095\n",
      "Epoch [3/10], Step [862/938], Loss: 0.0272\n",
      "Epoch [3/10], Step [864/938], Loss: 0.0008\n",
      "Epoch [3/10], Step [866/938], Loss: 0.0343\n",
      "Epoch [3/10], Step [868/938], Loss: 0.0056\n",
      "Epoch [3/10], Step [870/938], Loss: 0.0295\n",
      "Epoch [3/10], Step [872/938], Loss: 0.0024\n",
      "Epoch [3/10], Step [874/938], Loss: 0.0015\n",
      "Epoch [3/10], Step [876/938], Loss: 0.0305\n",
      "Epoch [3/10], Step [878/938], Loss: 0.0028\n",
      "Epoch [3/10], Step [880/938], Loss: 0.0682\n",
      "Epoch [3/10], Step [882/938], Loss: 0.0666\n",
      "Epoch [3/10], Step [884/938], Loss: 0.0925\n",
      "Epoch [3/10], Step [886/938], Loss: 0.1257\n",
      "Epoch [3/10], Step [888/938], Loss: 0.0042\n",
      "Epoch [3/10], Step [890/938], Loss: 0.0071\n",
      "Epoch [3/10], Step [892/938], Loss: 0.0022\n",
      "Epoch [3/10], Step [894/938], Loss: 0.0025\n",
      "Epoch [3/10], Step [896/938], Loss: 0.0021\n",
      "Epoch [3/10], Step [898/938], Loss: 0.1490\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0105\n",
      "Epoch [3/10], Step [902/938], Loss: 0.0061\n",
      "Epoch [3/10], Step [904/938], Loss: 0.0560\n",
      "Epoch [3/10], Step [906/938], Loss: 0.0118\n",
      "Epoch [3/10], Step [908/938], Loss: 0.0965\n",
      "Epoch [3/10], Step [910/938], Loss: 0.0867\n",
      "Epoch [3/10], Step [912/938], Loss: 0.1588\n",
      "Epoch [3/10], Step [914/938], Loss: 0.0019\n",
      "Epoch [3/10], Step [916/938], Loss: 0.0242\n",
      "Epoch [3/10], Step [918/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [920/938], Loss: 0.0056\n",
      "Epoch [3/10], Step [922/938], Loss: 0.0012\n",
      "Epoch [3/10], Step [924/938], Loss: 0.0470\n",
      "Epoch [3/10], Step [926/938], Loss: 0.0037\n",
      "Epoch [3/10], Step [928/938], Loss: 0.0171\n",
      "Epoch [3/10], Step [930/938], Loss: 0.0487\n",
      "Epoch [3/10], Step [932/938], Loss: 0.0101\n",
      "Epoch [3/10], Step [934/938], Loss: 0.0317\n",
      "Epoch [3/10], Step [936/938], Loss: 0.0083\n",
      "Epoch [3/10], Step [938/938], Loss: 0.0052\n",
      "Epoch [3/10], Loss: 0.0308\n",
      "Epoch [4/10], Step [2/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [4/938], Loss: 0.0330\n",
      "Epoch [4/10], Step [6/938], Loss: 0.0093\n",
      "Epoch [4/10], Step [8/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [10/938], Loss: 0.0138\n",
      "Epoch [4/10], Step [12/938], Loss: 0.0104\n",
      "Epoch [4/10], Step [14/938], Loss: 0.0019\n",
      "Epoch [4/10], Step [16/938], Loss: 0.0051\n",
      "Epoch [4/10], Step [18/938], Loss: 0.0112\n",
      "Epoch [4/10], Step [20/938], Loss: 0.0101\n",
      "Epoch [4/10], Step [22/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [24/938], Loss: 0.0899\n",
      "Epoch [4/10], Step [26/938], Loss: 0.0388\n",
      "Epoch [4/10], Step [28/938], Loss: 0.0183\n",
      "Epoch [4/10], Step [30/938], Loss: 0.0157\n",
      "Epoch [4/10], Step [32/938], Loss: 0.0044\n",
      "Epoch [4/10], Step [34/938], Loss: 0.0021\n",
      "Epoch [4/10], Step [36/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [38/938], Loss: 0.0009\n",
      "Epoch [4/10], Step [40/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [42/938], Loss: 0.0268\n",
      "Epoch [4/10], Step [44/938], Loss: 0.0110\n",
      "Epoch [4/10], Step [46/938], Loss: 0.0176\n",
      "Epoch [4/10], Step [48/938], Loss: 0.0003\n",
      "Epoch [4/10], Step [50/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [52/938], Loss: 0.0043\n",
      "Epoch [4/10], Step [54/938], Loss: 0.0348\n",
      "Epoch [4/10], Step [56/938], Loss: 0.1082\n",
      "Epoch [4/10], Step [58/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [60/938], Loss: 0.0066\n",
      "Epoch [4/10], Step [62/938], Loss: 0.0102\n",
      "Epoch [4/10], Step [64/938], Loss: 0.0157\n",
      "Epoch [4/10], Step [66/938], Loss: 0.0310\n",
      "Epoch [4/10], Step [68/938], Loss: 0.0074\n",
      "Epoch [4/10], Step [70/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [72/938], Loss: 0.0330\n",
      "Epoch [4/10], Step [74/938], Loss: 0.0169\n",
      "Epoch [4/10], Step [76/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [78/938], Loss: 0.0737\n",
      "Epoch [4/10], Step [80/938], Loss: 0.0003\n",
      "Epoch [4/10], Step [82/938], Loss: 0.0269\n",
      "Epoch [4/10], Step [84/938], Loss: 0.0928\n",
      "Epoch [4/10], Step [86/938], Loss: 0.0308\n",
      "Epoch [4/10], Step [88/938], Loss: 0.0383\n",
      "Epoch [4/10], Step [90/938], Loss: 0.0079\n",
      "Epoch [4/10], Step [92/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [94/938], Loss: 0.0126\n",
      "Epoch [4/10], Step [96/938], Loss: 0.0190\n",
      "Epoch [4/10], Step [98/938], Loss: 0.0180\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0053\n",
      "Epoch [4/10], Step [102/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [104/938], Loss: 0.0061\n",
      "Epoch [4/10], Step [106/938], Loss: 0.1031\n",
      "Epoch [4/10], Step [108/938], Loss: 0.0271\n",
      "Epoch [4/10], Step [110/938], Loss: 0.0778\n",
      "Epoch [4/10], Step [112/938], Loss: 0.0065\n",
      "Epoch [4/10], Step [114/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [116/938], Loss: 0.0097\n",
      "Epoch [4/10], Step [118/938], Loss: 0.0035\n",
      "Epoch [4/10], Step [120/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [122/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [124/938], Loss: 0.0601\n",
      "Epoch [4/10], Step [126/938], Loss: 0.0110\n",
      "Epoch [4/10], Step [128/938], Loss: 0.0239\n",
      "Epoch [4/10], Step [130/938], Loss: 0.0164\n",
      "Epoch [4/10], Step [132/938], Loss: 0.0115\n",
      "Epoch [4/10], Step [134/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [136/938], Loss: 0.0082\n",
      "Epoch [4/10], Step [138/938], Loss: 0.0171\n",
      "Epoch [4/10], Step [140/938], Loss: 0.0054\n",
      "Epoch [4/10], Step [142/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [144/938], Loss: 0.0752\n",
      "Epoch [4/10], Step [146/938], Loss: 0.0066\n",
      "Epoch [4/10], Step [148/938], Loss: 0.0057\n",
      "Epoch [4/10], Step [150/938], Loss: 0.0164\n",
      "Epoch [4/10], Step [152/938], Loss: 0.0268\n",
      "Epoch [4/10], Step [154/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [156/938], Loss: 0.0026\n",
      "Epoch [4/10], Step [158/938], Loss: 0.0041\n",
      "Epoch [4/10], Step [160/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [162/938], Loss: 0.0083\n",
      "Epoch [4/10], Step [164/938], Loss: 0.0083\n",
      "Epoch [4/10], Step [166/938], Loss: 0.0604\n",
      "Epoch [4/10], Step [168/938], Loss: 0.0002\n",
      "Epoch [4/10], Step [170/938], Loss: 0.0125\n",
      "Epoch [4/10], Step [172/938], Loss: 0.0039\n",
      "Epoch [4/10], Step [174/938], Loss: 0.0625\n",
      "Epoch [4/10], Step [176/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [178/938], Loss: 0.0148\n",
      "Epoch [4/10], Step [180/938], Loss: 0.0037\n",
      "Epoch [4/10], Step [182/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [184/938], Loss: 0.0415\n",
      "Epoch [4/10], Step [186/938], Loss: 0.0357\n",
      "Epoch [4/10], Step [188/938], Loss: 0.1145\n",
      "Epoch [4/10], Step [190/938], Loss: 0.0034\n",
      "Epoch [4/10], Step [192/938], Loss: 0.0272\n",
      "Epoch [4/10], Step [194/938], Loss: 0.0072\n",
      "Epoch [4/10], Step [196/938], Loss: 0.0056\n",
      "Epoch [4/10], Step [198/938], Loss: 0.0463\n",
      "Epoch [4/10], Step [200/938], Loss: 0.1170\n",
      "Epoch [4/10], Step [202/938], Loss: 0.0314\n",
      "Epoch [4/10], Step [204/938], Loss: 0.0047\n",
      "Epoch [4/10], Step [206/938], Loss: 0.0148\n",
      "Epoch [4/10], Step [208/938], Loss: 0.0076\n",
      "Epoch [4/10], Step [210/938], Loss: 0.0145\n",
      "Epoch [4/10], Step [212/938], Loss: 0.0143\n",
      "Epoch [4/10], Step [214/938], Loss: 0.0324\n",
      "Epoch [4/10], Step [216/938], Loss: 0.0432\n",
      "Epoch [4/10], Step [218/938], Loss: 0.0217\n",
      "Epoch [4/10], Step [220/938], Loss: 0.0098\n",
      "Epoch [4/10], Step [222/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [224/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [226/938], Loss: 0.0587\n",
      "Epoch [4/10], Step [228/938], Loss: 0.0195\n",
      "Epoch [4/10], Step [230/938], Loss: 0.0268\n",
      "Epoch [4/10], Step [232/938], Loss: 0.0335\n",
      "Epoch [4/10], Step [234/938], Loss: 0.0057\n",
      "Epoch [4/10], Step [236/938], Loss: 0.0210\n",
      "Epoch [4/10], Step [238/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [240/938], Loss: 0.0060\n",
      "Epoch [4/10], Step [242/938], Loss: 0.0805\n",
      "Epoch [4/10], Step [244/938], Loss: 0.0177\n",
      "Epoch [4/10], Step [246/938], Loss: 0.0178\n",
      "Epoch [4/10], Step [248/938], Loss: 0.0643\n",
      "Epoch [4/10], Step [250/938], Loss: 0.0636\n",
      "Epoch [4/10], Step [252/938], Loss: 0.0302\n",
      "Epoch [4/10], Step [254/938], Loss: 0.0062\n",
      "Epoch [4/10], Step [256/938], Loss: 0.0131\n",
      "Epoch [4/10], Step [258/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [260/938], Loss: 0.1122\n",
      "Epoch [4/10], Step [262/938], Loss: 0.0439\n",
      "Epoch [4/10], Step [264/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [266/938], Loss: 0.0086\n",
      "Epoch [4/10], Step [268/938], Loss: 0.0472\n",
      "Epoch [4/10], Step [270/938], Loss: 0.0164\n",
      "Epoch [4/10], Step [272/938], Loss: 0.0029\n",
      "Epoch [4/10], Step [274/938], Loss: 0.0009\n",
      "Epoch [4/10], Step [276/938], Loss: 0.0383\n",
      "Epoch [4/10], Step [278/938], Loss: 0.0043\n",
      "Epoch [4/10], Step [280/938], Loss: 0.0149\n",
      "Epoch [4/10], Step [282/938], Loss: 0.0104\n",
      "Epoch [4/10], Step [284/938], Loss: 0.0088\n",
      "Epoch [4/10], Step [286/938], Loss: 0.0160\n",
      "Epoch [4/10], Step [288/938], Loss: 0.0091\n",
      "Epoch [4/10], Step [290/938], Loss: 0.0447\n",
      "Epoch [4/10], Step [292/938], Loss: 0.0054\n",
      "Epoch [4/10], Step [294/938], Loss: 0.0059\n",
      "Epoch [4/10], Step [296/938], Loss: 0.0042\n",
      "Epoch [4/10], Step [298/938], Loss: 0.0211\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [302/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [304/938], Loss: 0.1452\n",
      "Epoch [4/10], Step [306/938], Loss: 0.0108\n",
      "Epoch [4/10], Step [308/938], Loss: 0.0063\n",
      "Epoch [4/10], Step [310/938], Loss: 0.0232\n",
      "Epoch [4/10], Step [312/938], Loss: 0.0090\n",
      "Epoch [4/10], Step [314/938], Loss: 0.0347\n",
      "Epoch [4/10], Step [316/938], Loss: 0.0337\n",
      "Epoch [4/10], Step [318/938], Loss: 0.0033\n",
      "Epoch [4/10], Step [320/938], Loss: 0.0298\n",
      "Epoch [4/10], Step [322/938], Loss: 0.0432\n",
      "Epoch [4/10], Step [324/938], Loss: 0.0039\n",
      "Epoch [4/10], Step [326/938], Loss: 0.0658\n",
      "Epoch [4/10], Step [328/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [330/938], Loss: 0.0287\n",
      "Epoch [4/10], Step [332/938], Loss: 0.0039\n",
      "Epoch [4/10], Step [334/938], Loss: 0.0058\n",
      "Epoch [4/10], Step [336/938], Loss: 0.0906\n",
      "Epoch [4/10], Step [338/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [340/938], Loss: 0.0202\n",
      "Epoch [4/10], Step [342/938], Loss: 0.0306\n",
      "Epoch [4/10], Step [344/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [346/938], Loss: 0.0248\n",
      "Epoch [4/10], Step [348/938], Loss: 0.0260\n",
      "Epoch [4/10], Step [350/938], Loss: 0.0095\n",
      "Epoch [4/10], Step [352/938], Loss: 0.0310\n",
      "Epoch [4/10], Step [354/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [356/938], Loss: 0.0574\n",
      "Epoch [4/10], Step [358/938], Loss: 0.0009\n",
      "Epoch [4/10], Step [360/938], Loss: 0.0509\n",
      "Epoch [4/10], Step [362/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [364/938], Loss: 0.0109\n",
      "Epoch [4/10], Step [366/938], Loss: 0.0145\n",
      "Epoch [4/10], Step [368/938], Loss: 0.0737\n",
      "Epoch [4/10], Step [370/938], Loss: 0.0386\n",
      "Epoch [4/10], Step [372/938], Loss: 0.0245\n",
      "Epoch [4/10], Step [374/938], Loss: 0.1048\n",
      "Epoch [4/10], Step [376/938], Loss: 0.0381\n",
      "Epoch [4/10], Step [378/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [380/938], Loss: 0.0077\n",
      "Epoch [4/10], Step [382/938], Loss: 0.0617\n",
      "Epoch [4/10], Step [384/938], Loss: 0.0695\n",
      "Epoch [4/10], Step [386/938], Loss: 0.1694\n",
      "Epoch [4/10], Step [388/938], Loss: 0.0053\n",
      "Epoch [4/10], Step [390/938], Loss: 0.0070\n",
      "Epoch [4/10], Step [392/938], Loss: 0.0526\n",
      "Epoch [4/10], Step [394/938], Loss: 0.0117\n",
      "Epoch [4/10], Step [396/938], Loss: 0.0171\n",
      "Epoch [4/10], Step [398/938], Loss: 0.0243\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [402/938], Loss: 0.0117\n",
      "Epoch [4/10], Step [404/938], Loss: 0.0051\n",
      "Epoch [4/10], Step [406/938], Loss: 0.0034\n",
      "Epoch [4/10], Step [408/938], Loss: 0.0067\n",
      "Epoch [4/10], Step [410/938], Loss: 0.0202\n",
      "Epoch [4/10], Step [412/938], Loss: 0.1408\n",
      "Epoch [4/10], Step [414/938], Loss: 0.0461\n",
      "Epoch [4/10], Step [416/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [418/938], Loss: 0.0554\n",
      "Epoch [4/10], Step [420/938], Loss: 0.0135\n",
      "Epoch [4/10], Step [422/938], Loss: 0.0062\n",
      "Epoch [4/10], Step [424/938], Loss: 0.0249\n",
      "Epoch [4/10], Step [426/938], Loss: 0.0173\n",
      "Epoch [4/10], Step [428/938], Loss: 0.0103\n",
      "Epoch [4/10], Step [430/938], Loss: 0.0637\n",
      "Epoch [4/10], Step [432/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [434/938], Loss: 0.0021\n",
      "Epoch [4/10], Step [436/938], Loss: 0.0149\n",
      "Epoch [4/10], Step [438/938], Loss: 0.0037\n",
      "Epoch [4/10], Step [440/938], Loss: 0.0054\n",
      "Epoch [4/10], Step [442/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [444/938], Loss: 0.0618\n",
      "Epoch [4/10], Step [446/938], Loss: 0.1032\n",
      "Epoch [4/10], Step [448/938], Loss: 0.0253\n",
      "Epoch [4/10], Step [450/938], Loss: 0.0115\n",
      "Epoch [4/10], Step [452/938], Loss: 0.0035\n",
      "Epoch [4/10], Step [454/938], Loss: 0.0077\n",
      "Epoch [4/10], Step [456/938], Loss: 0.0055\n",
      "Epoch [4/10], Step [458/938], Loss: 0.0105\n",
      "Epoch [4/10], Step [460/938], Loss: 0.0019\n",
      "Epoch [4/10], Step [462/938], Loss: 0.0042\n",
      "Epoch [4/10], Step [464/938], Loss: 0.0175\n",
      "Epoch [4/10], Step [466/938], Loss: 0.0335\n",
      "Epoch [4/10], Step [468/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [470/938], Loss: 0.0310\n",
      "Epoch [4/10], Step [472/938], Loss: 0.0315\n",
      "Epoch [4/10], Step [474/938], Loss: 0.0083\n",
      "Epoch [4/10], Step [476/938], Loss: 0.0706\n",
      "Epoch [4/10], Step [478/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [480/938], Loss: 0.0270\n",
      "Epoch [4/10], Step [482/938], Loss: 0.0189\n",
      "Epoch [4/10], Step [484/938], Loss: 0.0037\n",
      "Epoch [4/10], Step [486/938], Loss: 0.0456\n",
      "Epoch [4/10], Step [488/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [490/938], Loss: 0.0194\n",
      "Epoch [4/10], Step [492/938], Loss: 0.0021\n",
      "Epoch [4/10], Step [494/938], Loss: 0.0032\n",
      "Epoch [4/10], Step [496/938], Loss: 0.0013\n",
      "Epoch [4/10], Step [498/938], Loss: 0.0384\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [502/938], Loss: 0.0072\n",
      "Epoch [4/10], Step [504/938], Loss: 0.0263\n",
      "Epoch [4/10], Step [506/938], Loss: 0.0057\n",
      "Epoch [4/10], Step [508/938], Loss: 0.0159\n",
      "Epoch [4/10], Step [510/938], Loss: 0.0386\n",
      "Epoch [4/10], Step [512/938], Loss: 0.0117\n",
      "Epoch [4/10], Step [514/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [516/938], Loss: 0.0977\n",
      "Epoch [4/10], Step [518/938], Loss: 0.0021\n",
      "Epoch [4/10], Step [520/938], Loss: 0.0101\n",
      "Epoch [4/10], Step [522/938], Loss: 0.0511\n",
      "Epoch [4/10], Step [524/938], Loss: 0.0072\n",
      "Epoch [4/10], Step [526/938], Loss: 0.0051\n",
      "Epoch [4/10], Step [528/938], Loss: 0.0121\n",
      "Epoch [4/10], Step [530/938], Loss: 0.0175\n",
      "Epoch [4/10], Step [532/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [534/938], Loss: 0.0930\n",
      "Epoch [4/10], Step [536/938], Loss: 0.0145\n",
      "Epoch [4/10], Step [538/938], Loss: 0.0338\n",
      "Epoch [4/10], Step [540/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [542/938], Loss: 0.0143\n",
      "Epoch [4/10], Step [544/938], Loss: 0.2795\n",
      "Epoch [4/10], Step [546/938], Loss: 0.0019\n",
      "Epoch [4/10], Step [548/938], Loss: 0.0117\n",
      "Epoch [4/10], Step [550/938], Loss: 0.0298\n",
      "Epoch [4/10], Step [552/938], Loss: 0.0487\n",
      "Epoch [4/10], Step [554/938], Loss: 0.0353\n",
      "Epoch [4/10], Step [556/938], Loss: 0.0022\n",
      "Epoch [4/10], Step [558/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [560/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [562/938], Loss: 0.0035\n",
      "Epoch [4/10], Step [564/938], Loss: 0.0136\n",
      "Epoch [4/10], Step [566/938], Loss: 0.0063\n",
      "Epoch [4/10], Step [568/938], Loss: 0.0093\n",
      "Epoch [4/10], Step [570/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [572/938], Loss: 0.0252\n",
      "Epoch [4/10], Step [574/938], Loss: 0.0266\n",
      "Epoch [4/10], Step [576/938], Loss: 0.0071\n",
      "Epoch [4/10], Step [578/938], Loss: 0.0215\n",
      "Epoch [4/10], Step [580/938], Loss: 0.0068\n",
      "Epoch [4/10], Step [582/938], Loss: 0.0137\n",
      "Epoch [4/10], Step [584/938], Loss: 0.0808\n",
      "Epoch [4/10], Step [586/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [588/938], Loss: 0.0243\n",
      "Epoch [4/10], Step [590/938], Loss: 0.0377\n",
      "Epoch [4/10], Step [592/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [594/938], Loss: 0.0316\n",
      "Epoch [4/10], Step [596/938], Loss: 0.0367\n",
      "Epoch [4/10], Step [598/938], Loss: 0.0274\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [602/938], Loss: 0.0664\n",
      "Epoch [4/10], Step [604/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [606/938], Loss: 0.0196\n",
      "Epoch [4/10], Step [608/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [610/938], Loss: 0.0075\n",
      "Epoch [4/10], Step [612/938], Loss: 0.0194\n",
      "Epoch [4/10], Step [614/938], Loss: 0.0022\n",
      "Epoch [4/10], Step [616/938], Loss: 0.0008\n",
      "Epoch [4/10], Step [618/938], Loss: 0.0134\n",
      "Epoch [4/10], Step [620/938], Loss: 0.0155\n",
      "Epoch [4/10], Step [622/938], Loss: 0.0040\n",
      "Epoch [4/10], Step [624/938], Loss: 0.0192\n",
      "Epoch [4/10], Step [626/938], Loss: 0.0281\n",
      "Epoch [4/10], Step [628/938], Loss: 0.0141\n",
      "Epoch [4/10], Step [630/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [632/938], Loss: 0.0973\n",
      "Epoch [4/10], Step [634/938], Loss: 0.0212\n",
      "Epoch [4/10], Step [636/938], Loss: 0.0232\n",
      "Epoch [4/10], Step [638/938], Loss: 0.0308\n",
      "Epoch [4/10], Step [640/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [642/938], Loss: 0.0551\n",
      "Epoch [4/10], Step [644/938], Loss: 0.0615\n",
      "Epoch [4/10], Step [646/938], Loss: 0.0557\n",
      "Epoch [4/10], Step [648/938], Loss: 0.0311\n",
      "Epoch [4/10], Step [650/938], Loss: 0.0157\n",
      "Epoch [4/10], Step [652/938], Loss: 0.0191\n",
      "Epoch [4/10], Step [654/938], Loss: 0.0174\n",
      "Epoch [4/10], Step [656/938], Loss: 0.0536\n",
      "Epoch [4/10], Step [658/938], Loss: 0.0691\n",
      "Epoch [4/10], Step [660/938], Loss: 0.0638\n",
      "Epoch [4/10], Step [662/938], Loss: 0.0043\n",
      "Epoch [4/10], Step [664/938], Loss: 0.0065\n",
      "Epoch [4/10], Step [666/938], Loss: 0.0084\n",
      "Epoch [4/10], Step [668/938], Loss: 0.0187\n",
      "Epoch [4/10], Step [670/938], Loss: 0.0252\n",
      "Epoch [4/10], Step [672/938], Loss: 0.0149\n",
      "Epoch [4/10], Step [674/938], Loss: 0.0132\n",
      "Epoch [4/10], Step [676/938], Loss: 0.0013\n",
      "Epoch [4/10], Step [678/938], Loss: 0.0011\n",
      "Epoch [4/10], Step [680/938], Loss: 0.1166\n",
      "Epoch [4/10], Step [682/938], Loss: 0.0193\n",
      "Epoch [4/10], Step [684/938], Loss: 0.0116\n",
      "Epoch [4/10], Step [686/938], Loss: 0.0330\n",
      "Epoch [4/10], Step [688/938], Loss: 0.0729\n",
      "Epoch [4/10], Step [690/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [692/938], Loss: 0.0061\n",
      "Epoch [4/10], Step [694/938], Loss: 0.0031\n",
      "Epoch [4/10], Step [696/938], Loss: 0.0276\n",
      "Epoch [4/10], Step [698/938], Loss: 0.0042\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0061\n",
      "Epoch [4/10], Step [702/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [704/938], Loss: 0.0555\n",
      "Epoch [4/10], Step [706/938], Loss: 0.0071\n",
      "Epoch [4/10], Step [708/938], Loss: 0.0084\n",
      "Epoch [4/10], Step [710/938], Loss: 0.0093\n",
      "Epoch [4/10], Step [712/938], Loss: 0.0167\n",
      "Epoch [4/10], Step [714/938], Loss: 0.0029\n",
      "Epoch [4/10], Step [716/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [718/938], Loss: 0.0838\n",
      "Epoch [4/10], Step [720/938], Loss: 0.0681\n",
      "Epoch [4/10], Step [722/938], Loss: 0.0062\n",
      "Epoch [4/10], Step [724/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [726/938], Loss: 0.0344\n",
      "Epoch [4/10], Step [728/938], Loss: 0.0074\n",
      "Epoch [4/10], Step [730/938], Loss: 0.0242\n",
      "Epoch [4/10], Step [732/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [734/938], Loss: 0.0013\n",
      "Epoch [4/10], Step [736/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [738/938], Loss: 0.0056\n",
      "Epoch [4/10], Step [740/938], Loss: 0.0243\n",
      "Epoch [4/10], Step [742/938], Loss: 0.0423\n",
      "Epoch [4/10], Step [744/938], Loss: 0.0566\n",
      "Epoch [4/10], Step [746/938], Loss: 0.0411\n",
      "Epoch [4/10], Step [748/938], Loss: 0.0270\n",
      "Epoch [4/10], Step [750/938], Loss: 0.0352\n",
      "Epoch [4/10], Step [752/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [754/938], Loss: 0.0082\n",
      "Epoch [4/10], Step [756/938], Loss: 0.0144\n",
      "Epoch [4/10], Step [758/938], Loss: 0.0414\n",
      "Epoch [4/10], Step [760/938], Loss: 0.0107\n",
      "Epoch [4/10], Step [762/938], Loss: 0.0029\n",
      "Epoch [4/10], Step [764/938], Loss: 0.0102\n",
      "Epoch [4/10], Step [766/938], Loss: 0.0768\n",
      "Epoch [4/10], Step [768/938], Loss: 0.0216\n",
      "Epoch [4/10], Step [770/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [772/938], Loss: 0.0098\n",
      "Epoch [4/10], Step [774/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [776/938], Loss: 0.0465\n",
      "Epoch [4/10], Step [778/938], Loss: 0.0035\n",
      "Epoch [4/10], Step [780/938], Loss: 0.0110\n",
      "Epoch [4/10], Step [782/938], Loss: 0.0207\n",
      "Epoch [4/10], Step [784/938], Loss: 0.0231\n",
      "Epoch [4/10], Step [786/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [788/938], Loss: 0.0008\n",
      "Epoch [4/10], Step [790/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [792/938], Loss: 0.0262\n",
      "Epoch [4/10], Step [794/938], Loss: 0.0076\n",
      "Epoch [4/10], Step [796/938], Loss: 0.0013\n",
      "Epoch [4/10], Step [798/938], Loss: 0.0452\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0088\n",
      "Epoch [4/10], Step [802/938], Loss: 0.0044\n",
      "Epoch [4/10], Step [804/938], Loss: 0.0121\n",
      "Epoch [4/10], Step [806/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [808/938], Loss: 0.0102\n",
      "Epoch [4/10], Step [810/938], Loss: 0.0400\n",
      "Epoch [4/10], Step [812/938], Loss: 0.0003\n",
      "Epoch [4/10], Step [814/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [816/938], Loss: 0.0014\n",
      "Epoch [4/10], Step [818/938], Loss: 0.0033\n",
      "Epoch [4/10], Step [820/938], Loss: 0.0372\n",
      "Epoch [4/10], Step [822/938], Loss: 0.0310\n",
      "Epoch [4/10], Step [824/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [826/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [828/938], Loss: 0.0011\n",
      "Epoch [4/10], Step [830/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [832/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [834/938], Loss: 0.0037\n",
      "Epoch [4/10], Step [836/938], Loss: 0.0871\n",
      "Epoch [4/10], Step [838/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [840/938], Loss: 0.0044\n",
      "Epoch [4/10], Step [842/938], Loss: 0.0029\n",
      "Epoch [4/10], Step [844/938], Loss: 0.0092\n",
      "Epoch [4/10], Step [846/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [848/938], Loss: 0.0641\n",
      "Epoch [4/10], Step [850/938], Loss: 0.0210\n",
      "Epoch [4/10], Step [852/938], Loss: 0.0060\n",
      "Epoch [4/10], Step [854/938], Loss: 0.0009\n",
      "Epoch [4/10], Step [856/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [858/938], Loss: 0.0005\n",
      "Epoch [4/10], Step [860/938], Loss: 0.0732\n",
      "Epoch [4/10], Step [862/938], Loss: 0.0070\n",
      "Epoch [4/10], Step [864/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [866/938], Loss: 0.0042\n",
      "Epoch [4/10], Step [868/938], Loss: 0.0619\n",
      "Epoch [4/10], Step [870/938], Loss: 0.0022\n",
      "Epoch [4/10], Step [872/938], Loss: 0.0038\n",
      "Epoch [4/10], Step [874/938], Loss: 0.0122\n",
      "Epoch [4/10], Step [876/938], Loss: 0.1095\n",
      "Epoch [4/10], Step [878/938], Loss: 0.0116\n",
      "Epoch [4/10], Step [880/938], Loss: 0.0560\n",
      "Epoch [4/10], Step [882/938], Loss: 0.0302\n",
      "Epoch [4/10], Step [884/938], Loss: 0.0426\n",
      "Epoch [4/10], Step [886/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [888/938], Loss: 0.0034\n",
      "Epoch [4/10], Step [890/938], Loss: 0.0401\n",
      "Epoch [4/10], Step [892/938], Loss: 0.0642\n",
      "Epoch [4/10], Step [894/938], Loss: 0.0181\n",
      "Epoch [4/10], Step [896/938], Loss: 0.0189\n",
      "Epoch [4/10], Step [898/938], Loss: 0.0241\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0646\n",
      "Epoch [4/10], Step [902/938], Loss: 0.0920\n",
      "Epoch [4/10], Step [904/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [906/938], Loss: 0.0043\n",
      "Epoch [4/10], Step [908/938], Loss: 0.0484\n",
      "Epoch [4/10], Step [910/938], Loss: 0.0363\n",
      "Epoch [4/10], Step [912/938], Loss: 0.0589\n",
      "Epoch [4/10], Step [914/938], Loss: 0.0042\n",
      "Epoch [4/10], Step [916/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [918/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [920/938], Loss: 0.0142\n",
      "Epoch [4/10], Step [922/938], Loss: 0.0139\n",
      "Epoch [4/10], Step [924/938], Loss: 0.0329\n",
      "Epoch [4/10], Step [926/938], Loss: 0.0581\n",
      "Epoch [4/10], Step [928/938], Loss: 0.0044\n",
      "Epoch [4/10], Step [930/938], Loss: 0.0356\n",
      "Epoch [4/10], Step [932/938], Loss: 0.0026\n",
      "Epoch [4/10], Step [934/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [936/938], Loss: 0.0003\n",
      "Epoch [4/10], Step [938/938], Loss: 0.0121\n",
      "Epoch [4/10], Loss: 0.0225\n",
      "Epoch [5/10], Step [2/938], Loss: 0.0123\n",
      "Epoch [5/10], Step [4/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [6/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [8/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [10/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [12/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [14/938], Loss: 0.0082\n",
      "Epoch [5/10], Step [16/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [18/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [20/938], Loss: 0.0828\n",
      "Epoch [5/10], Step [22/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [24/938], Loss: 0.0103\n",
      "Epoch [5/10], Step [26/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [28/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [30/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [32/938], Loss: 0.0279\n",
      "Epoch [5/10], Step [34/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [36/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [38/938], Loss: 0.0160\n",
      "Epoch [5/10], Step [40/938], Loss: 0.0143\n",
      "Epoch [5/10], Step [42/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [44/938], Loss: 0.0035\n",
      "Epoch [5/10], Step [46/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [48/938], Loss: 0.0182\n",
      "Epoch [5/10], Step [50/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [52/938], Loss: 0.0036\n",
      "Epoch [5/10], Step [54/938], Loss: 0.0039\n",
      "Epoch [5/10], Step [56/938], Loss: 0.0348\n",
      "Epoch [5/10], Step [58/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [60/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [62/938], Loss: 0.0238\n",
      "Epoch [5/10], Step [64/938], Loss: 0.0031\n",
      "Epoch [5/10], Step [66/938], Loss: 0.0410\n",
      "Epoch [5/10], Step [68/938], Loss: 0.0108\n",
      "Epoch [5/10], Step [70/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [72/938], Loss: 0.0457\n",
      "Epoch [5/10], Step [74/938], Loss: 0.0136\n",
      "Epoch [5/10], Step [76/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [78/938], Loss: 0.0129\n",
      "Epoch [5/10], Step [80/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [82/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [84/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [86/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [88/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [90/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [92/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [94/938], Loss: 0.0194\n",
      "Epoch [5/10], Step [96/938], Loss: 0.0270\n",
      "Epoch [5/10], Step [98/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [102/938], Loss: 0.0073\n",
      "Epoch [5/10], Step [104/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [106/938], Loss: 0.0041\n",
      "Epoch [5/10], Step [108/938], Loss: 0.0198\n",
      "Epoch [5/10], Step [110/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [112/938], Loss: 0.0399\n",
      "Epoch [5/10], Step [114/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [116/938], Loss: 0.0050\n",
      "Epoch [5/10], Step [118/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [120/938], Loss: 0.0152\n",
      "Epoch [5/10], Step [122/938], Loss: 0.0031\n",
      "Epoch [5/10], Step [124/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [126/938], Loss: 0.0085\n",
      "Epoch [5/10], Step [128/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [130/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [132/938], Loss: 0.0391\n",
      "Epoch [5/10], Step [134/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [136/938], Loss: 0.0436\n",
      "Epoch [5/10], Step [138/938], Loss: 0.0062\n",
      "Epoch [5/10], Step [140/938], Loss: 0.0142\n",
      "Epoch [5/10], Step [142/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [144/938], Loss: 0.0098\n",
      "Epoch [5/10], Step [146/938], Loss: 0.0066\n",
      "Epoch [5/10], Step [148/938], Loss: 0.0076\n",
      "Epoch [5/10], Step [150/938], Loss: 0.0168\n",
      "Epoch [5/10], Step [152/938], Loss: 0.0076\n",
      "Epoch [5/10], Step [154/938], Loss: 0.0184\n",
      "Epoch [5/10], Step [156/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [158/938], Loss: 0.0099\n",
      "Epoch [5/10], Step [160/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [162/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [164/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [166/938], Loss: 0.0053\n",
      "Epoch [5/10], Step [168/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [170/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [172/938], Loss: 0.0109\n",
      "Epoch [5/10], Step [174/938], Loss: 0.0084\n",
      "Epoch [5/10], Step [176/938], Loss: 0.0631\n",
      "Epoch [5/10], Step [178/938], Loss: 0.0368\n",
      "Epoch [5/10], Step [180/938], Loss: 0.0041\n",
      "Epoch [5/10], Step [182/938], Loss: 0.0090\n",
      "Epoch [5/10], Step [184/938], Loss: 0.0107\n",
      "Epoch [5/10], Step [186/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [188/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [190/938], Loss: 0.0682\n",
      "Epoch [5/10], Step [192/938], Loss: 0.0496\n",
      "Epoch [5/10], Step [194/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [196/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [198/938], Loss: 0.0249\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0079\n",
      "Epoch [5/10], Step [202/938], Loss: 0.0089\n",
      "Epoch [5/10], Step [204/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [206/938], Loss: 0.0166\n",
      "Epoch [5/10], Step [208/938], Loss: 0.0741\n",
      "Epoch [5/10], Step [210/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [212/938], Loss: 0.0433\n",
      "Epoch [5/10], Step [214/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [216/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [218/938], Loss: 0.0126\n",
      "Epoch [5/10], Step [220/938], Loss: 0.0068\n",
      "Epoch [5/10], Step [222/938], Loss: 0.0072\n",
      "Epoch [5/10], Step [224/938], Loss: 0.0036\n",
      "Epoch [5/10], Step [226/938], Loss: 0.0070\n",
      "Epoch [5/10], Step [228/938], Loss: 0.0022\n",
      "Epoch [5/10], Step [230/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [232/938], Loss: 0.0241\n",
      "Epoch [5/10], Step [234/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [236/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [238/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [240/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [242/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [244/938], Loss: 0.0005\n",
      "Epoch [5/10], Step [246/938], Loss: 0.0110\n",
      "Epoch [5/10], Step [248/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [250/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [252/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [254/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [256/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [258/938], Loss: 0.0040\n",
      "Epoch [5/10], Step [260/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [262/938], Loss: 0.0561\n",
      "Epoch [5/10], Step [264/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [266/938], Loss: 0.0001\n",
      "Epoch [5/10], Step [268/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [270/938], Loss: 0.0066\n",
      "Epoch [5/10], Step [272/938], Loss: 0.0001\n",
      "Epoch [5/10], Step [274/938], Loss: 0.0065\n",
      "Epoch [5/10], Step [276/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [278/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [280/938], Loss: 0.0022\n",
      "Epoch [5/10], Step [282/938], Loss: 0.0077\n",
      "Epoch [5/10], Step [284/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [286/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [288/938], Loss: 0.0031\n",
      "Epoch [5/10], Step [290/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [292/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [294/938], Loss: 0.0119\n",
      "Epoch [5/10], Step [296/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [298/938], Loss: 0.0459\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0547\n",
      "Epoch [5/10], Step [302/938], Loss: 0.0481\n",
      "Epoch [5/10], Step [304/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [306/938], Loss: 0.0001\n",
      "Epoch [5/10], Step [308/938], Loss: 0.0950\n",
      "Epoch [5/10], Step [310/938], Loss: 0.0040\n",
      "Epoch [5/10], Step [312/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [314/938], Loss: 0.0072\n",
      "Epoch [5/10], Step [316/938], Loss: 0.0167\n",
      "Epoch [5/10], Step [318/938], Loss: 0.0052\n",
      "Epoch [5/10], Step [320/938], Loss: 0.0091\n",
      "Epoch [5/10], Step [322/938], Loss: 0.0031\n",
      "Epoch [5/10], Step [324/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [326/938], Loss: 0.0069\n",
      "Epoch [5/10], Step [328/938], Loss: 0.0165\n",
      "Epoch [5/10], Step [330/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [332/938], Loss: 0.0345\n",
      "Epoch [5/10], Step [334/938], Loss: 0.0334\n",
      "Epoch [5/10], Step [336/938], Loss: 0.0101\n",
      "Epoch [5/10], Step [338/938], Loss: 0.0106\n",
      "Epoch [5/10], Step [340/938], Loss: 0.0481\n",
      "Epoch [5/10], Step [342/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [344/938], Loss: 0.0202\n",
      "Epoch [5/10], Step [346/938], Loss: 0.0628\n",
      "Epoch [5/10], Step [348/938], Loss: 0.0356\n",
      "Epoch [5/10], Step [350/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [352/938], Loss: 0.0142\n",
      "Epoch [5/10], Step [354/938], Loss: 0.0053\n",
      "Epoch [5/10], Step [356/938], Loss: 0.0090\n",
      "Epoch [5/10], Step [358/938], Loss: 0.0864\n",
      "Epoch [5/10], Step [360/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [362/938], Loss: 0.0047\n",
      "Epoch [5/10], Step [364/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [366/938], Loss: 0.0054\n",
      "Epoch [5/10], Step [368/938], Loss: 0.0226\n",
      "Epoch [5/10], Step [370/938], Loss: 0.0351\n",
      "Epoch [5/10], Step [372/938], Loss: 0.0161\n",
      "Epoch [5/10], Step [374/938], Loss: 0.0082\n",
      "Epoch [5/10], Step [376/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [378/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [380/938], Loss: 0.0034\n",
      "Epoch [5/10], Step [382/938], Loss: 0.0234\n",
      "Epoch [5/10], Step [384/938], Loss: 0.0112\n",
      "Epoch [5/10], Step [386/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [388/938], Loss: 0.0470\n",
      "Epoch [5/10], Step [390/938], Loss: 0.0005\n",
      "Epoch [5/10], Step [392/938], Loss: 0.0101\n",
      "Epoch [5/10], Step [394/938], Loss: 0.0068\n",
      "Epoch [5/10], Step [396/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [398/938], Loss: 0.0743\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0229\n",
      "Epoch [5/10], Step [402/938], Loss: 0.0624\n",
      "Epoch [5/10], Step [404/938], Loss: 0.0387\n",
      "Epoch [5/10], Step [406/938], Loss: 0.0528\n",
      "Epoch [5/10], Step [408/938], Loss: 0.0211\n",
      "Epoch [5/10], Step [410/938], Loss: 0.0465\n",
      "Epoch [5/10], Step [412/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [414/938], Loss: 0.0022\n",
      "Epoch [5/10], Step [416/938], Loss: 0.0275\n",
      "Epoch [5/10], Step [418/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [420/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [422/938], Loss: 0.0185\n",
      "Epoch [5/10], Step [424/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [426/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [428/938], Loss: 0.0128\n",
      "Epoch [5/10], Step [430/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [432/938], Loss: 0.0302\n",
      "Epoch [5/10], Step [434/938], Loss: 0.0079\n",
      "Epoch [5/10], Step [436/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [438/938], Loss: 0.0158\n",
      "Epoch [5/10], Step [440/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [442/938], Loss: 0.0452\n",
      "Epoch [5/10], Step [444/938], Loss: 0.0192\n",
      "Epoch [5/10], Step [446/938], Loss: 0.1552\n",
      "Epoch [5/10], Step [448/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [450/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [452/938], Loss: 0.0035\n",
      "Epoch [5/10], Step [454/938], Loss: 0.0098\n",
      "Epoch [5/10], Step [456/938], Loss: 0.1251\n",
      "Epoch [5/10], Step [458/938], Loss: 0.0168\n",
      "Epoch [5/10], Step [460/938], Loss: 0.0049\n",
      "Epoch [5/10], Step [462/938], Loss: 0.0226\n",
      "Epoch [5/10], Step [464/938], Loss: 0.0446\n",
      "Epoch [5/10], Step [466/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [468/938], Loss: 0.0293\n",
      "Epoch [5/10], Step [470/938], Loss: 0.0214\n",
      "Epoch [5/10], Step [472/938], Loss: 0.0070\n",
      "Epoch [5/10], Step [474/938], Loss: 0.0104\n",
      "Epoch [5/10], Step [476/938], Loss: 0.0093\n",
      "Epoch [5/10], Step [478/938], Loss: 0.0622\n",
      "Epoch [5/10], Step [480/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [482/938], Loss: 0.0163\n",
      "Epoch [5/10], Step [484/938], Loss: 0.0587\n",
      "Epoch [5/10], Step [486/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [488/938], Loss: 0.0754\n",
      "Epoch [5/10], Step [490/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [492/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [494/938], Loss: 0.0054\n",
      "Epoch [5/10], Step [496/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [498/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [502/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [504/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [506/938], Loss: 0.0118\n",
      "Epoch [5/10], Step [508/938], Loss: 0.0054\n",
      "Epoch [5/10], Step [510/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [512/938], Loss: 0.0057\n",
      "Epoch [5/10], Step [514/938], Loss: 0.0268\n",
      "Epoch [5/10], Step [516/938], Loss: 0.0471\n",
      "Epoch [5/10], Step [518/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [520/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [522/938], Loss: 0.0035\n",
      "Epoch [5/10], Step [524/938], Loss: 0.0164\n",
      "Epoch [5/10], Step [526/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [528/938], Loss: 0.0040\n",
      "Epoch [5/10], Step [530/938], Loss: 0.0151\n",
      "Epoch [5/10], Step [532/938], Loss: 0.0259\n",
      "Epoch [5/10], Step [534/938], Loss: 0.0258\n",
      "Epoch [5/10], Step [536/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [538/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [540/938], Loss: 0.0537\n",
      "Epoch [5/10], Step [542/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [544/938], Loss: 0.0236\n",
      "Epoch [5/10], Step [546/938], Loss: 0.0175\n",
      "Epoch [5/10], Step [548/938], Loss: 0.0472\n",
      "Epoch [5/10], Step [550/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [552/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [554/938], Loss: 0.0052\n",
      "Epoch [5/10], Step [556/938], Loss: 0.0144\n",
      "Epoch [5/10], Step [558/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [560/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [562/938], Loss: 0.0043\n",
      "Epoch [5/10], Step [564/938], Loss: 0.0093\n",
      "Epoch [5/10], Step [566/938], Loss: 0.0190\n",
      "Epoch [5/10], Step [568/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [570/938], Loss: 0.0177\n",
      "Epoch [5/10], Step [572/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [574/938], Loss: 0.0068\n",
      "Epoch [5/10], Step [576/938], Loss: 0.0235\n",
      "Epoch [5/10], Step [578/938], Loss: 0.0322\n",
      "Epoch [5/10], Step [580/938], Loss: 0.0164\n",
      "Epoch [5/10], Step [582/938], Loss: 0.0079\n",
      "Epoch [5/10], Step [584/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [586/938], Loss: 0.0122\n",
      "Epoch [5/10], Step [588/938], Loss: 0.0161\n",
      "Epoch [5/10], Step [590/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [592/938], Loss: 0.0515\n",
      "Epoch [5/10], Step [594/938], Loss: 0.0197\n",
      "Epoch [5/10], Step [596/938], Loss: 0.0142\n",
      "Epoch [5/10], Step [598/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [602/938], Loss: 0.0055\n",
      "Epoch [5/10], Step [604/938], Loss: 0.0050\n",
      "Epoch [5/10], Step [606/938], Loss: 0.0147\n",
      "Epoch [5/10], Step [608/938], Loss: 0.0498\n",
      "Epoch [5/10], Step [610/938], Loss: 0.0126\n",
      "Epoch [5/10], Step [612/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [614/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [616/938], Loss: 0.0031\n",
      "Epoch [5/10], Step [618/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [620/938], Loss: 0.0022\n",
      "Epoch [5/10], Step [622/938], Loss: 0.1169\n",
      "Epoch [5/10], Step [624/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [626/938], Loss: 0.0810\n",
      "Epoch [5/10], Step [628/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [630/938], Loss: 0.0083\n",
      "Epoch [5/10], Step [632/938], Loss: 0.0310\n",
      "Epoch [5/10], Step [634/938], Loss: 0.0775\n",
      "Epoch [5/10], Step [636/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [638/938], Loss: 0.1028\n",
      "Epoch [5/10], Step [640/938], Loss: 0.0170\n",
      "Epoch [5/10], Step [642/938], Loss: 0.0071\n",
      "Epoch [5/10], Step [644/938], Loss: 0.0758\n",
      "Epoch [5/10], Step [646/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [648/938], Loss: 0.0039\n",
      "Epoch [5/10], Step [650/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [652/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [654/938], Loss: 0.0065\n",
      "Epoch [5/10], Step [656/938], Loss: 0.0154\n",
      "Epoch [5/10], Step [658/938], Loss: 0.0304\n",
      "Epoch [5/10], Step [660/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [662/938], Loss: 0.0110\n",
      "Epoch [5/10], Step [664/938], Loss: 0.0087\n",
      "Epoch [5/10], Step [666/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [668/938], Loss: 0.0627\n",
      "Epoch [5/10], Step [670/938], Loss: 0.0385\n",
      "Epoch [5/10], Step [672/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [674/938], Loss: 0.0259\n",
      "Epoch [5/10], Step [676/938], Loss: 0.0540\n",
      "Epoch [5/10], Step [678/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [680/938], Loss: 0.0405\n",
      "Epoch [5/10], Step [682/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [684/938], Loss: 0.0635\n",
      "Epoch [5/10], Step [686/938], Loss: 0.0040\n",
      "Epoch [5/10], Step [688/938], Loss: 0.0061\n",
      "Epoch [5/10], Step [690/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [692/938], Loss: 0.0496\n",
      "Epoch [5/10], Step [694/938], Loss: 0.0128\n",
      "Epoch [5/10], Step [696/938], Loss: 0.0124\n",
      "Epoch [5/10], Step [698/938], Loss: 0.1011\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0120\n",
      "Epoch [5/10], Step [702/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [704/938], Loss: 0.0074\n",
      "Epoch [5/10], Step [706/938], Loss: 0.0405\n",
      "Epoch [5/10], Step [708/938], Loss: 0.0345\n",
      "Epoch [5/10], Step [710/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [712/938], Loss: 0.0128\n",
      "Epoch [5/10], Step [714/938], Loss: 0.0061\n",
      "Epoch [5/10], Step [716/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [718/938], Loss: 0.0319\n",
      "Epoch [5/10], Step [720/938], Loss: 0.0384\n",
      "Epoch [5/10], Step [722/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [724/938], Loss: 0.0764\n",
      "Epoch [5/10], Step [726/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [728/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [730/938], Loss: 0.0566\n",
      "Epoch [5/10], Step [732/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [734/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [736/938], Loss: 0.0080\n",
      "Epoch [5/10], Step [738/938], Loss: 0.0521\n",
      "Epoch [5/10], Step [740/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [742/938], Loss: 0.0096\n",
      "Epoch [5/10], Step [744/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [746/938], Loss: 0.0068\n",
      "Epoch [5/10], Step [748/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [750/938], Loss: 0.0498\n",
      "Epoch [5/10], Step [752/938], Loss: 0.0111\n",
      "Epoch [5/10], Step [754/938], Loss: 0.0098\n",
      "Epoch [5/10], Step [756/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [758/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [760/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [762/938], Loss: 0.0182\n",
      "Epoch [5/10], Step [764/938], Loss: 0.0060\n",
      "Epoch [5/10], Step [766/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [768/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [770/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [772/938], Loss: 0.0530\n",
      "Epoch [5/10], Step [774/938], Loss: 0.0117\n",
      "Epoch [5/10], Step [776/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [778/938], Loss: 0.0049\n",
      "Epoch [5/10], Step [780/938], Loss: 0.0363\n",
      "Epoch [5/10], Step [782/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [784/938], Loss: 0.0060\n",
      "Epoch [5/10], Step [786/938], Loss: 0.0418\n",
      "Epoch [5/10], Step [788/938], Loss: 0.0391\n",
      "Epoch [5/10], Step [790/938], Loss: 0.0165\n",
      "Epoch [5/10], Step [792/938], Loss: 0.0088\n",
      "Epoch [5/10], Step [794/938], Loss: 0.0138\n",
      "Epoch [5/10], Step [796/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [798/938], Loss: 0.0141\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0436\n",
      "Epoch [5/10], Step [802/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [804/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [806/938], Loss: 0.0001\n",
      "Epoch [5/10], Step [808/938], Loss: 0.0163\n",
      "Epoch [5/10], Step [810/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [812/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [814/938], Loss: 0.0176\n",
      "Epoch [5/10], Step [816/938], Loss: 0.0037\n",
      "Epoch [5/10], Step [818/938], Loss: 0.0115\n",
      "Epoch [5/10], Step [820/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [822/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [824/938], Loss: 0.0043\n",
      "Epoch [5/10], Step [826/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [828/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [830/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [832/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [834/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [836/938], Loss: 0.0105\n",
      "Epoch [5/10], Step [838/938], Loss: 0.2059\n",
      "Epoch [5/10], Step [840/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [842/938], Loss: 0.1823\n",
      "Epoch [5/10], Step [844/938], Loss: 0.0048\n",
      "Epoch [5/10], Step [846/938], Loss: 0.0468\n",
      "Epoch [5/10], Step [848/938], Loss: 0.0034\n",
      "Epoch [5/10], Step [850/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [852/938], Loss: 0.0047\n",
      "Epoch [5/10], Step [854/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [856/938], Loss: 0.0555\n",
      "Epoch [5/10], Step [858/938], Loss: 0.1611\n",
      "Epoch [5/10], Step [860/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [862/938], Loss: 0.1028\n",
      "Epoch [5/10], Step [864/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [866/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [868/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [870/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [872/938], Loss: 0.0965\n",
      "Epoch [5/10], Step [874/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [876/938], Loss: 0.0089\n",
      "Epoch [5/10], Step [878/938], Loss: 0.0032\n",
      "Epoch [5/10], Step [880/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [882/938], Loss: 0.0069\n",
      "Epoch [5/10], Step [884/938], Loss: 0.0040\n",
      "Epoch [5/10], Step [886/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [888/938], Loss: 0.0126\n",
      "Epoch [5/10], Step [890/938], Loss: 0.0568\n",
      "Epoch [5/10], Step [892/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [894/938], Loss: 0.0573\n",
      "Epoch [5/10], Step [896/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [898/938], Loss: 0.0083\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0054\n",
      "Epoch [5/10], Step [902/938], Loss: 0.0331\n",
      "Epoch [5/10], Step [904/938], Loss: 0.0603\n",
      "Epoch [5/10], Step [906/938], Loss: 0.0119\n",
      "Epoch [5/10], Step [908/938], Loss: 0.0078\n",
      "Epoch [5/10], Step [910/938], Loss: 0.0068\n",
      "Epoch [5/10], Step [912/938], Loss: 0.0110\n",
      "Epoch [5/10], Step [914/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [916/938], Loss: 0.0063\n",
      "Epoch [5/10], Step [918/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [920/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [922/938], Loss: 0.0419\n",
      "Epoch [5/10], Step [924/938], Loss: 0.0032\n",
      "Epoch [5/10], Step [926/938], Loss: 0.0125\n",
      "Epoch [5/10], Step [928/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [930/938], Loss: 0.0035\n",
      "Epoch [5/10], Step [932/938], Loss: 0.0085\n",
      "Epoch [5/10], Step [934/938], Loss: 0.0124\n",
      "Epoch [5/10], Step [936/938], Loss: 0.0050\n",
      "Epoch [5/10], Step [938/938], Loss: 0.0011\n",
      "Epoch [5/10], Loss: 0.0165\n",
      "Epoch [6/10], Step [2/938], Loss: 0.0186\n",
      "Epoch [6/10], Step [4/938], Loss: 0.0140\n",
      "Epoch [6/10], Step [6/938], Loss: 0.0149\n",
      "Epoch [6/10], Step [8/938], Loss: 0.0115\n",
      "Epoch [6/10], Step [10/938], Loss: 0.0044\n",
      "Epoch [6/10], Step [12/938], Loss: 0.0055\n",
      "Epoch [6/10], Step [14/938], Loss: 0.0081\n",
      "Epoch [6/10], Step [16/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [18/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [20/938], Loss: 0.0117\n",
      "Epoch [6/10], Step [22/938], Loss: 0.0426\n",
      "Epoch [6/10], Step [24/938], Loss: 0.0396\n",
      "Epoch [6/10], Step [26/938], Loss: 0.0095\n",
      "Epoch [6/10], Step [28/938], Loss: 0.0374\n",
      "Epoch [6/10], Step [30/938], Loss: 0.0074\n",
      "Epoch [6/10], Step [32/938], Loss: 0.0294\n",
      "Epoch [6/10], Step [34/938], Loss: 0.0207\n",
      "Epoch [6/10], Step [36/938], Loss: 0.0521\n",
      "Epoch [6/10], Step [38/938], Loss: 0.0572\n",
      "Epoch [6/10], Step [40/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [42/938], Loss: 0.0327\n",
      "Epoch [6/10], Step [44/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [46/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [48/938], Loss: 0.0147\n",
      "Epoch [6/10], Step [50/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [52/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [54/938], Loss: 0.0028\n",
      "Epoch [6/10], Step [56/938], Loss: 0.0031\n",
      "Epoch [6/10], Step [58/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [60/938], Loss: 0.0152\n",
      "Epoch [6/10], Step [62/938], Loss: 0.0138\n",
      "Epoch [6/10], Step [64/938], Loss: 0.0286\n",
      "Epoch [6/10], Step [66/938], Loss: 0.0247\n",
      "Epoch [6/10], Step [68/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [70/938], Loss: 0.0071\n",
      "Epoch [6/10], Step [72/938], Loss: 0.0367\n",
      "Epoch [6/10], Step [74/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [76/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [78/938], Loss: 0.0207\n",
      "Epoch [6/10], Step [80/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [82/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [84/938], Loss: 0.1296\n",
      "Epoch [6/10], Step [86/938], Loss: 0.0175\n",
      "Epoch [6/10], Step [88/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [90/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [92/938], Loss: 0.0677\n",
      "Epoch [6/10], Step [94/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [96/938], Loss: 0.0404\n",
      "Epoch [6/10], Step [98/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0108\n",
      "Epoch [6/10], Step [102/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [104/938], Loss: 0.0242\n",
      "Epoch [6/10], Step [106/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [108/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [110/938], Loss: 0.0022\n",
      "Epoch [6/10], Step [112/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [114/938], Loss: 0.0132\n",
      "Epoch [6/10], Step [116/938], Loss: 0.0048\n",
      "Epoch [6/10], Step [118/938], Loss: 0.0075\n",
      "Epoch [6/10], Step [120/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [122/938], Loss: 0.0150\n",
      "Epoch [6/10], Step [124/938], Loss: 0.0081\n",
      "Epoch [6/10], Step [126/938], Loss: 0.0035\n",
      "Epoch [6/10], Step [128/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [130/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [132/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [134/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [136/938], Loss: 0.1480\n",
      "Epoch [6/10], Step [138/938], Loss: 0.0157\n",
      "Epoch [6/10], Step [140/938], Loss: 0.0386\n",
      "Epoch [6/10], Step [142/938], Loss: 0.0309\n",
      "Epoch [6/10], Step [144/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [146/938], Loss: 0.0041\n",
      "Epoch [6/10], Step [148/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [150/938], Loss: 0.0051\n",
      "Epoch [6/10], Step [152/938], Loss: 0.0327\n",
      "Epoch [6/10], Step [154/938], Loss: 0.0176\n",
      "Epoch [6/10], Step [156/938], Loss: 0.0223\n",
      "Epoch [6/10], Step [158/938], Loss: 0.0205\n",
      "Epoch [6/10], Step [160/938], Loss: 0.0082\n",
      "Epoch [6/10], Step [162/938], Loss: 0.0076\n",
      "Epoch [6/10], Step [164/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [166/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [168/938], Loss: 0.0097\n",
      "Epoch [6/10], Step [170/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [172/938], Loss: 0.0120\n",
      "Epoch [6/10], Step [174/938], Loss: 0.0590\n",
      "Epoch [6/10], Step [176/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [178/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [180/938], Loss: 0.0148\n",
      "Epoch [6/10], Step [182/938], Loss: 0.0058\n",
      "Epoch [6/10], Step [184/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [186/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [188/938], Loss: 0.0152\n",
      "Epoch [6/10], Step [190/938], Loss: 0.0146\n",
      "Epoch [6/10], Step [192/938], Loss: 0.0177\n",
      "Epoch [6/10], Step [194/938], Loss: 0.0059\n",
      "Epoch [6/10], Step [196/938], Loss: 0.0131\n",
      "Epoch [6/10], Step [198/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0120\n",
      "Epoch [6/10], Step [202/938], Loss: 0.0205\n",
      "Epoch [6/10], Step [204/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [206/938], Loss: 0.0257\n",
      "Epoch [6/10], Step [208/938], Loss: 0.0324\n",
      "Epoch [6/10], Step [210/938], Loss: 0.0060\n",
      "Epoch [6/10], Step [212/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [214/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [216/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [218/938], Loss: 0.0067\n",
      "Epoch [6/10], Step [220/938], Loss: 0.0088\n",
      "Epoch [6/10], Step [222/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [224/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [226/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [228/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [230/938], Loss: 0.0127\n",
      "Epoch [6/10], Step [232/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [234/938], Loss: 0.0052\n",
      "Epoch [6/10], Step [236/938], Loss: 0.0117\n",
      "Epoch [6/10], Step [238/938], Loss: 0.0206\n",
      "Epoch [6/10], Step [240/938], Loss: 0.0101\n",
      "Epoch [6/10], Step [242/938], Loss: 0.0109\n",
      "Epoch [6/10], Step [244/938], Loss: 0.0431\n",
      "Epoch [6/10], Step [246/938], Loss: 0.0094\n",
      "Epoch [6/10], Step [248/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [250/938], Loss: 0.0136\n",
      "Epoch [6/10], Step [252/938], Loss: 0.0113\n",
      "Epoch [6/10], Step [254/938], Loss: 0.1394\n",
      "Epoch [6/10], Step [256/938], Loss: 0.0477\n",
      "Epoch [6/10], Step [258/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [260/938], Loss: 0.0164\n",
      "Epoch [6/10], Step [262/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [264/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [266/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [268/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [270/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [272/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [274/938], Loss: 0.0330\n",
      "Epoch [6/10], Step [276/938], Loss: 0.0022\n",
      "Epoch [6/10], Step [278/938], Loss: 0.0047\n",
      "Epoch [6/10], Step [280/938], Loss: 0.0110\n",
      "Epoch [6/10], Step [282/938], Loss: 0.0048\n",
      "Epoch [6/10], Step [284/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [286/938], Loss: 0.0058\n",
      "Epoch [6/10], Step [288/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [290/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [292/938], Loss: 0.0051\n",
      "Epoch [6/10], Step [294/938], Loss: 0.0028\n",
      "Epoch [6/10], Step [296/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [298/938], Loss: 0.0023\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [302/938], Loss: 0.0034\n",
      "Epoch [6/10], Step [304/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [306/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [308/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [310/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [312/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [314/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [316/938], Loss: 0.0054\n",
      "Epoch [6/10], Step [318/938], Loss: 0.0475\n",
      "Epoch [6/10], Step [320/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [322/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [324/938], Loss: 0.0221\n",
      "Epoch [6/10], Step [326/938], Loss: 0.0086\n",
      "Epoch [6/10], Step [328/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [330/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [332/938], Loss: 0.0103\n",
      "Epoch [6/10], Step [334/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [336/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [338/938], Loss: 0.0168\n",
      "Epoch [6/10], Step [340/938], Loss: 0.0176\n",
      "Epoch [6/10], Step [342/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [344/938], Loss: 0.0097\n",
      "Epoch [6/10], Step [346/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [348/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [350/938], Loss: 0.0034\n",
      "Epoch [6/10], Step [352/938], Loss: 0.0100\n",
      "Epoch [6/10], Step [354/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [356/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [358/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [360/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [362/938], Loss: 0.0157\n",
      "Epoch [6/10], Step [364/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [366/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [368/938], Loss: 0.0066\n",
      "Epoch [6/10], Step [370/938], Loss: 0.0183\n",
      "Epoch [6/10], Step [372/938], Loss: 0.0305\n",
      "Epoch [6/10], Step [374/938], Loss: 0.0329\n",
      "Epoch [6/10], Step [376/938], Loss: 0.0039\n",
      "Epoch [6/10], Step [378/938], Loss: 0.0531\n",
      "Epoch [6/10], Step [380/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [382/938], Loss: 0.0035\n",
      "Epoch [6/10], Step [384/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [386/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [388/938], Loss: 0.0359\n",
      "Epoch [6/10], Step [390/938], Loss: 0.0972\n",
      "Epoch [6/10], Step [392/938], Loss: 0.0023\n",
      "Epoch [6/10], Step [394/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [396/938], Loss: 0.0124\n",
      "Epoch [6/10], Step [398/938], Loss: 0.0280\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0150\n",
      "Epoch [6/10], Step [402/938], Loss: 0.0478\n",
      "Epoch [6/10], Step [404/938], Loss: 0.0036\n",
      "Epoch [6/10], Step [406/938], Loss: 0.0104\n",
      "Epoch [6/10], Step [408/938], Loss: 0.0308\n",
      "Epoch [6/10], Step [410/938], Loss: 0.0220\n",
      "Epoch [6/10], Step [412/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [414/938], Loss: 0.0370\n",
      "Epoch [6/10], Step [416/938], Loss: 0.0144\n",
      "Epoch [6/10], Step [418/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [420/938], Loss: 0.0644\n",
      "Epoch [6/10], Step [422/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [424/938], Loss: 0.0036\n",
      "Epoch [6/10], Step [426/938], Loss: 0.0073\n",
      "Epoch [6/10], Step [428/938], Loss: 0.0220\n",
      "Epoch [6/10], Step [430/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [432/938], Loss: 0.0027\n",
      "Epoch [6/10], Step [434/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [436/938], Loss: 0.0093\n",
      "Epoch [6/10], Step [438/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [440/938], Loss: 0.0085\n",
      "Epoch [6/10], Step [442/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [444/938], Loss: 0.0226\n",
      "Epoch [6/10], Step [446/938], Loss: 0.0155\n",
      "Epoch [6/10], Step [448/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [450/938], Loss: 0.0227\n",
      "Epoch [6/10], Step [452/938], Loss: 0.0100\n",
      "Epoch [6/10], Step [454/938], Loss: 0.0242\n",
      "Epoch [6/10], Step [456/938], Loss: 0.0323\n",
      "Epoch [6/10], Step [458/938], Loss: 0.0035\n",
      "Epoch [6/10], Step [460/938], Loss: 0.0038\n",
      "Epoch [6/10], Step [462/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [464/938], Loss: 0.0142\n",
      "Epoch [6/10], Step [466/938], Loss: 0.0231\n",
      "Epoch [6/10], Step [468/938], Loss: 0.0049\n",
      "Epoch [6/10], Step [470/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [472/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [474/938], Loss: 0.0396\n",
      "Epoch [6/10], Step [476/938], Loss: 0.0455\n",
      "Epoch [6/10], Step [478/938], Loss: 0.1895\n",
      "Epoch [6/10], Step [480/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [482/938], Loss: 0.0248\n",
      "Epoch [6/10], Step [484/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [486/938], Loss: 0.0601\n",
      "Epoch [6/10], Step [488/938], Loss: 0.0108\n",
      "Epoch [6/10], Step [490/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [492/938], Loss: 0.0054\n",
      "Epoch [6/10], Step [494/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [496/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [498/938], Loss: 0.0078\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0060\n",
      "Epoch [6/10], Step [502/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [504/938], Loss: 0.0720\n",
      "Epoch [6/10], Step [506/938], Loss: 0.0060\n",
      "Epoch [6/10], Step [508/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [510/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [512/938], Loss: 0.0552\n",
      "Epoch [6/10], Step [514/938], Loss: 0.0031\n",
      "Epoch [6/10], Step [516/938], Loss: 0.0043\n",
      "Epoch [6/10], Step [518/938], Loss: 0.0266\n",
      "Epoch [6/10], Step [520/938], Loss: 0.0028\n",
      "Epoch [6/10], Step [522/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [524/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [526/938], Loss: 0.0033\n",
      "Epoch [6/10], Step [528/938], Loss: 0.0356\n",
      "Epoch [6/10], Step [530/938], Loss: 0.0567\n",
      "Epoch [6/10], Step [532/938], Loss: 0.0077\n",
      "Epoch [6/10], Step [534/938], Loss: 0.0246\n",
      "Epoch [6/10], Step [536/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [538/938], Loss: 0.0252\n",
      "Epoch [6/10], Step [540/938], Loss: 0.0216\n",
      "Epoch [6/10], Step [542/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [544/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [546/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [548/938], Loss: 0.0138\n",
      "Epoch [6/10], Step [550/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [552/938], Loss: 0.0035\n",
      "Epoch [6/10], Step [554/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [556/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [558/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [560/938], Loss: 0.0280\n",
      "Epoch [6/10], Step [562/938], Loss: 0.0241\n",
      "Epoch [6/10], Step [564/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [566/938], Loss: 0.0020\n",
      "Epoch [6/10], Step [568/938], Loss: 0.0059\n",
      "Epoch [6/10], Step [570/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [572/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [574/938], Loss: 0.0063\n",
      "Epoch [6/10], Step [576/938], Loss: 0.0526\n",
      "Epoch [6/10], Step [578/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [580/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [582/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [584/938], Loss: 0.0077\n",
      "Epoch [6/10], Step [586/938], Loss: 0.0373\n",
      "Epoch [6/10], Step [588/938], Loss: 0.0044\n",
      "Epoch [6/10], Step [590/938], Loss: 0.0397\n",
      "Epoch [6/10], Step [592/938], Loss: 0.0234\n",
      "Epoch [6/10], Step [594/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [596/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [598/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0553\n",
      "Epoch [6/10], Step [602/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [604/938], Loss: 0.0606\n",
      "Epoch [6/10], Step [606/938], Loss: 0.0241\n",
      "Epoch [6/10], Step [608/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [610/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [612/938], Loss: 0.0117\n",
      "Epoch [6/10], Step [614/938], Loss: 0.0213\n",
      "Epoch [6/10], Step [616/938], Loss: 0.0078\n",
      "Epoch [6/10], Step [618/938], Loss: 0.1053\n",
      "Epoch [6/10], Step [620/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [622/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [624/938], Loss: 0.0833\n",
      "Epoch [6/10], Step [626/938], Loss: 0.0131\n",
      "Epoch [6/10], Step [628/938], Loss: 0.0059\n",
      "Epoch [6/10], Step [630/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [632/938], Loss: 0.0064\n",
      "Epoch [6/10], Step [634/938], Loss: 0.0196\n",
      "Epoch [6/10], Step [636/938], Loss: 0.0354\n",
      "Epoch [6/10], Step [638/938], Loss: 0.0088\n",
      "Epoch [6/10], Step [640/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [642/938], Loss: 0.0075\n",
      "Epoch [6/10], Step [644/938], Loss: 0.0120\n",
      "Epoch [6/10], Step [646/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [648/938], Loss: 0.0744\n",
      "Epoch [6/10], Step [650/938], Loss: 0.0023\n",
      "Epoch [6/10], Step [652/938], Loss: 0.0117\n",
      "Epoch [6/10], Step [654/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [656/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [658/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [660/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [662/938], Loss: 0.0453\n",
      "Epoch [6/10], Step [664/938], Loss: 0.0055\n",
      "Epoch [6/10], Step [666/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [668/938], Loss: 0.0349\n",
      "Epoch [6/10], Step [670/938], Loss: 0.0063\n",
      "Epoch [6/10], Step [672/938], Loss: 0.1451\n",
      "Epoch [6/10], Step [674/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [676/938], Loss: 0.0043\n",
      "Epoch [6/10], Step [678/938], Loss: 0.0419\n",
      "Epoch [6/10], Step [680/938], Loss: 0.0067\n",
      "Epoch [6/10], Step [682/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [684/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [686/938], Loss: 0.0155\n",
      "Epoch [6/10], Step [688/938], Loss: 0.0089\n",
      "Epoch [6/10], Step [690/938], Loss: 0.0086\n",
      "Epoch [6/10], Step [692/938], Loss: 0.0053\n",
      "Epoch [6/10], Step [694/938], Loss: 0.0053\n",
      "Epoch [6/10], Step [696/938], Loss: 0.0144\n",
      "Epoch [6/10], Step [698/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [700/938], Loss: 0.1003\n",
      "Epoch [6/10], Step [702/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [704/938], Loss: 0.0030\n",
      "Epoch [6/10], Step [706/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [708/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [710/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [712/938], Loss: 0.0274\n",
      "Epoch [6/10], Step [714/938], Loss: 0.0121\n",
      "Epoch [6/10], Step [716/938], Loss: 0.0065\n",
      "Epoch [6/10], Step [718/938], Loss: 0.0303\n",
      "Epoch [6/10], Step [720/938], Loss: 0.0033\n",
      "Epoch [6/10], Step [722/938], Loss: 0.0150\n",
      "Epoch [6/10], Step [724/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [726/938], Loss: 0.0326\n",
      "Epoch [6/10], Step [728/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [730/938], Loss: 0.0360\n",
      "Epoch [6/10], Step [732/938], Loss: 0.0588\n",
      "Epoch [6/10], Step [734/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [736/938], Loss: 0.0306\n",
      "Epoch [6/10], Step [738/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [740/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [742/938], Loss: 0.0047\n",
      "Epoch [6/10], Step [744/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [746/938], Loss: 0.0140\n",
      "Epoch [6/10], Step [748/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [750/938], Loss: 0.0352\n",
      "Epoch [6/10], Step [752/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [754/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [756/938], Loss: 0.0139\n",
      "Epoch [6/10], Step [758/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [760/938], Loss: 0.0126\n",
      "Epoch [6/10], Step [762/938], Loss: 0.0110\n",
      "Epoch [6/10], Step [764/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [766/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [768/938], Loss: 0.0054\n",
      "Epoch [6/10], Step [770/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [772/938], Loss: 0.0079\n",
      "Epoch [6/10], Step [774/938], Loss: 0.0318\n",
      "Epoch [6/10], Step [776/938], Loss: 0.0228\n",
      "Epoch [6/10], Step [778/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [780/938], Loss: 0.0299\n",
      "Epoch [6/10], Step [782/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [784/938], Loss: 0.0585\n",
      "Epoch [6/10], Step [786/938], Loss: 0.0688\n",
      "Epoch [6/10], Step [788/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [790/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [792/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [794/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [796/938], Loss: 0.0047\n",
      "Epoch [6/10], Step [798/938], Loss: 0.0268\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [802/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [804/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [806/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [808/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [810/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [812/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [814/938], Loss: 0.0380\n",
      "Epoch [6/10], Step [816/938], Loss: 0.0167\n",
      "Epoch [6/10], Step [818/938], Loss: 0.0054\n",
      "Epoch [6/10], Step [820/938], Loss: 0.0490\n",
      "Epoch [6/10], Step [822/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [824/938], Loss: 0.0210\n",
      "Epoch [6/10], Step [826/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [828/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [830/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [832/938], Loss: 0.0041\n",
      "Epoch [6/10], Step [834/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [836/938], Loss: 0.0475\n",
      "Epoch [6/10], Step [838/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [840/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [842/938], Loss: 0.0022\n",
      "Epoch [6/10], Step [844/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [846/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [848/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [850/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [852/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [854/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [856/938], Loss: 0.0203\n",
      "Epoch [6/10], Step [858/938], Loss: 0.0201\n",
      "Epoch [6/10], Step [860/938], Loss: 0.0064\n",
      "Epoch [6/10], Step [862/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [864/938], Loss: 0.0214\n",
      "Epoch [6/10], Step [866/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [868/938], Loss: 0.0315\n",
      "Epoch [6/10], Step [870/938], Loss: 0.0071\n",
      "Epoch [6/10], Step [872/938], Loss: 0.0824\n",
      "Epoch [6/10], Step [874/938], Loss: 0.0177\n",
      "Epoch [6/10], Step [876/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [878/938], Loss: 0.0131\n",
      "Epoch [6/10], Step [880/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [882/938], Loss: 0.0070\n",
      "Epoch [6/10], Step [884/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [886/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [888/938], Loss: 0.0311\n",
      "Epoch [6/10], Step [890/938], Loss: 0.0692\n",
      "Epoch [6/10], Step [892/938], Loss: 0.0062\n",
      "Epoch [6/10], Step [894/938], Loss: 0.0212\n",
      "Epoch [6/10], Step [896/938], Loss: 0.0163\n",
      "Epoch [6/10], Step [898/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0139\n",
      "Epoch [6/10], Step [902/938], Loss: 0.0473\n",
      "Epoch [6/10], Step [904/938], Loss: 0.0052\n",
      "Epoch [6/10], Step [906/938], Loss: 0.1385\n",
      "Epoch [6/10], Step [908/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [910/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [912/938], Loss: 0.0070\n",
      "Epoch [6/10], Step [914/938], Loss: 0.0906\n",
      "Epoch [6/10], Step [916/938], Loss: 0.0235\n",
      "Epoch [6/10], Step [918/938], Loss: 0.0068\n",
      "Epoch [6/10], Step [920/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [922/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [924/938], Loss: 0.0142\n",
      "Epoch [6/10], Step [926/938], Loss: 0.0260\n",
      "Epoch [6/10], Step [928/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [930/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [932/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [934/938], Loss: 0.0309\n",
      "Epoch [6/10], Step [936/938], Loss: 0.0877\n",
      "Epoch [6/10], Step [938/938], Loss: 0.0308\n",
      "Epoch [6/10], Loss: 0.0145\n",
      "Epoch [7/10], Step [2/938], Loss: 0.0041\n",
      "Epoch [7/10], Step [4/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [6/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [8/938], Loss: 0.0609\n",
      "Epoch [7/10], Step [10/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [12/938], Loss: 0.0473\n",
      "Epoch [7/10], Step [14/938], Loss: 0.0244\n",
      "Epoch [7/10], Step [16/938], Loss: 0.0050\n",
      "Epoch [7/10], Step [18/938], Loss: 0.0332\n",
      "Epoch [7/10], Step [20/938], Loss: 0.0093\n",
      "Epoch [7/10], Step [22/938], Loss: 0.0082\n",
      "Epoch [7/10], Step [24/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [26/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [28/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [30/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [32/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [34/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [36/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [38/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [40/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [42/938], Loss: 0.0254\n",
      "Epoch [7/10], Step [44/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [46/938], Loss: 0.0204\n",
      "Epoch [7/10], Step [48/938], Loss: 0.0114\n",
      "Epoch [7/10], Step [50/938], Loss: 0.0852\n",
      "Epoch [7/10], Step [52/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [54/938], Loss: 0.0337\n",
      "Epoch [7/10], Step [56/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [58/938], Loss: 0.0065\n",
      "Epoch [7/10], Step [60/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [62/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [64/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [66/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [68/938], Loss: 0.0143\n",
      "Epoch [7/10], Step [70/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [72/938], Loss: 0.0060\n",
      "Epoch [7/10], Step [74/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [76/938], Loss: 0.0202\n",
      "Epoch [7/10], Step [78/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [80/938], Loss: 0.0093\n",
      "Epoch [7/10], Step [82/938], Loss: 0.0649\n",
      "Epoch [7/10], Step [84/938], Loss: 0.0251\n",
      "Epoch [7/10], Step [86/938], Loss: 0.0596\n",
      "Epoch [7/10], Step [88/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [90/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [92/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [94/938], Loss: 0.0031\n",
      "Epoch [7/10], Step [96/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [98/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0088\n",
      "Epoch [7/10], Step [102/938], Loss: 0.0367\n",
      "Epoch [7/10], Step [104/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [106/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [108/938], Loss: 0.0024\n",
      "Epoch [7/10], Step [110/938], Loss: 0.0509\n",
      "Epoch [7/10], Step [112/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [114/938], Loss: 0.0116\n",
      "Epoch [7/10], Step [116/938], Loss: 0.0400\n",
      "Epoch [7/10], Step [118/938], Loss: 0.0051\n",
      "Epoch [7/10], Step [120/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [122/938], Loss: 0.0052\n",
      "Epoch [7/10], Step [124/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [126/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [128/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [130/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [132/938], Loss: 0.0390\n",
      "Epoch [7/10], Step [134/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [136/938], Loss: 0.0184\n",
      "Epoch [7/10], Step [138/938], Loss: 0.0038\n",
      "Epoch [7/10], Step [140/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [142/938], Loss: 0.1699\n",
      "Epoch [7/10], Step [144/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [146/938], Loss: 0.0205\n",
      "Epoch [7/10], Step [148/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [150/938], Loss: 0.0063\n",
      "Epoch [7/10], Step [152/938], Loss: 0.0268\n",
      "Epoch [7/10], Step [154/938], Loss: 0.0024\n",
      "Epoch [7/10], Step [156/938], Loss: 0.0040\n",
      "Epoch [7/10], Step [158/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [160/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [162/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [164/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [166/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [168/938], Loss: 0.0664\n",
      "Epoch [7/10], Step [170/938], Loss: 0.0094\n",
      "Epoch [7/10], Step [172/938], Loss: 0.0099\n",
      "Epoch [7/10], Step [174/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [176/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [178/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [180/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [182/938], Loss: 0.0092\n",
      "Epoch [7/10], Step [184/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [186/938], Loss: 0.0318\n",
      "Epoch [7/10], Step [188/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [190/938], Loss: 0.0400\n",
      "Epoch [7/10], Step [192/938], Loss: 0.0081\n",
      "Epoch [7/10], Step [194/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [196/938], Loss: 0.0630\n",
      "Epoch [7/10], Step [198/938], Loss: 0.0172\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [202/938], Loss: 0.0029\n",
      "Epoch [7/10], Step [204/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [206/938], Loss: 0.0092\n",
      "Epoch [7/10], Step [208/938], Loss: 0.0096\n",
      "Epoch [7/10], Step [210/938], Loss: 0.0048\n",
      "Epoch [7/10], Step [212/938], Loss: 0.0027\n",
      "Epoch [7/10], Step [214/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [216/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [218/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [220/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [222/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [224/938], Loss: 0.0048\n",
      "Epoch [7/10], Step [226/938], Loss: 0.0074\n",
      "Epoch [7/10], Step [228/938], Loss: 0.0072\n",
      "Epoch [7/10], Step [230/938], Loss: 0.0030\n",
      "Epoch [7/10], Step [232/938], Loss: 0.0065\n",
      "Epoch [7/10], Step [234/938], Loss: 0.0145\n",
      "Epoch [7/10], Step [236/938], Loss: 0.0100\n",
      "Epoch [7/10], Step [238/938], Loss: 0.0036\n",
      "Epoch [7/10], Step [240/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [242/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [244/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [246/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [248/938], Loss: 0.0581\n",
      "Epoch [7/10], Step [250/938], Loss: 0.0024\n",
      "Epoch [7/10], Step [252/938], Loss: 0.0388\n",
      "Epoch [7/10], Step [254/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [256/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [258/938], Loss: 0.0403\n",
      "Epoch [7/10], Step [260/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [262/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [264/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [266/938], Loss: 0.0058\n",
      "Epoch [7/10], Step [268/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [270/938], Loss: 0.0099\n",
      "Epoch [7/10], Step [272/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [274/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [276/938], Loss: 0.0187\n",
      "Epoch [7/10], Step [278/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [280/938], Loss: 0.0228\n",
      "Epoch [7/10], Step [282/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [284/938], Loss: 0.0345\n",
      "Epoch [7/10], Step [286/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [288/938], Loss: 0.0027\n",
      "Epoch [7/10], Step [290/938], Loss: 0.0097\n",
      "Epoch [7/10], Step [292/938], Loss: 0.0081\n",
      "Epoch [7/10], Step [294/938], Loss: 0.0170\n",
      "Epoch [7/10], Step [296/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [298/938], Loss: 0.0072\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [302/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [304/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [306/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [308/938], Loss: 0.0120\n",
      "Epoch [7/10], Step [310/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [312/938], Loss: 0.0615\n",
      "Epoch [7/10], Step [314/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [316/938], Loss: 0.0067\n",
      "Epoch [7/10], Step [318/938], Loss: 0.0111\n",
      "Epoch [7/10], Step [320/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [322/938], Loss: 0.0419\n",
      "Epoch [7/10], Step [324/938], Loss: 0.0102\n",
      "Epoch [7/10], Step [326/938], Loss: 0.0089\n",
      "Epoch [7/10], Step [328/938], Loss: 0.0379\n",
      "Epoch [7/10], Step [330/938], Loss: 0.0052\n",
      "Epoch [7/10], Step [332/938], Loss: 0.0104\n",
      "Epoch [7/10], Step [334/938], Loss: 0.0121\n",
      "Epoch [7/10], Step [336/938], Loss: 0.0046\n",
      "Epoch [7/10], Step [338/938], Loss: 0.0067\n",
      "Epoch [7/10], Step [340/938], Loss: 0.0708\n",
      "Epoch [7/10], Step [342/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [344/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [346/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [348/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [350/938], Loss: 0.0226\n",
      "Epoch [7/10], Step [352/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [354/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [356/938], Loss: 0.0348\n",
      "Epoch [7/10], Step [358/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [360/938], Loss: 0.0582\n",
      "Epoch [7/10], Step [362/938], Loss: 0.0026\n",
      "Epoch [7/10], Step [364/938], Loss: 0.0185\n",
      "Epoch [7/10], Step [366/938], Loss: 0.0200\n",
      "Epoch [7/10], Step [368/938], Loss: 0.0466\n",
      "Epoch [7/10], Step [370/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [372/938], Loss: 0.0049\n",
      "Epoch [7/10], Step [374/938], Loss: 0.0061\n",
      "Epoch [7/10], Step [376/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [378/938], Loss: 0.0039\n",
      "Epoch [7/10], Step [380/938], Loss: 0.0133\n",
      "Epoch [7/10], Step [382/938], Loss: 0.0039\n",
      "Epoch [7/10], Step [384/938], Loss: 0.0154\n",
      "Epoch [7/10], Step [386/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [388/938], Loss: 0.0041\n",
      "Epoch [7/10], Step [390/938], Loss: 0.0058\n",
      "Epoch [7/10], Step [392/938], Loss: 0.0117\n",
      "Epoch [7/10], Step [394/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [396/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [398/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0033\n",
      "Epoch [7/10], Step [402/938], Loss: 0.0119\n",
      "Epoch [7/10], Step [404/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [406/938], Loss: 0.0211\n",
      "Epoch [7/10], Step [408/938], Loss: 0.0241\n",
      "Epoch [7/10], Step [410/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [412/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [414/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [416/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [418/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [420/938], Loss: 0.0084\n",
      "Epoch [7/10], Step [422/938], Loss: 0.0061\n",
      "Epoch [7/10], Step [424/938], Loss: 0.0466\n",
      "Epoch [7/10], Step [426/938], Loss: 0.0354\n",
      "Epoch [7/10], Step [428/938], Loss: 0.0027\n",
      "Epoch [7/10], Step [430/938], Loss: 0.0278\n",
      "Epoch [7/10], Step [432/938], Loss: 0.0037\n",
      "Epoch [7/10], Step [434/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [436/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [438/938], Loss: 0.0048\n",
      "Epoch [7/10], Step [440/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [442/938], Loss: 0.0343\n",
      "Epoch [7/10], Step [444/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [446/938], Loss: 0.0203\n",
      "Epoch [7/10], Step [448/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [450/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [452/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [454/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [456/938], Loss: 0.0276\n",
      "Epoch [7/10], Step [458/938], Loss: 0.0447\n",
      "Epoch [7/10], Step [460/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [462/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [464/938], Loss: 0.0502\n",
      "Epoch [7/10], Step [466/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [468/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [470/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [472/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [474/938], Loss: 0.0427\n",
      "Epoch [7/10], Step [476/938], Loss: 0.0586\n",
      "Epoch [7/10], Step [478/938], Loss: 0.0289\n",
      "Epoch [7/10], Step [480/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [482/938], Loss: 0.0197\n",
      "Epoch [7/10], Step [484/938], Loss: 0.0036\n",
      "Epoch [7/10], Step [486/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [488/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [490/938], Loss: 0.0041\n",
      "Epoch [7/10], Step [492/938], Loss: 0.0462\n",
      "Epoch [7/10], Step [494/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [496/938], Loss: 0.0117\n",
      "Epoch [7/10], Step [498/938], Loss: 0.0058\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [502/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [504/938], Loss: 0.0027\n",
      "Epoch [7/10], Step [506/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [508/938], Loss: 0.0425\n",
      "Epoch [7/10], Step [510/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [512/938], Loss: 0.0083\n",
      "Epoch [7/10], Step [514/938], Loss: 0.0460\n",
      "Epoch [7/10], Step [516/938], Loss: 0.0086\n",
      "Epoch [7/10], Step [518/938], Loss: 0.0041\n",
      "Epoch [7/10], Step [520/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [522/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [524/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [526/938], Loss: 0.0346\n",
      "Epoch [7/10], Step [528/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [530/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [532/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [534/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [536/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [538/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [540/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [542/938], Loss: 0.0033\n",
      "Epoch [7/10], Step [544/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [546/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [548/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [550/938], Loss: 0.0027\n",
      "Epoch [7/10], Step [552/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [554/938], Loss: 0.0031\n",
      "Epoch [7/10], Step [556/938], Loss: 0.0281\n",
      "Epoch [7/10], Step [558/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [560/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [562/938], Loss: 0.0817\n",
      "Epoch [7/10], Step [564/938], Loss: 0.0078\n",
      "Epoch [7/10], Step [566/938], Loss: 0.0089\n",
      "Epoch [7/10], Step [568/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [570/938], Loss: 0.0235\n",
      "Epoch [7/10], Step [572/938], Loss: 0.0030\n",
      "Epoch [7/10], Step [574/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [576/938], Loss: 0.0151\n",
      "Epoch [7/10], Step [578/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [580/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [582/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [584/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [586/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [588/938], Loss: 0.0027\n",
      "Epoch [7/10], Step [590/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [592/938], Loss: 0.0168\n",
      "Epoch [7/10], Step [594/938], Loss: 0.0435\n",
      "Epoch [7/10], Step [596/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [598/938], Loss: 0.0388\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [602/938], Loss: 0.0141\n",
      "Epoch [7/10], Step [604/938], Loss: 0.0720\n",
      "Epoch [7/10], Step [606/938], Loss: 0.0086\n",
      "Epoch [7/10], Step [608/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [610/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [612/938], Loss: 0.0580\n",
      "Epoch [7/10], Step [614/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [616/938], Loss: 0.0091\n",
      "Epoch [7/10], Step [618/938], Loss: 0.0185\n",
      "Epoch [7/10], Step [620/938], Loss: 0.0243\n",
      "Epoch [7/10], Step [622/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [624/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [626/938], Loss: 0.0049\n",
      "Epoch [7/10], Step [628/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [630/938], Loss: 0.0271\n",
      "Epoch [7/10], Step [632/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [634/938], Loss: 0.0086\n",
      "Epoch [7/10], Step [636/938], Loss: 0.0155\n",
      "Epoch [7/10], Step [638/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [640/938], Loss: 0.0095\n",
      "Epoch [7/10], Step [642/938], Loss: 0.0156\n",
      "Epoch [7/10], Step [644/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [646/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [648/938], Loss: 0.0211\n",
      "Epoch [7/10], Step [650/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [652/938], Loss: 0.0099\n",
      "Epoch [7/10], Step [654/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [656/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [658/938], Loss: 0.0830\n",
      "Epoch [7/10], Step [660/938], Loss: 0.0145\n",
      "Epoch [7/10], Step [662/938], Loss: 0.0115\n",
      "Epoch [7/10], Step [664/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [666/938], Loss: 0.1795\n",
      "Epoch [7/10], Step [668/938], Loss: 0.0033\n",
      "Epoch [7/10], Step [670/938], Loss: 0.0179\n",
      "Epoch [7/10], Step [672/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [674/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [676/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [678/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [680/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [682/938], Loss: 0.0048\n",
      "Epoch [7/10], Step [684/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [686/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [688/938], Loss: 0.0184\n",
      "Epoch [7/10], Step [690/938], Loss: 0.0047\n",
      "Epoch [7/10], Step [692/938], Loss: 0.0140\n",
      "Epoch [7/10], Step [694/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [696/938], Loss: 0.0033\n",
      "Epoch [7/10], Step [698/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0064\n",
      "Epoch [7/10], Step [702/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [704/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [706/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [708/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [710/938], Loss: 0.0153\n",
      "Epoch [7/10], Step [712/938], Loss: 0.0036\n",
      "Epoch [7/10], Step [714/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [716/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [718/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [720/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [722/938], Loss: 0.0457\n",
      "Epoch [7/10], Step [724/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [726/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [728/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [730/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [732/938], Loss: 0.0354\n",
      "Epoch [7/10], Step [734/938], Loss: 0.0036\n",
      "Epoch [7/10], Step [736/938], Loss: 0.0099\n",
      "Epoch [7/10], Step [738/938], Loss: 0.0024\n",
      "Epoch [7/10], Step [740/938], Loss: 0.0065\n",
      "Epoch [7/10], Step [742/938], Loss: 0.0030\n",
      "Epoch [7/10], Step [744/938], Loss: 0.0098\n",
      "Epoch [7/10], Step [746/938], Loss: 0.0105\n",
      "Epoch [7/10], Step [748/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [750/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [752/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [754/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [756/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [758/938], Loss: 0.0348\n",
      "Epoch [7/10], Step [760/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [762/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [764/938], Loss: 0.0401\n",
      "Epoch [7/10], Step [766/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [768/938], Loss: 0.0040\n",
      "Epoch [7/10], Step [770/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [772/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [774/938], Loss: 0.0405\n",
      "Epoch [7/10], Step [776/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [778/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [780/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [782/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [784/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [786/938], Loss: 0.0026\n",
      "Epoch [7/10], Step [788/938], Loss: 0.0062\n",
      "Epoch [7/10], Step [790/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [792/938], Loss: 0.0142\n",
      "Epoch [7/10], Step [794/938], Loss: 0.0057\n",
      "Epoch [7/10], Step [796/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [798/938], Loss: 0.0251\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0072\n",
      "Epoch [7/10], Step [802/938], Loss: 0.0060\n",
      "Epoch [7/10], Step [804/938], Loss: 0.0093\n",
      "Epoch [7/10], Step [806/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [808/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [810/938], Loss: 0.0644\n",
      "Epoch [7/10], Step [812/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [814/938], Loss: 0.0060\n",
      "Epoch [7/10], Step [816/938], Loss: 0.0210\n",
      "Epoch [7/10], Step [818/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [820/938], Loss: 0.0088\n",
      "Epoch [7/10], Step [822/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [824/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [826/938], Loss: 0.0149\n",
      "Epoch [7/10], Step [828/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [830/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [832/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [834/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [836/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [838/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [840/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [842/938], Loss: 0.0071\n",
      "Epoch [7/10], Step [844/938], Loss: 0.0127\n",
      "Epoch [7/10], Step [846/938], Loss: 0.0160\n",
      "Epoch [7/10], Step [848/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [850/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [852/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [854/938], Loss: 0.0475\n",
      "Epoch [7/10], Step [856/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [858/938], Loss: 0.0453\n",
      "Epoch [7/10], Step [860/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [862/938], Loss: 0.0938\n",
      "Epoch [7/10], Step [864/938], Loss: 0.0052\n",
      "Epoch [7/10], Step [866/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [868/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [870/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [872/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [874/938], Loss: 0.0373\n",
      "Epoch [7/10], Step [876/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [878/938], Loss: 0.0114\n",
      "Epoch [7/10], Step [880/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [882/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [884/938], Loss: 0.0181\n",
      "Epoch [7/10], Step [886/938], Loss: 0.0135\n",
      "Epoch [7/10], Step [888/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [890/938], Loss: 0.0418\n",
      "Epoch [7/10], Step [892/938], Loss: 0.0218\n",
      "Epoch [7/10], Step [894/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [896/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [898/938], Loss: 0.0024\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0157\n",
      "Epoch [7/10], Step [902/938], Loss: 0.0323\n",
      "Epoch [7/10], Step [904/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [906/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [908/938], Loss: 0.0066\n",
      "Epoch [7/10], Step [910/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [912/938], Loss: 0.0040\n",
      "Epoch [7/10], Step [914/938], Loss: 0.0029\n",
      "Epoch [7/10], Step [916/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [918/938], Loss: 0.0037\n",
      "Epoch [7/10], Step [920/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [922/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [924/938], Loss: 0.0128\n",
      "Epoch [7/10], Step [926/938], Loss: 0.0396\n",
      "Epoch [7/10], Step [928/938], Loss: 0.0069\n",
      "Epoch [7/10], Step [930/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [932/938], Loss: 0.0175\n",
      "Epoch [7/10], Step [934/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [936/938], Loss: 0.0145\n",
      "Epoch [7/10], Step [938/938], Loss: 0.0008\n",
      "Epoch [7/10], Loss: 0.0113\n",
      "Epoch [8/10], Step [2/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [4/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [6/938], Loss: 0.0406\n",
      "Epoch [8/10], Step [8/938], Loss: 0.0085\n",
      "Epoch [8/10], Step [10/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [12/938], Loss: 0.0097\n",
      "Epoch [8/10], Step [14/938], Loss: 0.0028\n",
      "Epoch [8/10], Step [16/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [18/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [20/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [22/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [24/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [26/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [28/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [30/938], Loss: 0.0105\n",
      "Epoch [8/10], Step [32/938], Loss: 0.0698\n",
      "Epoch [8/10], Step [34/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [36/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [38/938], Loss: 0.0054\n",
      "Epoch [8/10], Step [40/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [42/938], Loss: 0.0107\n",
      "Epoch [8/10], Step [44/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [46/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [48/938], Loss: 0.0059\n",
      "Epoch [8/10], Step [50/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [52/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [54/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [56/938], Loss: 0.0225\n",
      "Epoch [8/10], Step [58/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [60/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [62/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [64/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [66/938], Loss: 0.0050\n",
      "Epoch [8/10], Step [68/938], Loss: 0.0088\n",
      "Epoch [8/10], Step [70/938], Loss: 0.0024\n",
      "Epoch [8/10], Step [72/938], Loss: 0.0253\n",
      "Epoch [8/10], Step [74/938], Loss: 0.0043\n",
      "Epoch [8/10], Step [76/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [78/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [80/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [82/938], Loss: 0.0059\n",
      "Epoch [8/10], Step [84/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [86/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [88/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [90/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [92/938], Loss: 0.0377\n",
      "Epoch [8/10], Step [94/938], Loss: 0.0812\n",
      "Epoch [8/10], Step [96/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [98/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [102/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [104/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [106/938], Loss: 0.0548\n",
      "Epoch [8/10], Step [108/938], Loss: 0.1093\n",
      "Epoch [8/10], Step [110/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [112/938], Loss: 0.0618\n",
      "Epoch [8/10], Step [114/938], Loss: 0.0094\n",
      "Epoch [8/10], Step [116/938], Loss: 0.0099\n",
      "Epoch [8/10], Step [118/938], Loss: 0.0084\n",
      "Epoch [8/10], Step [120/938], Loss: 0.0905\n",
      "Epoch [8/10], Step [122/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [124/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [126/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [128/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [130/938], Loss: 0.0362\n",
      "Epoch [8/10], Step [132/938], Loss: 0.0063\n",
      "Epoch [8/10], Step [134/938], Loss: 0.0093\n",
      "Epoch [8/10], Step [136/938], Loss: 0.0113\n",
      "Epoch [8/10], Step [138/938], Loss: 0.0310\n",
      "Epoch [8/10], Step [140/938], Loss: 0.0082\n",
      "Epoch [8/10], Step [142/938], Loss: 0.0205\n",
      "Epoch [8/10], Step [144/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [146/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [148/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [150/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [152/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [154/938], Loss: 0.0168\n",
      "Epoch [8/10], Step [156/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [158/938], Loss: 0.0118\n",
      "Epoch [8/10], Step [160/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [162/938], Loss: 0.0107\n",
      "Epoch [8/10], Step [164/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [166/938], Loss: 0.0097\n",
      "Epoch [8/10], Step [168/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [170/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [172/938], Loss: 0.1661\n",
      "Epoch [8/10], Step [174/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [176/938], Loss: 0.0266\n",
      "Epoch [8/10], Step [178/938], Loss: 0.0075\n",
      "Epoch [8/10], Step [180/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [182/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [184/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [186/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [188/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [190/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [192/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [194/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [196/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [198/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [202/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [204/938], Loss: 0.0076\n",
      "Epoch [8/10], Step [206/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [208/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [210/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [212/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [214/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [216/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [218/938], Loss: 0.0016\n",
      "Epoch [8/10], Step [220/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [222/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [224/938], Loss: 0.0122\n",
      "Epoch [8/10], Step [226/938], Loss: 0.0109\n",
      "Epoch [8/10], Step [228/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [230/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [232/938], Loss: 0.0077\n",
      "Epoch [8/10], Step [234/938], Loss: 0.0057\n",
      "Epoch [8/10], Step [236/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [238/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [240/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [242/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [244/938], Loss: 0.0106\n",
      "Epoch [8/10], Step [246/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [248/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [250/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [252/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [254/938], Loss: 0.0553\n",
      "Epoch [8/10], Step [256/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [258/938], Loss: 0.0225\n",
      "Epoch [8/10], Step [260/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [262/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [264/938], Loss: 0.0030\n",
      "Epoch [8/10], Step [266/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [268/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [270/938], Loss: 0.0223\n",
      "Epoch [8/10], Step [272/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [274/938], Loss: 0.0040\n",
      "Epoch [8/10], Step [276/938], Loss: 0.0380\n",
      "Epoch [8/10], Step [278/938], Loss: 0.0069\n",
      "Epoch [8/10], Step [280/938], Loss: 0.0177\n",
      "Epoch [8/10], Step [282/938], Loss: 0.0524\n",
      "Epoch [8/10], Step [284/938], Loss: 0.0416\n",
      "Epoch [8/10], Step [286/938], Loss: 0.1301\n",
      "Epoch [8/10], Step [288/938], Loss: 0.0160\n",
      "Epoch [8/10], Step [290/938], Loss: 0.0200\n",
      "Epoch [8/10], Step [292/938], Loss: 0.0230\n",
      "Epoch [8/10], Step [294/938], Loss: 0.0245\n",
      "Epoch [8/10], Step [296/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [298/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [302/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [304/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [306/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [308/938], Loss: 0.0053\n",
      "Epoch [8/10], Step [310/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [312/938], Loss: 0.0099\n",
      "Epoch [8/10], Step [314/938], Loss: 0.1268\n",
      "Epoch [8/10], Step [316/938], Loss: 0.0045\n",
      "Epoch [8/10], Step [318/938], Loss: 0.0453\n",
      "Epoch [8/10], Step [320/938], Loss: 0.0161\n",
      "Epoch [8/10], Step [322/938], Loss: 0.0134\n",
      "Epoch [8/10], Step [324/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [326/938], Loss: 0.0420\n",
      "Epoch [8/10], Step [328/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [330/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [332/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [334/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [336/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [338/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [340/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [342/938], Loss: 0.0523\n",
      "Epoch [8/10], Step [344/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [346/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [348/938], Loss: 0.0243\n",
      "Epoch [8/10], Step [350/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [352/938], Loss: 0.0267\n",
      "Epoch [8/10], Step [354/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [356/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [358/938], Loss: 0.0077\n",
      "Epoch [8/10], Step [360/938], Loss: 0.0266\n",
      "Epoch [8/10], Step [362/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [364/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [366/938], Loss: 0.0055\n",
      "Epoch [8/10], Step [368/938], Loss: 0.0447\n",
      "Epoch [8/10], Step [370/938], Loss: 0.0202\n",
      "Epoch [8/10], Step [372/938], Loss: 0.0052\n",
      "Epoch [8/10], Step [374/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [376/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [378/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [380/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [382/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [384/938], Loss: 0.0096\n",
      "Epoch [8/10], Step [386/938], Loss: 0.0185\n",
      "Epoch [8/10], Step [388/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [390/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [392/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [394/938], Loss: 0.0143\n",
      "Epoch [8/10], Step [396/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [398/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0360\n",
      "Epoch [8/10], Step [402/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [404/938], Loss: 0.0028\n",
      "Epoch [8/10], Step [406/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [408/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [410/938], Loss: 0.0038\n",
      "Epoch [8/10], Step [412/938], Loss: 0.0777\n",
      "Epoch [8/10], Step [414/938], Loss: 0.0094\n",
      "Epoch [8/10], Step [416/938], Loss: 0.0187\n",
      "Epoch [8/10], Step [418/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [420/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [422/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [424/938], Loss: 0.0083\n",
      "Epoch [8/10], Step [426/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [428/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [430/938], Loss: 0.0030\n",
      "Epoch [8/10], Step [432/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [434/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [436/938], Loss: 0.0283\n",
      "Epoch [8/10], Step [438/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [440/938], Loss: 0.0053\n",
      "Epoch [8/10], Step [442/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [444/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [446/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [448/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [450/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [452/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [454/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [456/938], Loss: 0.0061\n",
      "Epoch [8/10], Step [458/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [460/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [462/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [464/938], Loss: 0.0089\n",
      "Epoch [8/10], Step [466/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [468/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [470/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [472/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [474/938], Loss: 0.0070\n",
      "Epoch [8/10], Step [476/938], Loss: 0.0074\n",
      "Epoch [8/10], Step [478/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [480/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [482/938], Loss: 0.0049\n",
      "Epoch [8/10], Step [484/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [486/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [488/938], Loss: 0.0078\n",
      "Epoch [8/10], Step [490/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [492/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [494/938], Loss: 0.0016\n",
      "Epoch [8/10], Step [496/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [498/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0317\n",
      "Epoch [8/10], Step [502/938], Loss: 0.0055\n",
      "Epoch [8/10], Step [504/938], Loss: 0.0531\n",
      "Epoch [8/10], Step [506/938], Loss: 0.0106\n",
      "Epoch [8/10], Step [508/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [510/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [512/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [514/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [516/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [518/938], Loss: 0.0464\n",
      "Epoch [8/10], Step [520/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [522/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [524/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [526/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [528/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [530/938], Loss: 0.0034\n",
      "Epoch [8/10], Step [532/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [534/938], Loss: 0.0330\n",
      "Epoch [8/10], Step [536/938], Loss: 0.0392\n",
      "Epoch [8/10], Step [538/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [540/938], Loss: 0.0493\n",
      "Epoch [8/10], Step [542/938], Loss: 0.0549\n",
      "Epoch [8/10], Step [544/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [546/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [548/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [550/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [552/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [554/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [556/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [558/938], Loss: 0.0435\n",
      "Epoch [8/10], Step [560/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [562/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [564/938], Loss: 0.0134\n",
      "Epoch [8/10], Step [566/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [568/938], Loss: 0.0044\n",
      "Epoch [8/10], Step [570/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [572/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [574/938], Loss: 0.0179\n",
      "Epoch [8/10], Step [576/938], Loss: 0.0666\n",
      "Epoch [8/10], Step [578/938], Loss: 0.0219\n",
      "Epoch [8/10], Step [580/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [582/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [584/938], Loss: 0.0034\n",
      "Epoch [8/10], Step [586/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [588/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [590/938], Loss: 0.0372\n",
      "Epoch [8/10], Step [592/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [594/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [596/938], Loss: 0.0243\n",
      "Epoch [8/10], Step [598/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0066\n",
      "Epoch [8/10], Step [602/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [604/938], Loss: 0.0058\n",
      "Epoch [8/10], Step [606/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [608/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [610/938], Loss: 0.0042\n",
      "Epoch [8/10], Step [612/938], Loss: 0.0058\n",
      "Epoch [8/10], Step [614/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [616/938], Loss: 0.0478\n",
      "Epoch [8/10], Step [618/938], Loss: 0.0280\n",
      "Epoch [8/10], Step [620/938], Loss: 0.0209\n",
      "Epoch [8/10], Step [622/938], Loss: 0.0332\n",
      "Epoch [8/10], Step [624/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [626/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [628/938], Loss: 0.0620\n",
      "Epoch [8/10], Step [630/938], Loss: 0.0061\n",
      "Epoch [8/10], Step [632/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [634/938], Loss: 0.1288\n",
      "Epoch [8/10], Step [636/938], Loss: 0.0231\n",
      "Epoch [8/10], Step [638/938], Loss: 0.0069\n",
      "Epoch [8/10], Step [640/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [642/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [644/938], Loss: 0.0139\n",
      "Epoch [8/10], Step [646/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [648/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [650/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [652/938], Loss: 0.0024\n",
      "Epoch [8/10], Step [654/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [656/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [658/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [660/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [662/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [664/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [666/938], Loss: 0.0400\n",
      "Epoch [8/10], Step [668/938], Loss: 0.0170\n",
      "Epoch [8/10], Step [670/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [672/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [674/938], Loss: 0.0169\n",
      "Epoch [8/10], Step [676/938], Loss: 0.0079\n",
      "Epoch [8/10], Step [678/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [680/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [682/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [684/938], Loss: 0.0052\n",
      "Epoch [8/10], Step [686/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [688/938], Loss: 0.0485\n",
      "Epoch [8/10], Step [690/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [692/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [694/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [696/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [698/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [702/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [704/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [706/938], Loss: 0.1128\n",
      "Epoch [8/10], Step [708/938], Loss: 0.0253\n",
      "Epoch [8/10], Step [710/938], Loss: 0.0469\n",
      "Epoch [8/10], Step [712/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [714/938], Loss: 0.0108\n",
      "Epoch [8/10], Step [716/938], Loss: 0.0215\n",
      "Epoch [8/10], Step [718/938], Loss: 0.0184\n",
      "Epoch [8/10], Step [720/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [722/938], Loss: 0.0065\n",
      "Epoch [8/10], Step [724/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [726/938], Loss: 0.0329\n",
      "Epoch [8/10], Step [728/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [730/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [732/938], Loss: 0.0024\n",
      "Epoch [8/10], Step [734/938], Loss: 0.0126\n",
      "Epoch [8/10], Step [736/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [738/938], Loss: 0.0304\n",
      "Epoch [8/10], Step [740/938], Loss: 0.0255\n",
      "Epoch [8/10], Step [742/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [744/938], Loss: 0.0022\n",
      "Epoch [8/10], Step [746/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [748/938], Loss: 0.0330\n",
      "Epoch [8/10], Step [750/938], Loss: 0.0141\n",
      "Epoch [8/10], Step [752/938], Loss: 0.0282\n",
      "Epoch [8/10], Step [754/938], Loss: 0.0074\n",
      "Epoch [8/10], Step [756/938], Loss: 0.0049\n",
      "Epoch [8/10], Step [758/938], Loss: 0.0167\n",
      "Epoch [8/10], Step [760/938], Loss: 0.0137\n",
      "Epoch [8/10], Step [762/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [764/938], Loss: 0.1442\n",
      "Epoch [8/10], Step [766/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [768/938], Loss: 0.0264\n",
      "Epoch [8/10], Step [770/938], Loss: 0.0117\n",
      "Epoch [8/10], Step [772/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [774/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [776/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [778/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [780/938], Loss: 0.0354\n",
      "Epoch [8/10], Step [782/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [784/938], Loss: 0.0163\n",
      "Epoch [8/10], Step [786/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [788/938], Loss: 0.0244\n",
      "Epoch [8/10], Step [790/938], Loss: 0.0203\n",
      "Epoch [8/10], Step [792/938], Loss: 0.0026\n",
      "Epoch [8/10], Step [794/938], Loss: 0.0209\n",
      "Epoch [8/10], Step [796/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [798/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [802/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [804/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [806/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [808/938], Loss: 0.0121\n",
      "Epoch [8/10], Step [810/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [812/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [814/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [816/938], Loss: 0.0048\n",
      "Epoch [8/10], Step [818/938], Loss: 0.0066\n",
      "Epoch [8/10], Step [820/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [822/938], Loss: 0.0016\n",
      "Epoch [8/10], Step [824/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [826/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [828/938], Loss: 0.0142\n",
      "Epoch [8/10], Step [830/938], Loss: 0.0025\n",
      "Epoch [8/10], Step [832/938], Loss: 0.0125\n",
      "Epoch [8/10], Step [834/938], Loss: 0.0063\n",
      "Epoch [8/10], Step [836/938], Loss: 0.0034\n",
      "Epoch [8/10], Step [838/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [840/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [842/938], Loss: 0.0214\n",
      "Epoch [8/10], Step [844/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [846/938], Loss: 0.0049\n",
      "Epoch [8/10], Step [848/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [850/938], Loss: 0.0043\n",
      "Epoch [8/10], Step [852/938], Loss: 0.1340\n",
      "Epoch [8/10], Step [854/938], Loss: 0.0077\n",
      "Epoch [8/10], Step [856/938], Loss: 0.0129\n",
      "Epoch [8/10], Step [858/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [860/938], Loss: 0.0158\n",
      "Epoch [8/10], Step [862/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [864/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [866/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [868/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [870/938], Loss: 0.0648\n",
      "Epoch [8/10], Step [872/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [874/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [876/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [878/938], Loss: 0.0077\n",
      "Epoch [8/10], Step [880/938], Loss: 0.0390\n",
      "Epoch [8/10], Step [882/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [884/938], Loss: 0.0362\n",
      "Epoch [8/10], Step [886/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [888/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [890/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [892/938], Loss: 0.0710\n",
      "Epoch [8/10], Step [894/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [896/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [898/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [902/938], Loss: 0.0082\n",
      "Epoch [8/10], Step [904/938], Loss: 0.0282\n",
      "Epoch [8/10], Step [906/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [908/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [910/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [912/938], Loss: 0.0085\n",
      "Epoch [8/10], Step [914/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [916/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [918/938], Loss: 0.0047\n",
      "Epoch [8/10], Step [920/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [922/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [924/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [926/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [928/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [930/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [932/938], Loss: 0.0030\n",
      "Epoch [8/10], Step [934/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [936/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [938/938], Loss: 0.0020\n",
      "Epoch [8/10], Loss: 0.0093\n",
      "Epoch [9/10], Step [2/938], Loss: 0.0024\n",
      "Epoch [9/10], Step [4/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [6/938], Loss: 0.0096\n",
      "Epoch [9/10], Step [8/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [10/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [12/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [14/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [16/938], Loss: 0.0346\n",
      "Epoch [9/10], Step [18/938], Loss: 0.0057\n",
      "Epoch [9/10], Step [20/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [22/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [24/938], Loss: 0.0036\n",
      "Epoch [9/10], Step [26/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [28/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [30/938], Loss: 0.0055\n",
      "Epoch [9/10], Step [32/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [34/938], Loss: 0.0175\n",
      "Epoch [9/10], Step [36/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [38/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [40/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [42/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [44/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [46/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [48/938], Loss: 0.1191\n",
      "Epoch [9/10], Step [50/938], Loss: 0.0109\n",
      "Epoch [9/10], Step [52/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [54/938], Loss: 0.0157\n",
      "Epoch [9/10], Step [56/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [58/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [60/938], Loss: 0.0077\n",
      "Epoch [9/10], Step [62/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [64/938], Loss: 0.0020\n",
      "Epoch [9/10], Step [66/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [68/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [70/938], Loss: 0.0073\n",
      "Epoch [9/10], Step [72/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [74/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [76/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [78/938], Loss: 0.0125\n",
      "Epoch [9/10], Step [80/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [82/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [84/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [86/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [88/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [90/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [92/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [94/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [96/938], Loss: 0.0042\n",
      "Epoch [9/10], Step [98/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [102/938], Loss: 0.0092\n",
      "Epoch [9/10], Step [104/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [106/938], Loss: 0.0248\n",
      "Epoch [9/10], Step [108/938], Loss: 0.0058\n",
      "Epoch [9/10], Step [110/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [112/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [114/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [116/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [118/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [120/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [122/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [124/938], Loss: 0.0244\n",
      "Epoch [9/10], Step [126/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [128/938], Loss: 0.0641\n",
      "Epoch [9/10], Step [130/938], Loss: 0.0117\n",
      "Epoch [9/10], Step [132/938], Loss: 0.0067\n",
      "Epoch [9/10], Step [134/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [136/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [138/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [140/938], Loss: 0.0026\n",
      "Epoch [9/10], Step [142/938], Loss: 0.0072\n",
      "Epoch [9/10], Step [144/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [146/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [148/938], Loss: 0.1261\n",
      "Epoch [9/10], Step [150/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [152/938], Loss: 0.0246\n",
      "Epoch [9/10], Step [154/938], Loss: 0.0054\n",
      "Epoch [9/10], Step [156/938], Loss: 0.0064\n",
      "Epoch [9/10], Step [158/938], Loss: 0.0057\n",
      "Epoch [9/10], Step [160/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [162/938], Loss: 0.0139\n",
      "Epoch [9/10], Step [164/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [166/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [168/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [170/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [172/938], Loss: 0.0048\n",
      "Epoch [9/10], Step [174/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [176/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [178/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [180/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [182/938], Loss: 0.0048\n",
      "Epoch [9/10], Step [184/938], Loss: 0.0063\n",
      "Epoch [9/10], Step [186/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [188/938], Loss: 0.0164\n",
      "Epoch [9/10], Step [190/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [192/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [194/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [196/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [198/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [202/938], Loss: 0.0062\n",
      "Epoch [9/10], Step [204/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [206/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [208/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [210/938], Loss: 0.0639\n",
      "Epoch [9/10], Step [212/938], Loss: 0.0030\n",
      "Epoch [9/10], Step [214/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [216/938], Loss: 0.0026\n",
      "Epoch [9/10], Step [218/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [220/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [222/938], Loss: 0.0070\n",
      "Epoch [9/10], Step [224/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [226/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [228/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [230/938], Loss: 0.0042\n",
      "Epoch [9/10], Step [232/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [234/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [236/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [238/938], Loss: 0.0035\n",
      "Epoch [9/10], Step [240/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [242/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [244/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [246/938], Loss: 0.0075\n",
      "Epoch [9/10], Step [248/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [250/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [252/938], Loss: 0.0041\n",
      "Epoch [9/10], Step [254/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [256/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [258/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [260/938], Loss: 0.0307\n",
      "Epoch [9/10], Step [262/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [264/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [266/938], Loss: 0.0663\n",
      "Epoch [9/10], Step [268/938], Loss: 0.0047\n",
      "Epoch [9/10], Step [270/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [272/938], Loss: 0.0849\n",
      "Epoch [9/10], Step [274/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [276/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [278/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [280/938], Loss: 0.0399\n",
      "Epoch [9/10], Step [282/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [284/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [286/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [288/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [290/938], Loss: 0.0155\n",
      "Epoch [9/10], Step [292/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [294/938], Loss: 0.0064\n",
      "Epoch [9/10], Step [296/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [298/938], Loss: 0.1390\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [302/938], Loss: 0.0539\n",
      "Epoch [9/10], Step [304/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [306/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [308/938], Loss: 0.0277\n",
      "Epoch [9/10], Step [310/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [312/938], Loss: 0.0034\n",
      "Epoch [9/10], Step [314/938], Loss: 0.0027\n",
      "Epoch [9/10], Step [316/938], Loss: 0.0054\n",
      "Epoch [9/10], Step [318/938], Loss: 0.0036\n",
      "Epoch [9/10], Step [320/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [322/938], Loss: 0.0185\n",
      "Epoch [9/10], Step [324/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [326/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [328/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [330/938], Loss: 0.0241\n",
      "Epoch [9/10], Step [332/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [334/938], Loss: 0.0355\n",
      "Epoch [9/10], Step [336/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [338/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [340/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [342/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [344/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [346/938], Loss: 0.0047\n",
      "Epoch [9/10], Step [348/938], Loss: 0.1066\n",
      "Epoch [9/10], Step [350/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [352/938], Loss: 0.0156\n",
      "Epoch [9/10], Step [354/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [356/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [358/938], Loss: 0.0048\n",
      "Epoch [9/10], Step [360/938], Loss: 0.0027\n",
      "Epoch [9/10], Step [362/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [364/938], Loss: 0.0113\n",
      "Epoch [9/10], Step [366/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [368/938], Loss: 0.0242\n",
      "Epoch [9/10], Step [370/938], Loss: 0.0142\n",
      "Epoch [9/10], Step [372/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [374/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [376/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [378/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [380/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [382/938], Loss: 0.0020\n",
      "Epoch [9/10], Step [384/938], Loss: 0.0112\n",
      "Epoch [9/10], Step [386/938], Loss: 0.0477\n",
      "Epoch [9/10], Step [388/938], Loss: 0.0248\n",
      "Epoch [9/10], Step [390/938], Loss: 0.0030\n",
      "Epoch [9/10], Step [392/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [394/938], Loss: 0.0059\n",
      "Epoch [9/10], Step [396/938], Loss: 0.0056\n",
      "Epoch [9/10], Step [398/938], Loss: 0.0219\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [402/938], Loss: 0.0168\n",
      "Epoch [9/10], Step [404/938], Loss: 0.0064\n",
      "Epoch [9/10], Step [406/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [408/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [410/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [412/938], Loss: 0.0387\n",
      "Epoch [9/10], Step [414/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [416/938], Loss: 0.0032\n",
      "Epoch [9/10], Step [418/938], Loss: 0.0114\n",
      "Epoch [9/10], Step [420/938], Loss: 0.0071\n",
      "Epoch [9/10], Step [422/938], Loss: 0.0357\n",
      "Epoch [9/10], Step [424/938], Loss: 0.0114\n",
      "Epoch [9/10], Step [426/938], Loss: 0.0113\n",
      "Epoch [9/10], Step [428/938], Loss: 0.0063\n",
      "Epoch [9/10], Step [430/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [432/938], Loss: 0.0125\n",
      "Epoch [9/10], Step [434/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [436/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [438/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [440/938], Loss: 0.0333\n",
      "Epoch [9/10], Step [442/938], Loss: 0.0024\n",
      "Epoch [9/10], Step [444/938], Loss: 0.0526\n",
      "Epoch [9/10], Step [446/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [448/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [450/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [452/938], Loss: 0.0197\n",
      "Epoch [9/10], Step [454/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [456/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [458/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [460/938], Loss: 0.0192\n",
      "Epoch [9/10], Step [462/938], Loss: 0.0658\n",
      "Epoch [9/10], Step [464/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [466/938], Loss: 0.0207\n",
      "Epoch [9/10], Step [468/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [470/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [472/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [474/938], Loss: 0.0030\n",
      "Epoch [9/10], Step [476/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [478/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [480/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [482/938], Loss: 0.0094\n",
      "Epoch [9/10], Step [484/938], Loss: 0.0042\n",
      "Epoch [9/10], Step [486/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [488/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [490/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [492/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [494/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [496/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [498/938], Loss: 0.0082\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0802\n",
      "Epoch [9/10], Step [502/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [504/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [506/938], Loss: 0.0129\n",
      "Epoch [9/10], Step [508/938], Loss: 0.0069\n",
      "Epoch [9/10], Step [510/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [512/938], Loss: 0.0024\n",
      "Epoch [9/10], Step [514/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [516/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [518/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [520/938], Loss: 0.0043\n",
      "Epoch [9/10], Step [522/938], Loss: 0.0172\n",
      "Epoch [9/10], Step [524/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [526/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [528/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [530/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [532/938], Loss: 0.0283\n",
      "Epoch [9/10], Step [534/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [536/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [538/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [540/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [542/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [544/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [546/938], Loss: 0.0825\n",
      "Epoch [9/10], Step [548/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [550/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [552/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [554/938], Loss: 0.0060\n",
      "Epoch [9/10], Step [556/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [558/938], Loss: 0.0026\n",
      "Epoch [9/10], Step [560/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [562/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [564/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [566/938], Loss: 0.0069\n",
      "Epoch [9/10], Step [568/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [570/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [572/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [574/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [576/938], Loss: 0.0333\n",
      "Epoch [9/10], Step [578/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [580/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [582/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [584/938], Loss: 0.0040\n",
      "Epoch [9/10], Step [586/938], Loss: 0.0074\n",
      "Epoch [9/10], Step [588/938], Loss: 0.0179\n",
      "Epoch [9/10], Step [590/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [592/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [594/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [596/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [598/938], Loss: 0.0214\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [602/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [604/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [606/938], Loss: 0.0136\n",
      "Epoch [9/10], Step [608/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [610/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [612/938], Loss: 0.0183\n",
      "Epoch [9/10], Step [614/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [616/938], Loss: 0.0118\n",
      "Epoch [9/10], Step [618/938], Loss: 0.0087\n",
      "Epoch [9/10], Step [620/938], Loss: 0.0195\n",
      "Epoch [9/10], Step [622/938], Loss: 0.0044\n",
      "Epoch [9/10], Step [624/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [626/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [628/938], Loss: 0.0033\n",
      "Epoch [9/10], Step [630/938], Loss: 0.0130\n",
      "Epoch [9/10], Step [632/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [634/938], Loss: 0.0377\n",
      "Epoch [9/10], Step [636/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [638/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [640/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [642/938], Loss: 0.0232\n",
      "Epoch [9/10], Step [644/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [646/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [648/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [650/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [652/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [654/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [656/938], Loss: 0.0058\n",
      "Epoch [9/10], Step [658/938], Loss: 0.0170\n",
      "Epoch [9/10], Step [660/938], Loss: 0.0684\n",
      "Epoch [9/10], Step [662/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [664/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [666/938], Loss: 0.0380\n",
      "Epoch [9/10], Step [668/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [670/938], Loss: 0.0202\n",
      "Epoch [9/10], Step [672/938], Loss: 0.0153\n",
      "Epoch [9/10], Step [674/938], Loss: 0.0220\n",
      "Epoch [9/10], Step [676/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [678/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [680/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [682/938], Loss: 0.0297\n",
      "Epoch [9/10], Step [684/938], Loss: 0.0269\n",
      "Epoch [9/10], Step [686/938], Loss: 0.0061\n",
      "Epoch [9/10], Step [688/938], Loss: 0.0249\n",
      "Epoch [9/10], Step [690/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [692/938], Loss: 0.0697\n",
      "Epoch [9/10], Step [694/938], Loss: 0.0056\n",
      "Epoch [9/10], Step [696/938], Loss: 0.0116\n",
      "Epoch [9/10], Step [698/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [702/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [704/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [706/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [708/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [710/938], Loss: 0.0029\n",
      "Epoch [9/10], Step [712/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [714/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [716/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [718/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [720/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [722/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [724/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [726/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [728/938], Loss: 0.0591\n",
      "Epoch [9/10], Step [730/938], Loss: 0.0101\n",
      "Epoch [9/10], Step [732/938], Loss: 0.0281\n",
      "Epoch [9/10], Step [734/938], Loss: 0.0403\n",
      "Epoch [9/10], Step [736/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [738/938], Loss: 0.0256\n",
      "Epoch [9/10], Step [740/938], Loss: 0.0591\n",
      "Epoch [9/10], Step [742/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [744/938], Loss: 0.0654\n",
      "Epoch [9/10], Step [746/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [748/938], Loss: 0.0729\n",
      "Epoch [9/10], Step [750/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [752/938], Loss: 0.0273\n",
      "Epoch [9/10], Step [754/938], Loss: 0.0074\n",
      "Epoch [9/10], Step [756/938], Loss: 0.0200\n",
      "Epoch [9/10], Step [758/938], Loss: 0.0026\n",
      "Epoch [9/10], Step [760/938], Loss: 0.0654\n",
      "Epoch [9/10], Step [762/938], Loss: 0.0380\n",
      "Epoch [9/10], Step [764/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [766/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [768/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [770/938], Loss: 0.0192\n",
      "Epoch [9/10], Step [772/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [774/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [776/938], Loss: 0.0238\n",
      "Epoch [9/10], Step [778/938], Loss: 0.0132\n",
      "Epoch [9/10], Step [780/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [782/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [784/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [786/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [788/938], Loss: 0.0149\n",
      "Epoch [9/10], Step [790/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [792/938], Loss: 0.0091\n",
      "Epoch [9/10], Step [794/938], Loss: 0.0041\n",
      "Epoch [9/10], Step [796/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [798/938], Loss: 0.0040\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [802/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [804/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [806/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [808/938], Loss: 0.0027\n",
      "Epoch [9/10], Step [810/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [812/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [814/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [816/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [818/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [820/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [822/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [824/938], Loss: 0.0094\n",
      "Epoch [9/10], Step [826/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [828/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [830/938], Loss: 0.0143\n",
      "Epoch [9/10], Step [832/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [834/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [836/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [838/938], Loss: 0.0324\n",
      "Epoch [9/10], Step [840/938], Loss: 0.0106\n",
      "Epoch [9/10], Step [842/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [844/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [846/938], Loss: 0.0078\n",
      "Epoch [9/10], Step [848/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [850/938], Loss: 0.0050\n",
      "Epoch [9/10], Step [852/938], Loss: 0.1115\n",
      "Epoch [9/10], Step [854/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [856/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [858/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [860/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [862/938], Loss: 0.0213\n",
      "Epoch [9/10], Step [864/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [866/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [868/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [870/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [872/938], Loss: 0.0037\n",
      "Epoch [9/10], Step [874/938], Loss: 0.0071\n",
      "Epoch [9/10], Step [876/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [878/938], Loss: 0.0194\n",
      "Epoch [9/10], Step [880/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [882/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [884/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [886/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [888/938], Loss: 0.0102\n",
      "Epoch [9/10], Step [890/938], Loss: 0.0303\n",
      "Epoch [9/10], Step [892/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [894/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [896/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [898/938], Loss: 0.0043\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [902/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [904/938], Loss: 0.0053\n",
      "Epoch [9/10], Step [906/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [908/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [910/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [912/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [914/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [916/938], Loss: 0.0291\n",
      "Epoch [9/10], Step [918/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [920/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [922/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [924/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [926/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [928/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [930/938], Loss: 0.0176\n",
      "Epoch [9/10], Step [932/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [934/938], Loss: 0.0076\n",
      "Epoch [9/10], Step [936/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [938/938], Loss: 0.0012\n",
      "Epoch [9/10], Loss: 0.0081\n",
      "Epoch [10/10], Step [2/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [4/938], Loss: 0.0318\n",
      "Epoch [10/10], Step [6/938], Loss: 0.0022\n",
      "Epoch [10/10], Step [8/938], Loss: 0.0418\n",
      "Epoch [10/10], Step [10/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [12/938], Loss: 0.0044\n",
      "Epoch [10/10], Step [14/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [16/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [18/938], Loss: 0.0568\n",
      "Epoch [10/10], Step [20/938], Loss: 0.0309\n",
      "Epoch [10/10], Step [22/938], Loss: 0.0058\n",
      "Epoch [10/10], Step [24/938], Loss: 0.0020\n",
      "Epoch [10/10], Step [26/938], Loss: 0.0199\n",
      "Epoch [10/10], Step [28/938], Loss: 0.0035\n",
      "Epoch [10/10], Step [30/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [32/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [34/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [36/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [38/938], Loss: 0.0107\n",
      "Epoch [10/10], Step [40/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [42/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [44/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [46/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [48/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [50/938], Loss: 0.0054\n",
      "Epoch [10/10], Step [52/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [54/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [56/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [58/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [60/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [62/938], Loss: 0.0058\n",
      "Epoch [10/10], Step [64/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [66/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [68/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [70/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [72/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [74/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [76/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [78/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [80/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [82/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [84/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [86/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [88/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [90/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [92/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [94/938], Loss: 0.0035\n",
      "Epoch [10/10], Step [96/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [98/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [102/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [104/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [106/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [108/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [110/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [112/938], Loss: 0.0055\n",
      "Epoch [10/10], Step [114/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [116/938], Loss: 0.0027\n",
      "Epoch [10/10], Step [118/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [120/938], Loss: 0.0031\n",
      "Epoch [10/10], Step [122/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [124/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [126/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [128/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [130/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [132/938], Loss: 0.0376\n",
      "Epoch [10/10], Step [134/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [136/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [138/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [140/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [142/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [144/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [146/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [148/938], Loss: 0.0251\n",
      "Epoch [10/10], Step [150/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [152/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [154/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [156/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [158/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [160/938], Loss: 0.0391\n",
      "Epoch [10/10], Step [162/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [164/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [166/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [168/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [170/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [172/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [174/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [176/938], Loss: 0.0481\n",
      "Epoch [10/10], Step [178/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [180/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [182/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [184/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [186/938], Loss: 0.0348\n",
      "Epoch [10/10], Step [188/938], Loss: 0.0985\n",
      "Epoch [10/10], Step [190/938], Loss: 0.0232\n",
      "Epoch [10/10], Step [192/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [194/938], Loss: 0.0022\n",
      "Epoch [10/10], Step [196/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [198/938], Loss: 0.0444\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0501\n",
      "Epoch [10/10], Step [202/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [204/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [206/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [208/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [210/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [212/938], Loss: 0.0075\n",
      "Epoch [10/10], Step [214/938], Loss: 0.0053\n",
      "Epoch [10/10], Step [216/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [218/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [220/938], Loss: 0.0153\n",
      "Epoch [10/10], Step [222/938], Loss: 0.0104\n",
      "Epoch [10/10], Step [224/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [226/938], Loss: 0.0066\n",
      "Epoch [10/10], Step [228/938], Loss: 0.0077\n",
      "Epoch [10/10], Step [230/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [232/938], Loss: 0.0053\n",
      "Epoch [10/10], Step [234/938], Loss: 0.0204\n",
      "Epoch [10/10], Step [236/938], Loss: 0.0050\n",
      "Epoch [10/10], Step [238/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [240/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [242/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [244/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [246/938], Loss: 0.0071\n",
      "Epoch [10/10], Step [248/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [250/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [252/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [254/938], Loss: 0.0310\n",
      "Epoch [10/10], Step [256/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [258/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [260/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [262/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [264/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [266/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [268/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [270/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [272/938], Loss: 0.0033\n",
      "Epoch [10/10], Step [274/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [276/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [278/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [280/938], Loss: 0.0147\n",
      "Epoch [10/10], Step [282/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [284/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [286/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [288/938], Loss: 0.0042\n",
      "Epoch [10/10], Step [290/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [292/938], Loss: 0.0066\n",
      "Epoch [10/10], Step [294/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [296/938], Loss: 0.0108\n",
      "Epoch [10/10], Step [298/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [302/938], Loss: 0.0109\n",
      "Epoch [10/10], Step [304/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [306/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [308/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [310/938], Loss: 0.0106\n",
      "Epoch [10/10], Step [312/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [314/938], Loss: 0.0060\n",
      "Epoch [10/10], Step [316/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [318/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [320/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [322/938], Loss: 0.0099\n",
      "Epoch [10/10], Step [324/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [326/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [328/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [330/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [332/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [334/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [336/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [338/938], Loss: 0.0285\n",
      "Epoch [10/10], Step [340/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [342/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [344/938], Loss: 0.0146\n",
      "Epoch [10/10], Step [346/938], Loss: 0.0037\n",
      "Epoch [10/10], Step [348/938], Loss: 0.0186\n",
      "Epoch [10/10], Step [350/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [352/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [354/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [356/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [358/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [360/938], Loss: 0.0147\n",
      "Epoch [10/10], Step [362/938], Loss: 0.0093\n",
      "Epoch [10/10], Step [364/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [366/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [368/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [370/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [372/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [374/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [376/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [378/938], Loss: 0.0068\n",
      "Epoch [10/10], Step [380/938], Loss: 0.0605\n",
      "Epoch [10/10], Step [382/938], Loss: 0.0094\n",
      "Epoch [10/10], Step [384/938], Loss: 0.0088\n",
      "Epoch [10/10], Step [386/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [388/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [390/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [392/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [394/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [396/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [398/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [402/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [404/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [406/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [408/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [410/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [412/938], Loss: 0.0047\n",
      "Epoch [10/10], Step [414/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [416/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [418/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [420/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [422/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [424/938], Loss: 0.0279\n",
      "Epoch [10/10], Step [426/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [428/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [430/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [432/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [434/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [436/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [438/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [440/938], Loss: 0.0096\n",
      "Epoch [10/10], Step [442/938], Loss: 0.0121\n",
      "Epoch [10/10], Step [444/938], Loss: 0.0032\n",
      "Epoch [10/10], Step [446/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [448/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [450/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [452/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [454/938], Loss: 0.0344\n",
      "Epoch [10/10], Step [456/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [458/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [460/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [462/938], Loss: 0.0038\n",
      "Epoch [10/10], Step [464/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [466/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [468/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [470/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [472/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [474/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [476/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [478/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [480/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [482/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [484/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [486/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [488/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [490/938], Loss: 0.0084\n",
      "Epoch [10/10], Step [492/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [494/938], Loss: 0.0037\n",
      "Epoch [10/10], Step [496/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [498/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0252\n",
      "Epoch [10/10], Step [502/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [504/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [506/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [508/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [510/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [512/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [514/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [516/938], Loss: 0.0427\n",
      "Epoch [10/10], Step [518/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [520/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [522/938], Loss: 0.0021\n",
      "Epoch [10/10], Step [524/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [526/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [528/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [530/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [532/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [534/938], Loss: 0.0190\n",
      "Epoch [10/10], Step [536/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [538/938], Loss: 0.0049\n",
      "Epoch [10/10], Step [540/938], Loss: 0.0069\n",
      "Epoch [10/10], Step [542/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [544/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [546/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [548/938], Loss: 0.0027\n",
      "Epoch [10/10], Step [550/938], Loss: 0.0028\n",
      "Epoch [10/10], Step [552/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [554/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [556/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [558/938], Loss: 0.0073\n",
      "Epoch [10/10], Step [560/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [562/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [564/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [566/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [568/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [570/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [572/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [574/938], Loss: 0.0032\n",
      "Epoch [10/10], Step [576/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [578/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [580/938], Loss: 0.0142\n",
      "Epoch [10/10], Step [582/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [584/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [586/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [588/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [590/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [592/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [594/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [596/938], Loss: 0.0049\n",
      "Epoch [10/10], Step [598/938], Loss: 0.0190\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0173\n",
      "Epoch [10/10], Step [602/938], Loss: 0.0100\n",
      "Epoch [10/10], Step [604/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [606/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [608/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [610/938], Loss: 0.0160\n",
      "Epoch [10/10], Step [612/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [614/938], Loss: 0.0159\n",
      "Epoch [10/10], Step [616/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [618/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [620/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [622/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [624/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [626/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [628/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [630/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [632/938], Loss: 0.0164\n",
      "Epoch [10/10], Step [634/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [636/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [638/938], Loss: 0.0146\n",
      "Epoch [10/10], Step [640/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [642/938], Loss: 0.0084\n",
      "Epoch [10/10], Step [644/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [646/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [648/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [650/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [652/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [654/938], Loss: 0.0345\n",
      "Epoch [10/10], Step [656/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [658/938], Loss: 0.0075\n",
      "Epoch [10/10], Step [660/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [662/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [664/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [666/938], Loss: 0.0246\n",
      "Epoch [10/10], Step [668/938], Loss: 0.0765\n",
      "Epoch [10/10], Step [670/938], Loss: 0.1303\n",
      "Epoch [10/10], Step [672/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [674/938], Loss: 0.0301\n",
      "Epoch [10/10], Step [676/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [678/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [680/938], Loss: 0.0144\n",
      "Epoch [10/10], Step [682/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [684/938], Loss: 0.0069\n",
      "Epoch [10/10], Step [686/938], Loss: 0.0130\n",
      "Epoch [10/10], Step [688/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [690/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [692/938], Loss: 0.0707\n",
      "Epoch [10/10], Step [694/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [696/938], Loss: 0.0198\n",
      "Epoch [10/10], Step [698/938], Loss: 0.0087\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0022\n",
      "Epoch [10/10], Step [702/938], Loss: 0.0091\n",
      "Epoch [10/10], Step [704/938], Loss: 0.0105\n",
      "Epoch [10/10], Step [706/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [708/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [710/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [712/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [714/938], Loss: 0.0180\n",
      "Epoch [10/10], Step [716/938], Loss: 0.0047\n",
      "Epoch [10/10], Step [718/938], Loss: 0.0080\n",
      "Epoch [10/10], Step [720/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [722/938], Loss: 0.0132\n",
      "Epoch [10/10], Step [724/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [726/938], Loss: 0.0809\n",
      "Epoch [10/10], Step [728/938], Loss: 0.0392\n",
      "Epoch [10/10], Step [730/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [732/938], Loss: 0.0215\n",
      "Epoch [10/10], Step [734/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [736/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [738/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [740/938], Loss: 0.0224\n",
      "Epoch [10/10], Step [742/938], Loss: 0.0105\n",
      "Epoch [10/10], Step [744/938], Loss: 0.0048\n",
      "Epoch [10/10], Step [746/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [748/938], Loss: 0.0231\n",
      "Epoch [10/10], Step [750/938], Loss: 0.0193\n",
      "Epoch [10/10], Step [752/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [754/938], Loss: 0.0055\n",
      "Epoch [10/10], Step [756/938], Loss: 0.0126\n",
      "Epoch [10/10], Step [758/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [760/938], Loss: 0.0099\n",
      "Epoch [10/10], Step [762/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [764/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [766/938], Loss: 0.0600\n",
      "Epoch [10/10], Step [768/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [770/938], Loss: 0.0208\n",
      "Epoch [10/10], Step [772/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [774/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [776/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [778/938], Loss: 0.0426\n",
      "Epoch [10/10], Step [780/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [782/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [784/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [786/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [788/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [790/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [792/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [794/938], Loss: 0.0895\n",
      "Epoch [10/10], Step [796/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [798/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [802/938], Loss: 0.0179\n",
      "Epoch [10/10], Step [804/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [806/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [808/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [810/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [812/938], Loss: 0.0081\n",
      "Epoch [10/10], Step [814/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [816/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [818/938], Loss: 0.0374\n",
      "Epoch [10/10], Step [820/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [822/938], Loss: 0.0020\n",
      "Epoch [10/10], Step [824/938], Loss: 0.0120\n",
      "Epoch [10/10], Step [826/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [828/938], Loss: 0.0302\n",
      "Epoch [10/10], Step [830/938], Loss: 0.0690\n",
      "Epoch [10/10], Step [832/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [834/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [836/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [838/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [840/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [842/938], Loss: 0.0138\n",
      "Epoch [10/10], Step [844/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [846/938], Loss: 0.0058\n",
      "Epoch [10/10], Step [848/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [850/938], Loss: 0.0146\n",
      "Epoch [10/10], Step [852/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [854/938], Loss: 0.0235\n",
      "Epoch [10/10], Step [856/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [858/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [860/938], Loss: 0.0331\n",
      "Epoch [10/10], Step [862/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [864/938], Loss: 0.0034\n",
      "Epoch [10/10], Step [866/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [868/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [870/938], Loss: 0.0076\n",
      "Epoch [10/10], Step [872/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [874/938], Loss: 0.0046\n",
      "Epoch [10/10], Step [876/938], Loss: 0.0033\n",
      "Epoch [10/10], Step [878/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [880/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [882/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [884/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [886/938], Loss: 0.0050\n",
      "Epoch [10/10], Step [888/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [890/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [892/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [894/938], Loss: 0.0528\n",
      "Epoch [10/10], Step [896/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [898/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [902/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [904/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [906/938], Loss: 0.0020\n",
      "Epoch [10/10], Step [908/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [910/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [912/938], Loss: 0.0242\n",
      "Epoch [10/10], Step [914/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [916/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [918/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [920/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [922/938], Loss: 0.0223\n",
      "Epoch [10/10], Step [924/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [926/938], Loss: 0.0484\n",
      "Epoch [10/10], Step [928/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [930/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [932/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [934/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [936/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0000\n",
      "Epoch [10/10], Loss: 0.0064\n",
      "Training is finished!\n",
      "Accuracy: 99.14%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model with Adam\")\n",
    "adam_model = SimpleCNN().to(device)\n",
    "adam_train_loss = train_model('Adam', adam_model, train_loader, criterion, num_epochs)\n",
    "accuracy_sgd = evaluate_model(adam_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9TmYsV-U4oe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9TmYsV-U4oe",
    "outputId": "85626e23-1479-4f30-aaf7-a1771d7bbebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with RMSprop\n",
      "Epoch [1/10], Step [2/938], Loss: 2.2754\n",
      "Epoch [1/10], Step [4/938], Loss: 2.2146\n",
      "Epoch [1/10], Step [6/938], Loss: 2.1226\n",
      "Epoch [1/10], Step [8/938], Loss: 1.8421\n",
      "Epoch [1/10], Step [10/938], Loss: 1.7309\n",
      "Epoch [1/10], Step [12/938], Loss: 1.5993\n",
      "Epoch [1/10], Step [14/938], Loss: 1.4635\n",
      "Epoch [1/10], Step [16/938], Loss: 1.0243\n",
      "Epoch [1/10], Step [18/938], Loss: 1.1445\n",
      "Epoch [1/10], Step [20/938], Loss: 0.8261\n",
      "Epoch [1/10], Step [22/938], Loss: 0.8210\n",
      "Epoch [1/10], Step [24/938], Loss: 0.7733\n",
      "Epoch [1/10], Step [26/938], Loss: 0.6714\n",
      "Epoch [1/10], Step [28/938], Loss: 0.6210\n",
      "Epoch [1/10], Step [30/938], Loss: 0.5213\n",
      "Epoch [1/10], Step [32/938], Loss: 0.6853\n",
      "Epoch [1/10], Step [34/938], Loss: 0.4545\n",
      "Epoch [1/10], Step [36/938], Loss: 0.4183\n",
      "Epoch [1/10], Step [38/938], Loss: 0.5717\n",
      "Epoch [1/10], Step [40/938], Loss: 0.5946\n",
      "Epoch [1/10], Step [42/938], Loss: 0.4253\n",
      "Epoch [1/10], Step [44/938], Loss: 0.3285\n",
      "Epoch [1/10], Step [46/938], Loss: 0.5343\n",
      "Epoch [1/10], Step [48/938], Loss: 0.2900\n",
      "Epoch [1/10], Step [50/938], Loss: 0.5148\n",
      "Epoch [1/10], Step [52/938], Loss: 0.3384\n",
      "Epoch [1/10], Step [54/938], Loss: 0.3861\n",
      "Epoch [1/10], Step [56/938], Loss: 0.2987\n",
      "Epoch [1/10], Step [58/938], Loss: 0.2383\n",
      "Epoch [1/10], Step [60/938], Loss: 0.2897\n",
      "Epoch [1/10], Step [62/938], Loss: 0.1275\n",
      "Epoch [1/10], Step [64/938], Loss: 0.2757\n",
      "Epoch [1/10], Step [66/938], Loss: 0.5305\n",
      "Epoch [1/10], Step [68/938], Loss: 0.1336\n",
      "Epoch [1/10], Step [70/938], Loss: 0.3157\n",
      "Epoch [1/10], Step [72/938], Loss: 0.2688\n",
      "Epoch [1/10], Step [74/938], Loss: 0.2929\n",
      "Epoch [1/10], Step [76/938], Loss: 0.2461\n",
      "Epoch [1/10], Step [78/938], Loss: 0.2959\n",
      "Epoch [1/10], Step [80/938], Loss: 0.2124\n",
      "Epoch [1/10], Step [82/938], Loss: 0.3023\n",
      "Epoch [1/10], Step [84/938], Loss: 0.1715\n",
      "Epoch [1/10], Step [86/938], Loss: 0.3562\n",
      "Epoch [1/10], Step [88/938], Loss: 0.1263\n",
      "Epoch [1/10], Step [90/938], Loss: 0.2328\n",
      "Epoch [1/10], Step [92/938], Loss: 0.1680\n",
      "Epoch [1/10], Step [94/938], Loss: 0.1907\n",
      "Epoch [1/10], Step [96/938], Loss: 0.3697\n",
      "Epoch [1/10], Step [98/938], Loss: 0.4216\n",
      "Epoch [1/10], Step [100/938], Loss: 0.1654\n",
      "Epoch [1/10], Step [102/938], Loss: 0.2069\n",
      "Epoch [1/10], Step [104/938], Loss: 0.1186\n",
      "Epoch [1/10], Step [106/938], Loss: 0.1572\n",
      "Epoch [1/10], Step [108/938], Loss: 0.2470\n",
      "Epoch [1/10], Step [110/938], Loss: 0.2454\n",
      "Epoch [1/10], Step [112/938], Loss: 0.1948\n",
      "Epoch [1/10], Step [114/938], Loss: 0.2802\n",
      "Epoch [1/10], Step [116/938], Loss: 0.1641\n",
      "Epoch [1/10], Step [118/938], Loss: 0.0874\n",
      "Epoch [1/10], Step [120/938], Loss: 0.1014\n",
      "Epoch [1/10], Step [122/938], Loss: 0.1325\n",
      "Epoch [1/10], Step [124/938], Loss: 0.0673\n",
      "Epoch [1/10], Step [126/938], Loss: 0.1121\n",
      "Epoch [1/10], Step [128/938], Loss: 0.0930\n",
      "Epoch [1/10], Step [130/938], Loss: 0.3180\n",
      "Epoch [1/10], Step [132/938], Loss: 0.1869\n",
      "Epoch [1/10], Step [134/938], Loss: 0.1932\n",
      "Epoch [1/10], Step [136/938], Loss: 0.2175\n",
      "Epoch [1/10], Step [138/938], Loss: 0.1771\n",
      "Epoch [1/10], Step [140/938], Loss: 0.2063\n",
      "Epoch [1/10], Step [142/938], Loss: 0.0974\n",
      "Epoch [1/10], Step [144/938], Loss: 0.0754\n",
      "Epoch [1/10], Step [146/938], Loss: 0.1100\n",
      "Epoch [1/10], Step [148/938], Loss: 0.2084\n",
      "Epoch [1/10], Step [150/938], Loss: 0.2051\n",
      "Epoch [1/10], Step [152/938], Loss: 0.1693\n",
      "Epoch [1/10], Step [154/938], Loss: 0.1056\n",
      "Epoch [1/10], Step [156/938], Loss: 0.0866\n",
      "Epoch [1/10], Step [158/938], Loss: 0.1373\n",
      "Epoch [1/10], Step [160/938], Loss: 0.1183\n",
      "Epoch [1/10], Step [162/938], Loss: 0.2318\n",
      "Epoch [1/10], Step [164/938], Loss: 0.2513\n",
      "Epoch [1/10], Step [166/938], Loss: 0.1709\n",
      "Epoch [1/10], Step [168/938], Loss: 0.2779\n",
      "Epoch [1/10], Step [170/938], Loss: 0.1854\n",
      "Epoch [1/10], Step [172/938], Loss: 0.2991\n",
      "Epoch [1/10], Step [174/938], Loss: 0.2048\n",
      "Epoch [1/10], Step [176/938], Loss: 0.1608\n",
      "Epoch [1/10], Step [178/938], Loss: 0.1100\n",
      "Epoch [1/10], Step [180/938], Loss: 0.1805\n",
      "Epoch [1/10], Step [182/938], Loss: 0.2086\n",
      "Epoch [1/10], Step [184/938], Loss: 0.1890\n",
      "Epoch [1/10], Step [186/938], Loss: 0.1479\n",
      "Epoch [1/10], Step [188/938], Loss: 0.1494\n",
      "Epoch [1/10], Step [190/938], Loss: 0.1559\n",
      "Epoch [1/10], Step [192/938], Loss: 0.2011\n",
      "Epoch [1/10], Step [194/938], Loss: 0.0632\n",
      "Epoch [1/10], Step [196/938], Loss: 0.1626\n",
      "Epoch [1/10], Step [198/938], Loss: 0.1457\n",
      "Epoch [1/10], Step [200/938], Loss: 0.2476\n",
      "Epoch [1/10], Step [202/938], Loss: 0.0836\n",
      "Epoch [1/10], Step [204/938], Loss: 0.0517\n",
      "Epoch [1/10], Step [206/938], Loss: 0.0968\n",
      "Epoch [1/10], Step [208/938], Loss: 0.0853\n",
      "Epoch [1/10], Step [210/938], Loss: 0.1293\n",
      "Epoch [1/10], Step [212/938], Loss: 0.1904\n",
      "Epoch [1/10], Step [214/938], Loss: 0.0638\n",
      "Epoch [1/10], Step [216/938], Loss: 0.0567\n",
      "Epoch [1/10], Step [218/938], Loss: 0.2023\n",
      "Epoch [1/10], Step [220/938], Loss: 0.0621\n",
      "Epoch [1/10], Step [222/938], Loss: 0.1113\n",
      "Epoch [1/10], Step [224/938], Loss: 0.1081\n",
      "Epoch [1/10], Step [226/938], Loss: 0.0655\n",
      "Epoch [1/10], Step [228/938], Loss: 0.1172\n",
      "Epoch [1/10], Step [230/938], Loss: 0.2922\n",
      "Epoch [1/10], Step [232/938], Loss: 0.3314\n",
      "Epoch [1/10], Step [234/938], Loss: 0.0532\n",
      "Epoch [1/10], Step [236/938], Loss: 0.0866\n",
      "Epoch [1/10], Step [238/938], Loss: 0.2813\n",
      "Epoch [1/10], Step [240/938], Loss: 0.0883\n",
      "Epoch [1/10], Step [242/938], Loss: 0.1532\n",
      "Epoch [1/10], Step [244/938], Loss: 0.1050\n",
      "Epoch [1/10], Step [246/938], Loss: 0.1939\n",
      "Epoch [1/10], Step [248/938], Loss: 0.0520\n",
      "Epoch [1/10], Step [250/938], Loss: 0.1594\n",
      "Epoch [1/10], Step [252/938], Loss: 0.0634\n",
      "Epoch [1/10], Step [254/938], Loss: 0.0463\n",
      "Epoch [1/10], Step [256/938], Loss: 0.0892\n",
      "Epoch [1/10], Step [258/938], Loss: 0.1200\n",
      "Epoch [1/10], Step [260/938], Loss: 0.1108\n",
      "Epoch [1/10], Step [262/938], Loss: 0.0518\n",
      "Epoch [1/10], Step [264/938], Loss: 0.0311\n",
      "Epoch [1/10], Step [266/938], Loss: 0.1278\n",
      "Epoch [1/10], Step [268/938], Loss: 0.0681\n",
      "Epoch [1/10], Step [270/938], Loss: 0.1183\n",
      "Epoch [1/10], Step [272/938], Loss: 0.1448\n",
      "Epoch [1/10], Step [274/938], Loss: 0.0596\n",
      "Epoch [1/10], Step [276/938], Loss: 0.0850\n",
      "Epoch [1/10], Step [278/938], Loss: 0.2005\n",
      "Epoch [1/10], Step [280/938], Loss: 0.0796\n",
      "Epoch [1/10], Step [282/938], Loss: 0.0382\n",
      "Epoch [1/10], Step [284/938], Loss: 0.1908\n",
      "Epoch [1/10], Step [286/938], Loss: 0.0801\n",
      "Epoch [1/10], Step [288/938], Loss: 0.0959\n",
      "Epoch [1/10], Step [290/938], Loss: 0.0614\n",
      "Epoch [1/10], Step [292/938], Loss: 0.0636\n",
      "Epoch [1/10], Step [294/938], Loss: 0.0791\n",
      "Epoch [1/10], Step [296/938], Loss: 0.0617\n",
      "Epoch [1/10], Step [298/938], Loss: 0.0760\n",
      "Epoch [1/10], Step [300/938], Loss: 0.1048\n",
      "Epoch [1/10], Step [302/938], Loss: 0.2819\n",
      "Epoch [1/10], Step [304/938], Loss: 0.1580\n",
      "Epoch [1/10], Step [306/938], Loss: 0.0914\n",
      "Epoch [1/10], Step [308/938], Loss: 0.1225\n",
      "Epoch [1/10], Step [310/938], Loss: 0.0469\n",
      "Epoch [1/10], Step [312/938], Loss: 0.2944\n",
      "Epoch [1/10], Step [314/938], Loss: 0.1725\n",
      "Epoch [1/10], Step [316/938], Loss: 0.1051\n",
      "Epoch [1/10], Step [318/938], Loss: 0.0674\n",
      "Epoch [1/10], Step [320/938], Loss: 0.0948\n",
      "Epoch [1/10], Step [322/938], Loss: 0.0605\n",
      "Epoch [1/10], Step [324/938], Loss: 0.0306\n",
      "Epoch [1/10], Step [326/938], Loss: 0.0413\n",
      "Epoch [1/10], Step [328/938], Loss: 0.1274\n",
      "Epoch [1/10], Step [330/938], Loss: 0.0961\n",
      "Epoch [1/10], Step [332/938], Loss: 0.0315\n",
      "Epoch [1/10], Step [334/938], Loss: 0.0732\n",
      "Epoch [1/10], Step [336/938], Loss: 0.1518\n",
      "Epoch [1/10], Step [338/938], Loss: 0.1010\n",
      "Epoch [1/10], Step [340/938], Loss: 0.1337\n",
      "Epoch [1/10], Step [342/938], Loss: 0.0847\n",
      "Epoch [1/10], Step [344/938], Loss: 0.0876\n",
      "Epoch [1/10], Step [346/938], Loss: 0.0672\n",
      "Epoch [1/10], Step [348/938], Loss: 0.1994\n",
      "Epoch [1/10], Step [350/938], Loss: 0.0706\n",
      "Epoch [1/10], Step [352/938], Loss: 0.2267\n",
      "Epoch [1/10], Step [354/938], Loss: 0.0581\n",
      "Epoch [1/10], Step [356/938], Loss: 0.0479\n",
      "Epoch [1/10], Step [358/938], Loss: 0.1203\n",
      "Epoch [1/10], Step [360/938], Loss: 0.0980\n",
      "Epoch [1/10], Step [362/938], Loss: 0.0794\n",
      "Epoch [1/10], Step [364/938], Loss: 0.0649\n",
      "Epoch [1/10], Step [366/938], Loss: 0.0276\n",
      "Epoch [1/10], Step [368/938], Loss: 0.1505\n",
      "Epoch [1/10], Step [370/938], Loss: 0.1020\n",
      "Epoch [1/10], Step [372/938], Loss: 0.1373\n",
      "Epoch [1/10], Step [374/938], Loss: 0.0925\n",
      "Epoch [1/10], Step [376/938], Loss: 0.1901\n",
      "Epoch [1/10], Step [378/938], Loss: 0.0517\n",
      "Epoch [1/10], Step [380/938], Loss: 0.0143\n",
      "Epoch [1/10], Step [382/938], Loss: 0.0963\n",
      "Epoch [1/10], Step [384/938], Loss: 0.0497\n",
      "Epoch [1/10], Step [386/938], Loss: 0.0674\n",
      "Epoch [1/10], Step [388/938], Loss: 0.1545\n",
      "Epoch [1/10], Step [390/938], Loss: 0.1714\n",
      "Epoch [1/10], Step [392/938], Loss: 0.0128\n",
      "Epoch [1/10], Step [394/938], Loss: 0.0949\n",
      "Epoch [1/10], Step [396/938], Loss: 0.0140\n",
      "Epoch [1/10], Step [398/938], Loss: 0.0207\n",
      "Epoch [1/10], Step [400/938], Loss: 0.0699\n",
      "Epoch [1/10], Step [402/938], Loss: 0.0746\n",
      "Epoch [1/10], Step [404/938], Loss: 0.0986\n",
      "Epoch [1/10], Step [406/938], Loss: 0.1342\n",
      "Epoch [1/10], Step [408/938], Loss: 0.2155\n",
      "Epoch [1/10], Step [410/938], Loss: 0.1235\n",
      "Epoch [1/10], Step [412/938], Loss: 0.0439\n",
      "Epoch [1/10], Step [414/938], Loss: 0.0171\n",
      "Epoch [1/10], Step [416/938], Loss: 0.1110\n",
      "Epoch [1/10], Step [418/938], Loss: 0.0580\n",
      "Epoch [1/10], Step [420/938], Loss: 0.0978\n",
      "Epoch [1/10], Step [422/938], Loss: 0.0566\n",
      "Epoch [1/10], Step [424/938], Loss: 0.0775\n",
      "Epoch [1/10], Step [426/938], Loss: 0.1046\n",
      "Epoch [1/10], Step [428/938], Loss: 0.1562\n",
      "Epoch [1/10], Step [430/938], Loss: 0.2332\n",
      "Epoch [1/10], Step [432/938], Loss: 0.0792\n",
      "Epoch [1/10], Step [434/938], Loss: 0.0714\n",
      "Epoch [1/10], Step [436/938], Loss: 0.0979\n",
      "Epoch [1/10], Step [438/938], Loss: 0.0972\n",
      "Epoch [1/10], Step [440/938], Loss: 0.0751\n",
      "Epoch [1/10], Step [442/938], Loss: 0.1448\n",
      "Epoch [1/10], Step [444/938], Loss: 0.0777\n",
      "Epoch [1/10], Step [446/938], Loss: 0.1506\n",
      "Epoch [1/10], Step [448/938], Loss: 0.1474\n",
      "Epoch [1/10], Step [450/938], Loss: 0.0752\n",
      "Epoch [1/10], Step [452/938], Loss: 0.0340\n",
      "Epoch [1/10], Step [454/938], Loss: 0.0988\n",
      "Epoch [1/10], Step [456/938], Loss: 0.0349\n",
      "Epoch [1/10], Step [458/938], Loss: 0.1115\n",
      "Epoch [1/10], Step [460/938], Loss: 0.0252\n",
      "Epoch [1/10], Step [462/938], Loss: 0.0321\n",
      "Epoch [1/10], Step [464/938], Loss: 0.0247\n",
      "Epoch [1/10], Step [466/938], Loss: 0.0329\n",
      "Epoch [1/10], Step [468/938], Loss: 0.0869\n",
      "Epoch [1/10], Step [470/938], Loss: 0.0224\n",
      "Epoch [1/10], Step [472/938], Loss: 0.0644\n",
      "Epoch [1/10], Step [474/938], Loss: 0.0685\n",
      "Epoch [1/10], Step [476/938], Loss: 0.0285\n",
      "Epoch [1/10], Step [478/938], Loss: 0.2936\n",
      "Epoch [1/10], Step [480/938], Loss: 0.0479\n",
      "Epoch [1/10], Step [482/938], Loss: 0.1141\n",
      "Epoch [1/10], Step [484/938], Loss: 0.0812\n",
      "Epoch [1/10], Step [486/938], Loss: 0.0659\n",
      "Epoch [1/10], Step [488/938], Loss: 0.0438\n",
      "Epoch [1/10], Step [490/938], Loss: 0.1662\n",
      "Epoch [1/10], Step [492/938], Loss: 0.1184\n",
      "Epoch [1/10], Step [494/938], Loss: 0.1074\n",
      "Epoch [1/10], Step [496/938], Loss: 0.0833\n",
      "Epoch [1/10], Step [498/938], Loss: 0.0652\n",
      "Epoch [1/10], Step [500/938], Loss: 0.0501\n",
      "Epoch [1/10], Step [502/938], Loss: 0.2014\n",
      "Epoch [1/10], Step [504/938], Loss: 0.0236\n",
      "Epoch [1/10], Step [506/938], Loss: 0.0744\n",
      "Epoch [1/10], Step [508/938], Loss: 0.1068\n",
      "Epoch [1/10], Step [510/938], Loss: 0.0130\n",
      "Epoch [1/10], Step [512/938], Loss: 0.0861\n",
      "Epoch [1/10], Step [514/938], Loss: 0.0324\n",
      "Epoch [1/10], Step [516/938], Loss: 0.0536\n",
      "Epoch [1/10], Step [518/938], Loss: 0.0679\n",
      "Epoch [1/10], Step [520/938], Loss: 0.1668\n",
      "Epoch [1/10], Step [522/938], Loss: 0.0143\n",
      "Epoch [1/10], Step [524/938], Loss: 0.1141\n",
      "Epoch [1/10], Step [526/938], Loss: 0.0237\n",
      "Epoch [1/10], Step [528/938], Loss: 0.1723\n",
      "Epoch [1/10], Step [530/938], Loss: 0.1159\n",
      "Epoch [1/10], Step [532/938], Loss: 0.1310\n",
      "Epoch [1/10], Step [534/938], Loss: 0.0814\n",
      "Epoch [1/10], Step [536/938], Loss: 0.0324\n",
      "Epoch [1/10], Step [538/938], Loss: 0.1083\n",
      "Epoch [1/10], Step [540/938], Loss: 0.1067\n",
      "Epoch [1/10], Step [542/938], Loss: 0.0450\n",
      "Epoch [1/10], Step [544/938], Loss: 0.0527\n",
      "Epoch [1/10], Step [546/938], Loss: 0.0971\n",
      "Epoch [1/10], Step [548/938], Loss: 0.0682\n",
      "Epoch [1/10], Step [550/938], Loss: 0.0330\n",
      "Epoch [1/10], Step [552/938], Loss: 0.1507\n",
      "Epoch [1/10], Step [554/938], Loss: 0.1633\n",
      "Epoch [1/10], Step [556/938], Loss: 0.1047\n",
      "Epoch [1/10], Step [558/938], Loss: 0.0342\n",
      "Epoch [1/10], Step [560/938], Loss: 0.0176\n",
      "Epoch [1/10], Step [562/938], Loss: 0.0341\n",
      "Epoch [1/10], Step [564/938], Loss: 0.3050\n",
      "Epoch [1/10], Step [566/938], Loss: 0.0967\n",
      "Epoch [1/10], Step [568/938], Loss: 0.0512\n",
      "Epoch [1/10], Step [570/938], Loss: 0.0366\n",
      "Epoch [1/10], Step [572/938], Loss: 0.0266\n",
      "Epoch [1/10], Step [574/938], Loss: 0.0162\n",
      "Epoch [1/10], Step [576/938], Loss: 0.0514\n",
      "Epoch [1/10], Step [578/938], Loss: 0.0617\n",
      "Epoch [1/10], Step [580/938], Loss: 0.1145\n",
      "Epoch [1/10], Step [582/938], Loss: 0.0645\n",
      "Epoch [1/10], Step [584/938], Loss: 0.0373\n",
      "Epoch [1/10], Step [586/938], Loss: 0.0525\n",
      "Epoch [1/10], Step [588/938], Loss: 0.0304\n",
      "Epoch [1/10], Step [590/938], Loss: 0.0600\n",
      "Epoch [1/10], Step [592/938], Loss: 0.1588\n",
      "Epoch [1/10], Step [594/938], Loss: 0.0631\n",
      "Epoch [1/10], Step [596/938], Loss: 0.0610\n",
      "Epoch [1/10], Step [598/938], Loss: 0.0129\n",
      "Epoch [1/10], Step [600/938], Loss: 0.0587\n",
      "Epoch [1/10], Step [602/938], Loss: 0.1431\n",
      "Epoch [1/10], Step [604/938], Loss: 0.1092\n",
      "Epoch [1/10], Step [606/938], Loss: 0.0571\n",
      "Epoch [1/10], Step [608/938], Loss: 0.0955\n",
      "Epoch [1/10], Step [610/938], Loss: 0.0291\n",
      "Epoch [1/10], Step [612/938], Loss: 0.0639\n",
      "Epoch [1/10], Step [614/938], Loss: 0.0298\n",
      "Epoch [1/10], Step [616/938], Loss: 0.0922\n",
      "Epoch [1/10], Step [618/938], Loss: 0.0428\n",
      "Epoch [1/10], Step [620/938], Loss: 0.0599\n",
      "Epoch [1/10], Step [622/938], Loss: 0.0249\n",
      "Epoch [1/10], Step [624/938], Loss: 0.0715\n",
      "Epoch [1/10], Step [626/938], Loss: 0.0112\n",
      "Epoch [1/10], Step [628/938], Loss: 0.1250\n",
      "Epoch [1/10], Step [630/938], Loss: 0.0547\n",
      "Epoch [1/10], Step [632/938], Loss: 0.0700\n",
      "Epoch [1/10], Step [634/938], Loss: 0.0702\n",
      "Epoch [1/10], Step [636/938], Loss: 0.0263\n",
      "Epoch [1/10], Step [638/938], Loss: 0.0178\n",
      "Epoch [1/10], Step [640/938], Loss: 0.1062\n",
      "Epoch [1/10], Step [642/938], Loss: 0.0468\n",
      "Epoch [1/10], Step [644/938], Loss: 0.0100\n",
      "Epoch [1/10], Step [646/938], Loss: 0.0405\n",
      "Epoch [1/10], Step [648/938], Loss: 0.0132\n",
      "Epoch [1/10], Step [650/938], Loss: 0.0152\n",
      "Epoch [1/10], Step [652/938], Loss: 0.2064\n",
      "Epoch [1/10], Step [654/938], Loss: 0.0187\n",
      "Epoch [1/10], Step [656/938], Loss: 0.1838\n",
      "Epoch [1/10], Step [658/938], Loss: 0.0551\n",
      "Epoch [1/10], Step [660/938], Loss: 0.0234\n",
      "Epoch [1/10], Step [662/938], Loss: 0.0489\n",
      "Epoch [1/10], Step [664/938], Loss: 0.0787\n",
      "Epoch [1/10], Step [666/938], Loss: 0.0221\n",
      "Epoch [1/10], Step [668/938], Loss: 0.0408\n",
      "Epoch [1/10], Step [670/938], Loss: 0.0182\n",
      "Epoch [1/10], Step [672/938], Loss: 0.0777\n",
      "Epoch [1/10], Step [674/938], Loss: 0.0650\n",
      "Epoch [1/10], Step [676/938], Loss: 0.0320\n",
      "Epoch [1/10], Step [678/938], Loss: 0.0482\n",
      "Epoch [1/10], Step [680/938], Loss: 0.0915\n",
      "Epoch [1/10], Step [682/938], Loss: 0.0141\n",
      "Epoch [1/10], Step [684/938], Loss: 0.0726\n",
      "Epoch [1/10], Step [686/938], Loss: 0.1816\n",
      "Epoch [1/10], Step [688/938], Loss: 0.0119\n",
      "Epoch [1/10], Step [690/938], Loss: 0.1595\n",
      "Epoch [1/10], Step [692/938], Loss: 0.0605\n",
      "Epoch [1/10], Step [694/938], Loss: 0.0529\n",
      "Epoch [1/10], Step [696/938], Loss: 0.0698\n",
      "Epoch [1/10], Step [698/938], Loss: 0.0683\n",
      "Epoch [1/10], Step [700/938], Loss: 0.0100\n",
      "Epoch [1/10], Step [702/938], Loss: 0.0291\n",
      "Epoch [1/10], Step [704/938], Loss: 0.0168\n",
      "Epoch [1/10], Step [706/938], Loss: 0.0479\n",
      "Epoch [1/10], Step [708/938], Loss: 0.0148\n",
      "Epoch [1/10], Step [710/938], Loss: 0.0970\n",
      "Epoch [1/10], Step [712/938], Loss: 0.0112\n",
      "Epoch [1/10], Step [714/938], Loss: 0.0199\n",
      "Epoch [1/10], Step [716/938], Loss: 0.0327\n",
      "Epoch [1/10], Step [718/938], Loss: 0.0381\n",
      "Epoch [1/10], Step [720/938], Loss: 0.1204\n",
      "Epoch [1/10], Step [722/938], Loss: 0.0564\n",
      "Epoch [1/10], Step [724/938], Loss: 0.0539\n",
      "Epoch [1/10], Step [726/938], Loss: 0.0605\n",
      "Epoch [1/10], Step [728/938], Loss: 0.0454\n",
      "Epoch [1/10], Step [730/938], Loss: 0.0370\n",
      "Epoch [1/10], Step [732/938], Loss: 0.0861\n",
      "Epoch [1/10], Step [734/938], Loss: 0.0071\n",
      "Epoch [1/10], Step [736/938], Loss: 0.0073\n",
      "Epoch [1/10], Step [738/938], Loss: 0.0139\n",
      "Epoch [1/10], Step [740/938], Loss: 0.0365\n",
      "Epoch [1/10], Step [742/938], Loss: 0.0488\n",
      "Epoch [1/10], Step [744/938], Loss: 0.0888\n",
      "Epoch [1/10], Step [746/938], Loss: 0.0976\n",
      "Epoch [1/10], Step [748/938], Loss: 0.1266\n",
      "Epoch [1/10], Step [750/938], Loss: 0.0820\n",
      "Epoch [1/10], Step [752/938], Loss: 0.0267\n",
      "Epoch [1/10], Step [754/938], Loss: 0.0815\n",
      "Epoch [1/10], Step [756/938], Loss: 0.0721\n",
      "Epoch [1/10], Step [758/938], Loss: 0.1646\n",
      "Epoch [1/10], Step [760/938], Loss: 0.0127\n",
      "Epoch [1/10], Step [762/938], Loss: 0.0337\n",
      "Epoch [1/10], Step [764/938], Loss: 0.0566\n",
      "Epoch [1/10], Step [766/938], Loss: 0.1074\n",
      "Epoch [1/10], Step [768/938], Loss: 0.1339\n",
      "Epoch [1/10], Step [770/938], Loss: 0.0580\n",
      "Epoch [1/10], Step [772/938], Loss: 0.0249\n",
      "Epoch [1/10], Step [774/938], Loss: 0.0598\n",
      "Epoch [1/10], Step [776/938], Loss: 0.0321\n",
      "Epoch [1/10], Step [778/938], Loss: 0.0541\n",
      "Epoch [1/10], Step [780/938], Loss: 0.0256\n",
      "Epoch [1/10], Step [782/938], Loss: 0.0170\n",
      "Epoch [1/10], Step [784/938], Loss: 0.1159\n",
      "Epoch [1/10], Step [786/938], Loss: 0.0505\n",
      "Epoch [1/10], Step [788/938], Loss: 0.0551\n",
      "Epoch [1/10], Step [790/938], Loss: 0.0174\n",
      "Epoch [1/10], Step [792/938], Loss: 0.0588\n",
      "Epoch [1/10], Step [794/938], Loss: 0.0427\n",
      "Epoch [1/10], Step [796/938], Loss: 0.0594\n",
      "Epoch [1/10], Step [798/938], Loss: 0.0125\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0383\n",
      "Epoch [1/10], Step [802/938], Loss: 0.1015\n",
      "Epoch [1/10], Step [804/938], Loss: 0.0877\n",
      "Epoch [1/10], Step [806/938], Loss: 0.0251\n",
      "Epoch [1/10], Step [808/938], Loss: 0.0369\n",
      "Epoch [1/10], Step [810/938], Loss: 0.0048\n",
      "Epoch [1/10], Step [812/938], Loss: 0.0450\n",
      "Epoch [1/10], Step [814/938], Loss: 0.0872\n",
      "Epoch [1/10], Step [816/938], Loss: 0.0058\n",
      "Epoch [1/10], Step [818/938], Loss: 0.0361\n",
      "Epoch [1/10], Step [820/938], Loss: 0.1161\n",
      "Epoch [1/10], Step [822/938], Loss: 0.0823\n",
      "Epoch [1/10], Step [824/938], Loss: 0.1188\n",
      "Epoch [1/10], Step [826/938], Loss: 0.0175\n",
      "Epoch [1/10], Step [828/938], Loss: 0.0239\n",
      "Epoch [1/10], Step [830/938], Loss: 0.0784\n",
      "Epoch [1/10], Step [832/938], Loss: 0.0744\n",
      "Epoch [1/10], Step [834/938], Loss: 0.0755\n",
      "Epoch [1/10], Step [836/938], Loss: 0.0123\n",
      "Epoch [1/10], Step [838/938], Loss: 0.0273\n",
      "Epoch [1/10], Step [840/938], Loss: 0.1846\n",
      "Epoch [1/10], Step [842/938], Loss: 0.1429\n",
      "Epoch [1/10], Step [844/938], Loss: 0.0188\n",
      "Epoch [1/10], Step [846/938], Loss: 0.0936\n",
      "Epoch [1/10], Step [848/938], Loss: 0.1233\n",
      "Epoch [1/10], Step [850/938], Loss: 0.0608\n",
      "Epoch [1/10], Step [852/938], Loss: 0.0154\n",
      "Epoch [1/10], Step [854/938], Loss: 0.0550\n",
      "Epoch [1/10], Step [856/938], Loss: 0.0724\n",
      "Epoch [1/10], Step [858/938], Loss: 0.2029\n",
      "Epoch [1/10], Step [860/938], Loss: 0.0322\n",
      "Epoch [1/10], Step [862/938], Loss: 0.0223\n",
      "Epoch [1/10], Step [864/938], Loss: 0.0214\n",
      "Epoch [1/10], Step [866/938], Loss: 0.0829\n",
      "Epoch [1/10], Step [868/938], Loss: 0.0585\n",
      "Epoch [1/10], Step [870/938], Loss: 0.0511\n",
      "Epoch [1/10], Step [872/938], Loss: 0.0148\n",
      "Epoch [1/10], Step [874/938], Loss: 0.0587\n",
      "Epoch [1/10], Step [876/938], Loss: 0.0305\n",
      "Epoch [1/10], Step [878/938], Loss: 0.0273\n",
      "Epoch [1/10], Step [880/938], Loss: 0.0044\n",
      "Epoch [1/10], Step [882/938], Loss: 0.0932\n",
      "Epoch [1/10], Step [884/938], Loss: 0.0524\n",
      "Epoch [1/10], Step [886/938], Loss: 0.0564\n",
      "Epoch [1/10], Step [888/938], Loss: 0.0069\n",
      "Epoch [1/10], Step [890/938], Loss: 0.0749\n",
      "Epoch [1/10], Step [892/938], Loss: 0.0178\n",
      "Epoch [1/10], Step [894/938], Loss: 0.0295\n",
      "Epoch [1/10], Step [896/938], Loss: 0.0586\n",
      "Epoch [1/10], Step [898/938], Loss: 0.1395\n",
      "Epoch [1/10], Step [900/938], Loss: 0.0284\n",
      "Epoch [1/10], Step [902/938], Loss: 0.0196\n",
      "Epoch [1/10], Step [904/938], Loss: 0.0426\n",
      "Epoch [1/10], Step [906/938], Loss: 0.0670\n",
      "Epoch [1/10], Step [908/938], Loss: 0.0254\n",
      "Epoch [1/10], Step [910/938], Loss: 0.0703\n",
      "Epoch [1/10], Step [912/938], Loss: 0.0102\n",
      "Epoch [1/10], Step [914/938], Loss: 0.1097\n",
      "Epoch [1/10], Step [916/938], Loss: 0.0247\n",
      "Epoch [1/10], Step [918/938], Loss: 0.0525\n",
      "Epoch [1/10], Step [920/938], Loss: 0.0153\n",
      "Epoch [1/10], Step [922/938], Loss: 0.0656\n",
      "Epoch [1/10], Step [924/938], Loss: 0.0462\n",
      "Epoch [1/10], Step [926/938], Loss: 0.1695\n",
      "Epoch [1/10], Step [928/938], Loss: 0.0303\n",
      "Epoch [1/10], Step [930/938], Loss: 0.0355\n",
      "Epoch [1/10], Step [932/938], Loss: 0.0525\n",
      "Epoch [1/10], Step [934/938], Loss: 0.0212\n",
      "Epoch [1/10], Step [936/938], Loss: 0.1385\n",
      "Epoch [1/10], Step [938/938], Loss: 0.0650\n",
      "Epoch [1/10], Loss: 0.1487\n",
      "Epoch [2/10], Step [2/938], Loss: 0.0786\n",
      "Epoch [2/10], Step [4/938], Loss: 0.1196\n",
      "Epoch [2/10], Step [6/938], Loss: 0.0328\n",
      "Epoch [2/10], Step [8/938], Loss: 0.0140\n",
      "Epoch [2/10], Step [10/938], Loss: 0.0439\n",
      "Epoch [2/10], Step [12/938], Loss: 0.0270\n",
      "Epoch [2/10], Step [14/938], Loss: 0.0871\n",
      "Epoch [2/10], Step [16/938], Loss: 0.0156\n",
      "Epoch [2/10], Step [18/938], Loss: 0.0318\n",
      "Epoch [2/10], Step [20/938], Loss: 0.0617\n",
      "Epoch [2/10], Step [22/938], Loss: 0.0162\n",
      "Epoch [2/10], Step [24/938], Loss: 0.0059\n",
      "Epoch [2/10], Step [26/938], Loss: 0.0255\n",
      "Epoch [2/10], Step [28/938], Loss: 0.0540\n",
      "Epoch [2/10], Step [30/938], Loss: 0.0819\n",
      "Epoch [2/10], Step [32/938], Loss: 0.0375\n",
      "Epoch [2/10], Step [34/938], Loss: 0.0600\n",
      "Epoch [2/10], Step [36/938], Loss: 0.0354\n",
      "Epoch [2/10], Step [38/938], Loss: 0.0978\n",
      "Epoch [2/10], Step [40/938], Loss: 0.0250\n",
      "Epoch [2/10], Step [42/938], Loss: 0.0075\n",
      "Epoch [2/10], Step [44/938], Loss: 0.0129\n",
      "Epoch [2/10], Step [46/938], Loss: 0.0280\n",
      "Epoch [2/10], Step [48/938], Loss: 0.0323\n",
      "Epoch [2/10], Step [50/938], Loss: 0.0153\n",
      "Epoch [2/10], Step [52/938], Loss: 0.0491\n",
      "Epoch [2/10], Step [54/938], Loss: 0.0316\n",
      "Epoch [2/10], Step [56/938], Loss: 0.0891\n",
      "Epoch [2/10], Step [58/938], Loss: 0.0186\n",
      "Epoch [2/10], Step [60/938], Loss: 0.0118\n",
      "Epoch [2/10], Step [62/938], Loss: 0.0114\n",
      "Epoch [2/10], Step [64/938], Loss: 0.0448\n",
      "Epoch [2/10], Step [66/938], Loss: 0.0634\n",
      "Epoch [2/10], Step [68/938], Loss: 0.0117\n",
      "Epoch [2/10], Step [70/938], Loss: 0.0053\n",
      "Epoch [2/10], Step [72/938], Loss: 0.0529\n",
      "Epoch [2/10], Step [74/938], Loss: 0.1182\n",
      "Epoch [2/10], Step [76/938], Loss: 0.0680\n",
      "Epoch [2/10], Step [78/938], Loss: 0.0332\n",
      "Epoch [2/10], Step [80/938], Loss: 0.0257\n",
      "Epoch [2/10], Step [82/938], Loss: 0.2335\n",
      "Epoch [2/10], Step [84/938], Loss: 0.0109\n",
      "Epoch [2/10], Step [86/938], Loss: 0.0066\n",
      "Epoch [2/10], Step [88/938], Loss: 0.0449\n",
      "Epoch [2/10], Step [90/938], Loss: 0.1080\n",
      "Epoch [2/10], Step [92/938], Loss: 0.0086\n",
      "Epoch [2/10], Step [94/938], Loss: 0.0907\n",
      "Epoch [2/10], Step [96/938], Loss: 0.0523\n",
      "Epoch [2/10], Step [98/938], Loss: 0.0329\n",
      "Epoch [2/10], Step [100/938], Loss: 0.0353\n",
      "Epoch [2/10], Step [102/938], Loss: 0.0306\n",
      "Epoch [2/10], Step [104/938], Loss: 0.0446\n",
      "Epoch [2/10], Step [106/938], Loss: 0.0767\n",
      "Epoch [2/10], Step [108/938], Loss: 0.0531\n",
      "Epoch [2/10], Step [110/938], Loss: 0.0164\n",
      "Epoch [2/10], Step [112/938], Loss: 0.0589\n",
      "Epoch [2/10], Step [114/938], Loss: 0.0180\n",
      "Epoch [2/10], Step [116/938], Loss: 0.0123\n",
      "Epoch [2/10], Step [118/938], Loss: 0.1466\n",
      "Epoch [2/10], Step [120/938], Loss: 0.0243\n",
      "Epoch [2/10], Step [122/938], Loss: 0.0449\n",
      "Epoch [2/10], Step [124/938], Loss: 0.0359\n",
      "Epoch [2/10], Step [126/938], Loss: 0.0165\n",
      "Epoch [2/10], Step [128/938], Loss: 0.0898\n",
      "Epoch [2/10], Step [130/938], Loss: 0.0310\n",
      "Epoch [2/10], Step [132/938], Loss: 0.0971\n",
      "Epoch [2/10], Step [134/938], Loss: 0.0581\n",
      "Epoch [2/10], Step [136/938], Loss: 0.0247\n",
      "Epoch [2/10], Step [138/938], Loss: 0.0590\n",
      "Epoch [2/10], Step [140/938], Loss: 0.0180\n",
      "Epoch [2/10], Step [142/938], Loss: 0.0720\n",
      "Epoch [2/10], Step [144/938], Loss: 0.0561\n",
      "Epoch [2/10], Step [146/938], Loss: 0.0046\n",
      "Epoch [2/10], Step [148/938], Loss: 0.0205\n",
      "Epoch [2/10], Step [150/938], Loss: 0.0162\n",
      "Epoch [2/10], Step [152/938], Loss: 0.0442\n",
      "Epoch [2/10], Step [154/938], Loss: 0.1625\n",
      "Epoch [2/10], Step [156/938], Loss: 0.0113\n",
      "Epoch [2/10], Step [158/938], Loss: 0.0248\n",
      "Epoch [2/10], Step [160/938], Loss: 0.0522\n",
      "Epoch [2/10], Step [162/938], Loss: 0.0326\n",
      "Epoch [2/10], Step [164/938], Loss: 0.0558\n",
      "Epoch [2/10], Step [166/938], Loss: 0.0273\n",
      "Epoch [2/10], Step [168/938], Loss: 0.0553\n",
      "Epoch [2/10], Step [170/938], Loss: 0.0610\n",
      "Epoch [2/10], Step [172/938], Loss: 0.0760\n",
      "Epoch [2/10], Step [174/938], Loss: 0.1097\n",
      "Epoch [2/10], Step [176/938], Loss: 0.1346\n",
      "Epoch [2/10], Step [178/938], Loss: 0.0269\n",
      "Epoch [2/10], Step [180/938], Loss: 0.0130\n",
      "Epoch [2/10], Step [182/938], Loss: 0.0887\n",
      "Epoch [2/10], Step [184/938], Loss: 0.1995\n",
      "Epoch [2/10], Step [186/938], Loss: 0.1179\n",
      "Epoch [2/10], Step [188/938], Loss: 0.0743\n",
      "Epoch [2/10], Step [190/938], Loss: 0.0535\n",
      "Epoch [2/10], Step [192/938], Loss: 0.0757\n",
      "Epoch [2/10], Step [194/938], Loss: 0.0108\n",
      "Epoch [2/10], Step [196/938], Loss: 0.0542\n",
      "Epoch [2/10], Step [198/938], Loss: 0.0683\n",
      "Epoch [2/10], Step [200/938], Loss: 0.0212\n",
      "Epoch [2/10], Step [202/938], Loss: 0.0356\n",
      "Epoch [2/10], Step [204/938], Loss: 0.0157\n",
      "Epoch [2/10], Step [206/938], Loss: 0.0245\n",
      "Epoch [2/10], Step [208/938], Loss: 0.0299\n",
      "Epoch [2/10], Step [210/938], Loss: 0.0856\n",
      "Epoch [2/10], Step [212/938], Loss: 0.0493\n",
      "Epoch [2/10], Step [214/938], Loss: 0.0790\n",
      "Epoch [2/10], Step [216/938], Loss: 0.0460\n",
      "Epoch [2/10], Step [218/938], Loss: 0.0202\n",
      "Epoch [2/10], Step [220/938], Loss: 0.0418\n",
      "Epoch [2/10], Step [222/938], Loss: 0.0165\n",
      "Epoch [2/10], Step [224/938], Loss: 0.0085\n",
      "Epoch [2/10], Step [226/938], Loss: 0.0029\n",
      "Epoch [2/10], Step [228/938], Loss: 0.0138\n",
      "Epoch [2/10], Step [230/938], Loss: 0.0033\n",
      "Epoch [2/10], Step [232/938], Loss: 0.0104\n",
      "Epoch [2/10], Step [234/938], Loss: 0.0052\n",
      "Epoch [2/10], Step [236/938], Loss: 0.0577\n",
      "Epoch [2/10], Step [238/938], Loss: 0.0492\n",
      "Epoch [2/10], Step [240/938], Loss: 0.0584\n",
      "Epoch [2/10], Step [242/938], Loss: 0.1507\n",
      "Epoch [2/10], Step [244/938], Loss: 0.0527\n",
      "Epoch [2/10], Step [246/938], Loss: 0.0278\n",
      "Epoch [2/10], Step [248/938], Loss: 0.0129\n",
      "Epoch [2/10], Step [250/938], Loss: 0.0524\n",
      "Epoch [2/10], Step [252/938], Loss: 0.0675\n",
      "Epoch [2/10], Step [254/938], Loss: 0.0719\n",
      "Epoch [2/10], Step [256/938], Loss: 0.0511\n",
      "Epoch [2/10], Step [258/938], Loss: 0.0360\n",
      "Epoch [2/10], Step [260/938], Loss: 0.0434\n",
      "Epoch [2/10], Step [262/938], Loss: 0.0803\n",
      "Epoch [2/10], Step [264/938], Loss: 0.0297\n",
      "Epoch [2/10], Step [266/938], Loss: 0.0410\n",
      "Epoch [2/10], Step [268/938], Loss: 0.0100\n",
      "Epoch [2/10], Step [270/938], Loss: 0.0135\n",
      "Epoch [2/10], Step [272/938], Loss: 0.0263\n",
      "Epoch [2/10], Step [274/938], Loss: 0.0145\n",
      "Epoch [2/10], Step [276/938], Loss: 0.1271\n",
      "Epoch [2/10], Step [278/938], Loss: 0.0332\n",
      "Epoch [2/10], Step [280/938], Loss: 0.0487\n",
      "Epoch [2/10], Step [282/938], Loss: 0.0077\n",
      "Epoch [2/10], Step [284/938], Loss: 0.1137\n",
      "Epoch [2/10], Step [286/938], Loss: 0.2465\n",
      "Epoch [2/10], Step [288/938], Loss: 0.0179\n",
      "Epoch [2/10], Step [290/938], Loss: 0.1262\n",
      "Epoch [2/10], Step [292/938], Loss: 0.1738\n",
      "Epoch [2/10], Step [294/938], Loss: 0.0124\n",
      "Epoch [2/10], Step [296/938], Loss: 0.0821\n",
      "Epoch [2/10], Step [298/938], Loss: 0.0169\n",
      "Epoch [2/10], Step [300/938], Loss: 0.0814\n",
      "Epoch [2/10], Step [302/938], Loss: 0.0705\n",
      "Epoch [2/10], Step [304/938], Loss: 0.0359\n",
      "Epoch [2/10], Step [306/938], Loss: 0.0397\n",
      "Epoch [2/10], Step [308/938], Loss: 0.0864\n",
      "Epoch [2/10], Step [310/938], Loss: 0.0111\n",
      "Epoch [2/10], Step [312/938], Loss: 0.0349\n",
      "Epoch [2/10], Step [314/938], Loss: 0.0989\n",
      "Epoch [2/10], Step [316/938], Loss: 0.0098\n",
      "Epoch [2/10], Step [318/938], Loss: 0.0216\n",
      "Epoch [2/10], Step [320/938], Loss: 0.0307\n",
      "Epoch [2/10], Step [322/938], Loss: 0.0800\n",
      "Epoch [2/10], Step [324/938], Loss: 0.0240\n",
      "Epoch [2/10], Step [326/938], Loss: 0.0633\n",
      "Epoch [2/10], Step [328/938], Loss: 0.1770\n",
      "Epoch [2/10], Step [330/938], Loss: 0.0026\n",
      "Epoch [2/10], Step [332/938], Loss: 0.0062\n",
      "Epoch [2/10], Step [334/938], Loss: 0.0134\n",
      "Epoch [2/10], Step [336/938], Loss: 0.0072\n",
      "Epoch [2/10], Step [338/938], Loss: 0.0158\n",
      "Epoch [2/10], Step [340/938], Loss: 0.1012\n",
      "Epoch [2/10], Step [342/938], Loss: 0.0127\n",
      "Epoch [2/10], Step [344/938], Loss: 0.0588\n",
      "Epoch [2/10], Step [346/938], Loss: 0.0439\n",
      "Epoch [2/10], Step [348/938], Loss: 0.0144\n",
      "Epoch [2/10], Step [350/938], Loss: 0.0087\n",
      "Epoch [2/10], Step [352/938], Loss: 0.0095\n",
      "Epoch [2/10], Step [354/938], Loss: 0.1946\n",
      "Epoch [2/10], Step [356/938], Loss: 0.1137\n",
      "Epoch [2/10], Step [358/938], Loss: 0.0628\n",
      "Epoch [2/10], Step [360/938], Loss: 0.0196\n",
      "Epoch [2/10], Step [362/938], Loss: 0.0551\n",
      "Epoch [2/10], Step [364/938], Loss: 0.0577\n",
      "Epoch [2/10], Step [366/938], Loss: 0.0057\n",
      "Epoch [2/10], Step [368/938], Loss: 0.0154\n",
      "Epoch [2/10], Step [370/938], Loss: 0.0763\n",
      "Epoch [2/10], Step [372/938], Loss: 0.0609\n",
      "Epoch [2/10], Step [374/938], Loss: 0.0335\n",
      "Epoch [2/10], Step [376/938], Loss: 0.0294\n",
      "Epoch [2/10], Step [378/938], Loss: 0.0372\n",
      "Epoch [2/10], Step [380/938], Loss: 0.0194\n",
      "Epoch [2/10], Step [382/938], Loss: 0.0050\n",
      "Epoch [2/10], Step [384/938], Loss: 0.0056\n",
      "Epoch [2/10], Step [386/938], Loss: 0.0649\n",
      "Epoch [2/10], Step [388/938], Loss: 0.1404\n",
      "Epoch [2/10], Step [390/938], Loss: 0.0265\n",
      "Epoch [2/10], Step [392/938], Loss: 0.0759\n",
      "Epoch [2/10], Step [394/938], Loss: 0.0017\n",
      "Epoch [2/10], Step [396/938], Loss: 0.0410\n",
      "Epoch [2/10], Step [398/938], Loss: 0.0354\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1012\n",
      "Epoch [2/10], Step [402/938], Loss: 0.0306\n",
      "Epoch [2/10], Step [404/938], Loss: 0.1003\n",
      "Epoch [2/10], Step [406/938], Loss: 0.0126\n",
      "Epoch [2/10], Step [408/938], Loss: 0.0545\n",
      "Epoch [2/10], Step [410/938], Loss: 0.0298\n",
      "Epoch [2/10], Step [412/938], Loss: 0.0196\n",
      "Epoch [2/10], Step [414/938], Loss: 0.0836\n",
      "Epoch [2/10], Step [416/938], Loss: 0.0391\n",
      "Epoch [2/10], Step [418/938], Loss: 0.0555\n",
      "Epoch [2/10], Step [420/938], Loss: 0.0130\n",
      "Epoch [2/10], Step [422/938], Loss: 0.0171\n",
      "Epoch [2/10], Step [424/938], Loss: 0.1057\n",
      "Epoch [2/10], Step [426/938], Loss: 0.0602\n",
      "Epoch [2/10], Step [428/938], Loss: 0.0260\n",
      "Epoch [2/10], Step [430/938], Loss: 0.0234\n",
      "Epoch [2/10], Step [432/938], Loss: 0.0459\n",
      "Epoch [2/10], Step [434/938], Loss: 0.0133\n",
      "Epoch [2/10], Step [436/938], Loss: 0.0337\n",
      "Epoch [2/10], Step [438/938], Loss: 0.0604\n",
      "Epoch [2/10], Step [440/938], Loss: 0.0295\n",
      "Epoch [2/10], Step [442/938], Loss: 0.0472\n",
      "Epoch [2/10], Step [444/938], Loss: 0.0922\n",
      "Epoch [2/10], Step [446/938], Loss: 0.0820\n",
      "Epoch [2/10], Step [448/938], Loss: 0.0291\n",
      "Epoch [2/10], Step [450/938], Loss: 0.1015\n",
      "Epoch [2/10], Step [452/938], Loss: 0.0212\n",
      "Epoch [2/10], Step [454/938], Loss: 0.0181\n",
      "Epoch [2/10], Step [456/938], Loss: 0.0122\n",
      "Epoch [2/10], Step [458/938], Loss: 0.1022\n",
      "Epoch [2/10], Step [460/938], Loss: 0.0118\n",
      "Epoch [2/10], Step [462/938], Loss: 0.0383\n",
      "Epoch [2/10], Step [464/938], Loss: 0.0569\n",
      "Epoch [2/10], Step [466/938], Loss: 0.0424\n",
      "Epoch [2/10], Step [468/938], Loss: 0.0742\n",
      "Epoch [2/10], Step [470/938], Loss: 0.0557\n",
      "Epoch [2/10], Step [472/938], Loss: 0.0165\n",
      "Epoch [2/10], Step [474/938], Loss: 0.0456\n",
      "Epoch [2/10], Step [476/938], Loss: 0.0679\n",
      "Epoch [2/10], Step [478/938], Loss: 0.0624\n",
      "Epoch [2/10], Step [480/938], Loss: 0.1718\n",
      "Epoch [2/10], Step [482/938], Loss: 0.0113\n",
      "Epoch [2/10], Step [484/938], Loss: 0.1357\n",
      "Epoch [2/10], Step [486/938], Loss: 0.0477\n",
      "Epoch [2/10], Step [488/938], Loss: 0.0208\n",
      "Epoch [2/10], Step [490/938], Loss: 0.0790\n",
      "Epoch [2/10], Step [492/938], Loss: 0.0451\n",
      "Epoch [2/10], Step [494/938], Loss: 0.0072\n",
      "Epoch [2/10], Step [496/938], Loss: 0.0119\n",
      "Epoch [2/10], Step [498/938], Loss: 0.0415\n",
      "Epoch [2/10], Step [500/938], Loss: 0.0772\n",
      "Epoch [2/10], Step [502/938], Loss: 0.0038\n",
      "Epoch [2/10], Step [504/938], Loss: 0.0242\n",
      "Epoch [2/10], Step [506/938], Loss: 0.0164\n",
      "Epoch [2/10], Step [508/938], Loss: 0.0391\n",
      "Epoch [2/10], Step [510/938], Loss: 0.0991\n",
      "Epoch [2/10], Step [512/938], Loss: 0.0064\n",
      "Epoch [2/10], Step [514/938], Loss: 0.0228\n",
      "Epoch [2/10], Step [516/938], Loss: 0.0187\n",
      "Epoch [2/10], Step [518/938], Loss: 0.0228\n",
      "Epoch [2/10], Step [520/938], Loss: 0.0145\n",
      "Epoch [2/10], Step [522/938], Loss: 0.0778\n",
      "Epoch [2/10], Step [524/938], Loss: 0.0164\n",
      "Epoch [2/10], Step [526/938], Loss: 0.0061\n",
      "Epoch [2/10], Step [528/938], Loss: 0.0186\n",
      "Epoch [2/10], Step [530/938], Loss: 0.0158\n",
      "Epoch [2/10], Step [532/938], Loss: 0.0780\n",
      "Epoch [2/10], Step [534/938], Loss: 0.0699\n",
      "Epoch [2/10], Step [536/938], Loss: 0.0744\n",
      "Epoch [2/10], Step [538/938], Loss: 0.0271\n",
      "Epoch [2/10], Step [540/938], Loss: 0.0109\n",
      "Epoch [2/10], Step [542/938], Loss: 0.1193\n",
      "Epoch [2/10], Step [544/938], Loss: 0.0069\n",
      "Epoch [2/10], Step [546/938], Loss: 0.0032\n",
      "Epoch [2/10], Step [548/938], Loss: 0.0147\n",
      "Epoch [2/10], Step [550/938], Loss: 0.0015\n",
      "Epoch [2/10], Step [552/938], Loss: 0.0561\n",
      "Epoch [2/10], Step [554/938], Loss: 0.0419\n",
      "Epoch [2/10], Step [556/938], Loss: 0.0983\n",
      "Epoch [2/10], Step [558/938], Loss: 0.0230\n",
      "Epoch [2/10], Step [560/938], Loss: 0.0180\n",
      "Epoch [2/10], Step [562/938], Loss: 0.0368\n",
      "Epoch [2/10], Step [564/938], Loss: 0.0064\n",
      "Epoch [2/10], Step [566/938], Loss: 0.0444\n",
      "Epoch [2/10], Step [568/938], Loss: 0.0227\n",
      "Epoch [2/10], Step [570/938], Loss: 0.0328\n",
      "Epoch [2/10], Step [572/938], Loss: 0.0371\n",
      "Epoch [2/10], Step [574/938], Loss: 0.0049\n",
      "Epoch [2/10], Step [576/938], Loss: 0.0275\n",
      "Epoch [2/10], Step [578/938], Loss: 0.0453\n",
      "Epoch [2/10], Step [580/938], Loss: 0.0093\n",
      "Epoch [2/10], Step [582/938], Loss: 0.0507\n",
      "Epoch [2/10], Step [584/938], Loss: 0.0059\n",
      "Epoch [2/10], Step [586/938], Loss: 0.0474\n",
      "Epoch [2/10], Step [588/938], Loss: 0.0267\n",
      "Epoch [2/10], Step [590/938], Loss: 0.0419\n",
      "Epoch [2/10], Step [592/938], Loss: 0.0066\n",
      "Epoch [2/10], Step [594/938], Loss: 0.1280\n",
      "Epoch [2/10], Step [596/938], Loss: 0.0162\n",
      "Epoch [2/10], Step [598/938], Loss: 0.0344\n",
      "Epoch [2/10], Step [600/938], Loss: 0.0657\n",
      "Epoch [2/10], Step [602/938], Loss: 0.0661\n",
      "Epoch [2/10], Step [604/938], Loss: 0.0232\n",
      "Epoch [2/10], Step [606/938], Loss: 0.0079\n",
      "Epoch [2/10], Step [608/938], Loss: 0.0648\n",
      "Epoch [2/10], Step [610/938], Loss: 0.0114\n",
      "Epoch [2/10], Step [612/938], Loss: 0.0121\n",
      "Epoch [2/10], Step [614/938], Loss: 0.0047\n",
      "Epoch [2/10], Step [616/938], Loss: 0.0251\n",
      "Epoch [2/10], Step [618/938], Loss: 0.0169\n",
      "Epoch [2/10], Step [620/938], Loss: 0.0504\n",
      "Epoch [2/10], Step [622/938], Loss: 0.0401\n",
      "Epoch [2/10], Step [624/938], Loss: 0.0081\n",
      "Epoch [2/10], Step [626/938], Loss: 0.0677\n",
      "Epoch [2/10], Step [628/938], Loss: 0.0544\n",
      "Epoch [2/10], Step [630/938], Loss: 0.0083\n",
      "Epoch [2/10], Step [632/938], Loss: 0.0559\n",
      "Epoch [2/10], Step [634/938], Loss: 0.0624\n",
      "Epoch [2/10], Step [636/938], Loss: 0.1011\n",
      "Epoch [2/10], Step [638/938], Loss: 0.0238\n",
      "Epoch [2/10], Step [640/938], Loss: 0.0432\n",
      "Epoch [2/10], Step [642/938], Loss: 0.0870\n",
      "Epoch [2/10], Step [644/938], Loss: 0.0474\n",
      "Epoch [2/10], Step [646/938], Loss: 0.0651\n",
      "Epoch [2/10], Step [648/938], Loss: 0.0462\n",
      "Epoch [2/10], Step [650/938], Loss: 0.0676\n",
      "Epoch [2/10], Step [652/938], Loss: 0.0181\n",
      "Epoch [2/10], Step [654/938], Loss: 0.0478\n",
      "Epoch [2/10], Step [656/938], Loss: 0.0117\n",
      "Epoch [2/10], Step [658/938], Loss: 0.0471\n",
      "Epoch [2/10], Step [660/938], Loss: 0.0122\n",
      "Epoch [2/10], Step [662/938], Loss: 0.0132\n",
      "Epoch [2/10], Step [664/938], Loss: 0.0835\n",
      "Epoch [2/10], Step [666/938], Loss: 0.1315\n",
      "Epoch [2/10], Step [668/938], Loss: 0.0060\n",
      "Epoch [2/10], Step [670/938], Loss: 0.0996\n",
      "Epoch [2/10], Step [672/938], Loss: 0.0622\n",
      "Epoch [2/10], Step [674/938], Loss: 0.0030\n",
      "Epoch [2/10], Step [676/938], Loss: 0.1199\n",
      "Epoch [2/10], Step [678/938], Loss: 0.0517\n",
      "Epoch [2/10], Step [680/938], Loss: 0.1025\n",
      "Epoch [2/10], Step [682/938], Loss: 0.0045\n",
      "Epoch [2/10], Step [684/938], Loss: 0.0184\n",
      "Epoch [2/10], Step [686/938], Loss: 0.0309\n",
      "Epoch [2/10], Step [688/938], Loss: 0.0358\n",
      "Epoch [2/10], Step [690/938], Loss: 0.0210\n",
      "Epoch [2/10], Step [692/938], Loss: 0.1142\n",
      "Epoch [2/10], Step [694/938], Loss: 0.0471\n",
      "Epoch [2/10], Step [696/938], Loss: 0.0703\n",
      "Epoch [2/10], Step [698/938], Loss: 0.0453\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0249\n",
      "Epoch [2/10], Step [702/938], Loss: 0.0072\n",
      "Epoch [2/10], Step [704/938], Loss: 0.0706\n",
      "Epoch [2/10], Step [706/938], Loss: 0.0260\n",
      "Epoch [2/10], Step [708/938], Loss: 0.0743\n",
      "Epoch [2/10], Step [710/938], Loss: 0.1873\n",
      "Epoch [2/10], Step [712/938], Loss: 0.0417\n",
      "Epoch [2/10], Step [714/938], Loss: 0.0047\n",
      "Epoch [2/10], Step [716/938], Loss: 0.1182\n",
      "Epoch [2/10], Step [718/938], Loss: 0.0114\n",
      "Epoch [2/10], Step [720/938], Loss: 0.0702\n",
      "Epoch [2/10], Step [722/938], Loss: 0.0171\n",
      "Epoch [2/10], Step [724/938], Loss: 0.0116\n",
      "Epoch [2/10], Step [726/938], Loss: 0.0259\n",
      "Epoch [2/10], Step [728/938], Loss: 0.0091\n",
      "Epoch [2/10], Step [730/938], Loss: 0.0108\n",
      "Epoch [2/10], Step [732/938], Loss: 0.0056\n",
      "Epoch [2/10], Step [734/938], Loss: 0.1275\n",
      "Epoch [2/10], Step [736/938], Loss: 0.0053\n",
      "Epoch [2/10], Step [738/938], Loss: 0.0510\n",
      "Epoch [2/10], Step [740/938], Loss: 0.0092\n",
      "Epoch [2/10], Step [742/938], Loss: 0.0638\n",
      "Epoch [2/10], Step [744/938], Loss: 0.1209\n",
      "Epoch [2/10], Step [746/938], Loss: 0.0075\n",
      "Epoch [2/10], Step [748/938], Loss: 0.0255\n",
      "Epoch [2/10], Step [750/938], Loss: 0.0553\n",
      "Epoch [2/10], Step [752/938], Loss: 0.0260\n",
      "Epoch [2/10], Step [754/938], Loss: 0.0744\n",
      "Epoch [2/10], Step [756/938], Loss: 0.0037\n",
      "Epoch [2/10], Step [758/938], Loss: 0.0394\n",
      "Epoch [2/10], Step [760/938], Loss: 0.0052\n",
      "Epoch [2/10], Step [762/938], Loss: 0.0562\n",
      "Epoch [2/10], Step [764/938], Loss: 0.0137\n",
      "Epoch [2/10], Step [766/938], Loss: 0.0208\n",
      "Epoch [2/10], Step [768/938], Loss: 0.0225\n",
      "Epoch [2/10], Step [770/938], Loss: 0.1045\n",
      "Epoch [2/10], Step [772/938], Loss: 0.0609\n",
      "Epoch [2/10], Step [774/938], Loss: 0.0056\n",
      "Epoch [2/10], Step [776/938], Loss: 0.0103\n",
      "Epoch [2/10], Step [778/938], Loss: 0.0301\n",
      "Epoch [2/10], Step [780/938], Loss: 0.0023\n",
      "Epoch [2/10], Step [782/938], Loss: 0.0041\n",
      "Epoch [2/10], Step [784/938], Loss: 0.0554\n",
      "Epoch [2/10], Step [786/938], Loss: 0.0186\n",
      "Epoch [2/10], Step [788/938], Loss: 0.1091\n",
      "Epoch [2/10], Step [790/938], Loss: 0.0141\n",
      "Epoch [2/10], Step [792/938], Loss: 0.0154\n",
      "Epoch [2/10], Step [794/938], Loss: 0.1569\n",
      "Epoch [2/10], Step [796/938], Loss: 0.0063\n",
      "Epoch [2/10], Step [798/938], Loss: 0.0283\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0056\n",
      "Epoch [2/10], Step [802/938], Loss: 0.1044\n",
      "Epoch [2/10], Step [804/938], Loss: 0.0029\n",
      "Epoch [2/10], Step [806/938], Loss: 0.0456\n",
      "Epoch [2/10], Step [808/938], Loss: 0.0167\n",
      "Epoch [2/10], Step [810/938], Loss: 0.0299\n",
      "Epoch [2/10], Step [812/938], Loss: 0.0211\n",
      "Epoch [2/10], Step [814/938], Loss: 0.0046\n",
      "Epoch [2/10], Step [816/938], Loss: 0.0131\n",
      "Epoch [2/10], Step [818/938], Loss: 0.0108\n",
      "Epoch [2/10], Step [820/938], Loss: 0.0450\n",
      "Epoch [2/10], Step [822/938], Loss: 0.0194\n",
      "Epoch [2/10], Step [824/938], Loss: 0.0044\n",
      "Epoch [2/10], Step [826/938], Loss: 0.1112\n",
      "Epoch [2/10], Step [828/938], Loss: 0.1119\n",
      "Epoch [2/10], Step [830/938], Loss: 0.0155\n",
      "Epoch [2/10], Step [832/938], Loss: 0.0491\n",
      "Epoch [2/10], Step [834/938], Loss: 0.0201\n",
      "Epoch [2/10], Step [836/938], Loss: 0.0032\n",
      "Epoch [2/10], Step [838/938], Loss: 0.1063\n",
      "Epoch [2/10], Step [840/938], Loss: 0.1243\n",
      "Epoch [2/10], Step [842/938], Loss: 0.0374\n",
      "Epoch [2/10], Step [844/938], Loss: 0.0295\n",
      "Epoch [2/10], Step [846/938], Loss: 0.0122\n",
      "Epoch [2/10], Step [848/938], Loss: 0.0272\n",
      "Epoch [2/10], Step [850/938], Loss: 0.0590\n",
      "Epoch [2/10], Step [852/938], Loss: 0.0143\n",
      "Epoch [2/10], Step [854/938], Loss: 0.0420\n",
      "Epoch [2/10], Step [856/938], Loss: 0.0445\n",
      "Epoch [2/10], Step [858/938], Loss: 0.1121\n",
      "Epoch [2/10], Step [860/938], Loss: 0.0136\n",
      "Epoch [2/10], Step [862/938], Loss: 0.0026\n",
      "Epoch [2/10], Step [864/938], Loss: 0.0516\n",
      "Epoch [2/10], Step [866/938], Loss: 0.0382\n",
      "Epoch [2/10], Step [868/938], Loss: 0.0270\n",
      "Epoch [2/10], Step [870/938], Loss: 0.0094\n",
      "Epoch [2/10], Step [872/938], Loss: 0.0081\n",
      "Epoch [2/10], Step [874/938], Loss: 0.0255\n",
      "Epoch [2/10], Step [876/938], Loss: 0.2002\n",
      "Epoch [2/10], Step [878/938], Loss: 0.1192\n",
      "Epoch [2/10], Step [880/938], Loss: 0.0050\n",
      "Epoch [2/10], Step [882/938], Loss: 0.0203\n",
      "Epoch [2/10], Step [884/938], Loss: 0.0015\n",
      "Epoch [2/10], Step [886/938], Loss: 0.0061\n",
      "Epoch [2/10], Step [888/938], Loss: 0.0123\n",
      "Epoch [2/10], Step [890/938], Loss: 0.0230\n",
      "Epoch [2/10], Step [892/938], Loss: 0.0849\n",
      "Epoch [2/10], Step [894/938], Loss: 0.0814\n",
      "Epoch [2/10], Step [896/938], Loss: 0.0049\n",
      "Epoch [2/10], Step [898/938], Loss: 0.0045\n",
      "Epoch [2/10], Step [900/938], Loss: 0.0171\n",
      "Epoch [2/10], Step [902/938], Loss: 0.0039\n",
      "Epoch [2/10], Step [904/938], Loss: 0.2324\n",
      "Epoch [2/10], Step [906/938], Loss: 0.0187\n",
      "Epoch [2/10], Step [908/938], Loss: 0.0142\n",
      "Epoch [2/10], Step [910/938], Loss: 0.0057\n",
      "Epoch [2/10], Step [912/938], Loss: 0.0117\n",
      "Epoch [2/10], Step [914/938], Loss: 0.0127\n",
      "Epoch [2/10], Step [916/938], Loss: 0.0589\n",
      "Epoch [2/10], Step [918/938], Loss: 0.0144\n",
      "Epoch [2/10], Step [920/938], Loss: 0.0476\n",
      "Epoch [2/10], Step [922/938], Loss: 0.0099\n",
      "Epoch [2/10], Step [924/938], Loss: 0.0436\n",
      "Epoch [2/10], Step [926/938], Loss: 0.0012\n",
      "Epoch [2/10], Step [928/938], Loss: 0.0084\n",
      "Epoch [2/10], Step [930/938], Loss: 0.0058\n",
      "Epoch [2/10], Step [932/938], Loss: 0.0090\n",
      "Epoch [2/10], Step [934/938], Loss: 0.2299\n",
      "Epoch [2/10], Step [936/938], Loss: 0.0128\n",
      "Epoch [2/10], Step [938/938], Loss: 0.0406\n",
      "Epoch [2/10], Loss: 0.0456\n",
      "Epoch [3/10], Step [2/938], Loss: 0.0046\n",
      "Epoch [3/10], Step [4/938], Loss: 0.0317\n",
      "Epoch [3/10], Step [6/938], Loss: 0.0018\n",
      "Epoch [3/10], Step [8/938], Loss: 0.0025\n",
      "Epoch [3/10], Step [10/938], Loss: 0.0017\n",
      "Epoch [3/10], Step [12/938], Loss: 0.0253\n",
      "Epoch [3/10], Step [14/938], Loss: 0.0078\n",
      "Epoch [3/10], Step [16/938], Loss: 0.0366\n",
      "Epoch [3/10], Step [18/938], Loss: 0.0298\n",
      "Epoch [3/10], Step [20/938], Loss: 0.0036\n",
      "Epoch [3/10], Step [22/938], Loss: 0.0085\n",
      "Epoch [3/10], Step [24/938], Loss: 0.0121\n",
      "Epoch [3/10], Step [26/938], Loss: 0.0378\n",
      "Epoch [3/10], Step [28/938], Loss: 0.0057\n",
      "Epoch [3/10], Step [30/938], Loss: 0.1067\n",
      "Epoch [3/10], Step [32/938], Loss: 0.1302\n",
      "Epoch [3/10], Step [34/938], Loss: 0.0080\n",
      "Epoch [3/10], Step [36/938], Loss: 0.0436\n",
      "Epoch [3/10], Step [38/938], Loss: 0.0645\n",
      "Epoch [3/10], Step [40/938], Loss: 0.0116\n",
      "Epoch [3/10], Step [42/938], Loss: 0.0042\n",
      "Epoch [3/10], Step [44/938], Loss: 0.0077\n",
      "Epoch [3/10], Step [46/938], Loss: 0.0143\n",
      "Epoch [3/10], Step [48/938], Loss: 0.0765\n",
      "Epoch [3/10], Step [50/938], Loss: 0.0124\n",
      "Epoch [3/10], Step [52/938], Loss: 0.0037\n",
      "Epoch [3/10], Step [54/938], Loss: 0.0438\n",
      "Epoch [3/10], Step [56/938], Loss: 0.0061\n",
      "Epoch [3/10], Step [58/938], Loss: 0.0156\n",
      "Epoch [3/10], Step [60/938], Loss: 0.0036\n",
      "Epoch [3/10], Step [62/938], Loss: 0.0464\n",
      "Epoch [3/10], Step [64/938], Loss: 0.0246\n",
      "Epoch [3/10], Step [66/938], Loss: 0.0031\n",
      "Epoch [3/10], Step [68/938], Loss: 0.0137\n",
      "Epoch [3/10], Step [70/938], Loss: 0.0280\n",
      "Epoch [3/10], Step [72/938], Loss: 0.0019\n",
      "Epoch [3/10], Step [74/938], Loss: 0.0014\n",
      "Epoch [3/10], Step [76/938], Loss: 0.0121\n",
      "Epoch [3/10], Step [78/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [80/938], Loss: 0.0182\n",
      "Epoch [3/10], Step [82/938], Loss: 0.0365\n",
      "Epoch [3/10], Step [84/938], Loss: 0.0043\n",
      "Epoch [3/10], Step [86/938], Loss: 0.0510\n",
      "Epoch [3/10], Step [88/938], Loss: 0.0258\n",
      "Epoch [3/10], Step [90/938], Loss: 0.0138\n",
      "Epoch [3/10], Step [92/938], Loss: 0.0797\n",
      "Epoch [3/10], Step [94/938], Loss: 0.0254\n",
      "Epoch [3/10], Step [96/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [98/938], Loss: 0.0769\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0048\n",
      "Epoch [3/10], Step [102/938], Loss: 0.0404\n",
      "Epoch [3/10], Step [104/938], Loss: 0.0100\n",
      "Epoch [3/10], Step [106/938], Loss: 0.0171\n",
      "Epoch [3/10], Step [108/938], Loss: 0.0146\n",
      "Epoch [3/10], Step [110/938], Loss: 0.0519\n",
      "Epoch [3/10], Step [112/938], Loss: 0.0080\n",
      "Epoch [3/10], Step [114/938], Loss: 0.0500\n",
      "Epoch [3/10], Step [116/938], Loss: 0.0064\n",
      "Epoch [3/10], Step [118/938], Loss: 0.0376\n",
      "Epoch [3/10], Step [120/938], Loss: 0.0087\n",
      "Epoch [3/10], Step [122/938], Loss: 0.0211\n",
      "Epoch [3/10], Step [124/938], Loss: 0.0081\n",
      "Epoch [3/10], Step [126/938], Loss: 0.0136\n",
      "Epoch [3/10], Step [128/938], Loss: 0.0230\n",
      "Epoch [3/10], Step [130/938], Loss: 0.0128\n",
      "Epoch [3/10], Step [132/938], Loss: 0.0657\n",
      "Epoch [3/10], Step [134/938], Loss: 0.0129\n",
      "Epoch [3/10], Step [136/938], Loss: 0.0128\n",
      "Epoch [3/10], Step [138/938], Loss: 0.0175\n",
      "Epoch [3/10], Step [140/938], Loss: 0.0304\n",
      "Epoch [3/10], Step [142/938], Loss: 0.0191\n",
      "Epoch [3/10], Step [144/938], Loss: 0.0454\n",
      "Epoch [3/10], Step [146/938], Loss: 0.0109\n",
      "Epoch [3/10], Step [148/938], Loss: 0.1054\n",
      "Epoch [3/10], Step [150/938], Loss: 0.0859\n",
      "Epoch [3/10], Step [152/938], Loss: 0.0235\n",
      "Epoch [3/10], Step [154/938], Loss: 0.0135\n",
      "Epoch [3/10], Step [156/938], Loss: 0.0661\n",
      "Epoch [3/10], Step [158/938], Loss: 0.0267\n",
      "Epoch [3/10], Step [160/938], Loss: 0.0102\n",
      "Epoch [3/10], Step [162/938], Loss: 0.0293\n",
      "Epoch [3/10], Step [164/938], Loss: 0.0701\n",
      "Epoch [3/10], Step [166/938], Loss: 0.0097\n",
      "Epoch [3/10], Step [168/938], Loss: 0.0278\n",
      "Epoch [3/10], Step [170/938], Loss: 0.0539\n",
      "Epoch [3/10], Step [172/938], Loss: 0.0221\n",
      "Epoch [3/10], Step [174/938], Loss: 0.0320\n",
      "Epoch [3/10], Step [176/938], Loss: 0.0192\n",
      "Epoch [3/10], Step [178/938], Loss: 0.0041\n",
      "Epoch [3/10], Step [180/938], Loss: 0.0201\n",
      "Epoch [3/10], Step [182/938], Loss: 0.0036\n",
      "Epoch [3/10], Step [184/938], Loss: 0.0045\n",
      "Epoch [3/10], Step [186/938], Loss: 0.0057\n",
      "Epoch [3/10], Step [188/938], Loss: 0.0190\n",
      "Epoch [3/10], Step [190/938], Loss: 0.0422\n",
      "Epoch [3/10], Step [192/938], Loss: 0.0088\n",
      "Epoch [3/10], Step [194/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [196/938], Loss: 0.0706\n",
      "Epoch [3/10], Step [198/938], Loss: 0.0094\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0165\n",
      "Epoch [3/10], Step [202/938], Loss: 0.0087\n",
      "Epoch [3/10], Step [204/938], Loss: 0.0230\n",
      "Epoch [3/10], Step [206/938], Loss: 0.0263\n",
      "Epoch [3/10], Step [208/938], Loss: 0.0017\n",
      "Epoch [3/10], Step [210/938], Loss: 0.0086\n",
      "Epoch [3/10], Step [212/938], Loss: 0.0030\n",
      "Epoch [3/10], Step [214/938], Loss: 0.0055\n",
      "Epoch [3/10], Step [216/938], Loss: 0.0277\n",
      "Epoch [3/10], Step [218/938], Loss: 0.0453\n",
      "Epoch [3/10], Step [220/938], Loss: 0.0173\n",
      "Epoch [3/10], Step [222/938], Loss: 0.0031\n",
      "Epoch [3/10], Step [224/938], Loss: 0.0074\n",
      "Epoch [3/10], Step [226/938], Loss: 0.0226\n",
      "Epoch [3/10], Step [228/938], Loss: 0.0575\n",
      "Epoch [3/10], Step [230/938], Loss: 0.0171\n",
      "Epoch [3/10], Step [232/938], Loss: 0.0022\n",
      "Epoch [3/10], Step [234/938], Loss: 0.0161\n",
      "Epoch [3/10], Step [236/938], Loss: 0.0255\n",
      "Epoch [3/10], Step [238/938], Loss: 0.0510\n",
      "Epoch [3/10], Step [240/938], Loss: 0.0043\n",
      "Epoch [3/10], Step [242/938], Loss: 0.0184\n",
      "Epoch [3/10], Step [244/938], Loss: 0.0594\n",
      "Epoch [3/10], Step [246/938], Loss: 0.0092\n",
      "Epoch [3/10], Step [248/938], Loss: 0.0691\n",
      "Epoch [3/10], Step [250/938], Loss: 0.0250\n",
      "Epoch [3/10], Step [252/938], Loss: 0.0050\n",
      "Epoch [3/10], Step [254/938], Loss: 0.1463\n",
      "Epoch [3/10], Step [256/938], Loss: 0.0101\n",
      "Epoch [3/10], Step [258/938], Loss: 0.0070\n",
      "Epoch [3/10], Step [260/938], Loss: 0.0166\n",
      "Epoch [3/10], Step [262/938], Loss: 0.0130\n",
      "Epoch [3/10], Step [264/938], Loss: 0.0260\n",
      "Epoch [3/10], Step [266/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [268/938], Loss: 0.0499\n",
      "Epoch [3/10], Step [270/938], Loss: 0.0032\n",
      "Epoch [3/10], Step [272/938], Loss: 0.0093\n",
      "Epoch [3/10], Step [274/938], Loss: 0.0605\n",
      "Epoch [3/10], Step [276/938], Loss: 0.0684\n",
      "Epoch [3/10], Step [278/938], Loss: 0.0577\n",
      "Epoch [3/10], Step [280/938], Loss: 0.0328\n",
      "Epoch [3/10], Step [282/938], Loss: 0.0543\n",
      "Epoch [3/10], Step [284/938], Loss: 0.0016\n",
      "Epoch [3/10], Step [286/938], Loss: 0.0019\n",
      "Epoch [3/10], Step [288/938], Loss: 0.0123\n",
      "Epoch [3/10], Step [290/938], Loss: 0.0090\n",
      "Epoch [3/10], Step [292/938], Loss: 0.0311\n",
      "Epoch [3/10], Step [294/938], Loss: 0.0895\n",
      "Epoch [3/10], Step [296/938], Loss: 0.0032\n",
      "Epoch [3/10], Step [298/938], Loss: 0.0042\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0239\n",
      "Epoch [3/10], Step [302/938], Loss: 0.0619\n",
      "Epoch [3/10], Step [304/938], Loss: 0.0419\n",
      "Epoch [3/10], Step [306/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [308/938], Loss: 0.0122\n",
      "Epoch [3/10], Step [310/938], Loss: 0.0259\n",
      "Epoch [3/10], Step [312/938], Loss: 0.0026\n",
      "Epoch [3/10], Step [314/938], Loss: 0.0356\n",
      "Epoch [3/10], Step [316/938], Loss: 0.0085\n",
      "Epoch [3/10], Step [318/938], Loss: 0.0214\n",
      "Epoch [3/10], Step [320/938], Loss: 0.0206\n",
      "Epoch [3/10], Step [322/938], Loss: 0.0354\n",
      "Epoch [3/10], Step [324/938], Loss: 0.0093\n",
      "Epoch [3/10], Step [326/938], Loss: 0.0182\n",
      "Epoch [3/10], Step [328/938], Loss: 0.0381\n",
      "Epoch [3/10], Step [330/938], Loss: 0.0172\n",
      "Epoch [3/10], Step [332/938], Loss: 0.0661\n",
      "Epoch [3/10], Step [334/938], Loss: 0.0036\n",
      "Epoch [3/10], Step [336/938], Loss: 0.0192\n",
      "Epoch [3/10], Step [338/938], Loss: 0.0009\n",
      "Epoch [3/10], Step [340/938], Loss: 0.0096\n",
      "Epoch [3/10], Step [342/938], Loss: 0.0037\n",
      "Epoch [3/10], Step [344/938], Loss: 0.0013\n",
      "Epoch [3/10], Step [346/938], Loss: 0.0576\n",
      "Epoch [3/10], Step [348/938], Loss: 0.0557\n",
      "Epoch [3/10], Step [350/938], Loss: 0.0111\n",
      "Epoch [3/10], Step [352/938], Loss: 0.0054\n",
      "Epoch [3/10], Step [354/938], Loss: 0.0053\n",
      "Epoch [3/10], Step [356/938], Loss: 0.0076\n",
      "Epoch [3/10], Step [358/938], Loss: 0.0115\n",
      "Epoch [3/10], Step [360/938], Loss: 0.0216\n",
      "Epoch [3/10], Step [362/938], Loss: 0.0016\n",
      "Epoch [3/10], Step [364/938], Loss: 0.0450\n",
      "Epoch [3/10], Step [366/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [368/938], Loss: 0.0023\n",
      "Epoch [3/10], Step [370/938], Loss: 0.0257\n",
      "Epoch [3/10], Step [372/938], Loss: 0.0287\n",
      "Epoch [3/10], Step [374/938], Loss: 0.0178\n",
      "Epoch [3/10], Step [376/938], Loss: 0.0184\n",
      "Epoch [3/10], Step [378/938], Loss: 0.0886\n",
      "Epoch [3/10], Step [380/938], Loss: 0.0224\n",
      "Epoch [3/10], Step [382/938], Loss: 0.0861\n",
      "Epoch [3/10], Step [384/938], Loss: 0.0026\n",
      "Epoch [3/10], Step [386/938], Loss: 0.0194\n",
      "Epoch [3/10], Step [388/938], Loss: 0.0199\n",
      "Epoch [3/10], Step [390/938], Loss: 0.1031\n",
      "Epoch [3/10], Step [392/938], Loss: 0.0453\n",
      "Epoch [3/10], Step [394/938], Loss: 0.0070\n",
      "Epoch [3/10], Step [396/938], Loss: 0.0475\n",
      "Epoch [3/10], Step [398/938], Loss: 0.0225\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0686\n",
      "Epoch [3/10], Step [402/938], Loss: 0.0202\n",
      "Epoch [3/10], Step [404/938], Loss: 0.0442\n",
      "Epoch [3/10], Step [406/938], Loss: 0.0155\n",
      "Epoch [3/10], Step [408/938], Loss: 0.0552\n",
      "Epoch [3/10], Step [410/938], Loss: 0.0977\n",
      "Epoch [3/10], Step [412/938], Loss: 0.0500\n",
      "Epoch [3/10], Step [414/938], Loss: 0.0787\n",
      "Epoch [3/10], Step [416/938], Loss: 0.0035\n",
      "Epoch [3/10], Step [418/938], Loss: 0.0334\n",
      "Epoch [3/10], Step [420/938], Loss: 0.0021\n",
      "Epoch [3/10], Step [422/938], Loss: 0.0831\n",
      "Epoch [3/10], Step [424/938], Loss: 0.0121\n",
      "Epoch [3/10], Step [426/938], Loss: 0.0678\n",
      "Epoch [3/10], Step [428/938], Loss: 0.0350\n",
      "Epoch [3/10], Step [430/938], Loss: 0.1346\n",
      "Epoch [3/10], Step [432/938], Loss: 0.0288\n",
      "Epoch [3/10], Step [434/938], Loss: 0.0114\n",
      "Epoch [3/10], Step [436/938], Loss: 0.0184\n",
      "Epoch [3/10], Step [438/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [440/938], Loss: 0.1046\n",
      "Epoch [3/10], Step [442/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [444/938], Loss: 0.0015\n",
      "Epoch [3/10], Step [446/938], Loss: 0.0053\n",
      "Epoch [3/10], Step [448/938], Loss: 0.0032\n",
      "Epoch [3/10], Step [450/938], Loss: 0.0329\n",
      "Epoch [3/10], Step [452/938], Loss: 0.0215\n",
      "Epoch [3/10], Step [454/938], Loss: 0.0029\n",
      "Epoch [3/10], Step [456/938], Loss: 0.0116\n",
      "Epoch [3/10], Step [458/938], Loss: 0.0339\n",
      "Epoch [3/10], Step [460/938], Loss: 0.0526\n",
      "Epoch [3/10], Step [462/938], Loss: 0.0043\n",
      "Epoch [3/10], Step [464/938], Loss: 0.0135\n",
      "Epoch [3/10], Step [466/938], Loss: 0.0016\n",
      "Epoch [3/10], Step [468/938], Loss: 0.0057\n",
      "Epoch [3/10], Step [470/938], Loss: 0.0086\n",
      "Epoch [3/10], Step [472/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [474/938], Loss: 0.0283\n",
      "Epoch [3/10], Step [476/938], Loss: 0.0107\n",
      "Epoch [3/10], Step [478/938], Loss: 0.0076\n",
      "Epoch [3/10], Step [480/938], Loss: 0.0188\n",
      "Epoch [3/10], Step [482/938], Loss: 0.0273\n",
      "Epoch [3/10], Step [484/938], Loss: 0.0036\n",
      "Epoch [3/10], Step [486/938], Loss: 0.0045\n",
      "Epoch [3/10], Step [488/938], Loss: 0.0183\n",
      "Epoch [3/10], Step [490/938], Loss: 0.0069\n",
      "Epoch [3/10], Step [492/938], Loss: 0.0083\n",
      "Epoch [3/10], Step [494/938], Loss: 0.0591\n",
      "Epoch [3/10], Step [496/938], Loss: 0.0210\n",
      "Epoch [3/10], Step [498/938], Loss: 0.0308\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0483\n",
      "Epoch [3/10], Step [502/938], Loss: 0.0642\n",
      "Epoch [3/10], Step [504/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [506/938], Loss: 0.1022\n",
      "Epoch [3/10], Step [508/938], Loss: 0.0389\n",
      "Epoch [3/10], Step [510/938], Loss: 0.0009\n",
      "Epoch [3/10], Step [512/938], Loss: 0.0246\n",
      "Epoch [3/10], Step [514/938], Loss: 0.0170\n",
      "Epoch [3/10], Step [516/938], Loss: 0.0632\n",
      "Epoch [3/10], Step [518/938], Loss: 0.1196\n",
      "Epoch [3/10], Step [520/938], Loss: 0.0818\n",
      "Epoch [3/10], Step [522/938], Loss: 0.0250\n",
      "Epoch [3/10], Step [524/938], Loss: 0.0163\n",
      "Epoch [3/10], Step [526/938], Loss: 0.0180\n",
      "Epoch [3/10], Step [528/938], Loss: 0.0391\n",
      "Epoch [3/10], Step [530/938], Loss: 0.0152\n",
      "Epoch [3/10], Step [532/938], Loss: 0.0173\n",
      "Epoch [3/10], Step [534/938], Loss: 0.0678\n",
      "Epoch [3/10], Step [536/938], Loss: 0.0118\n",
      "Epoch [3/10], Step [538/938], Loss: 0.0272\n",
      "Epoch [3/10], Step [540/938], Loss: 0.0399\n",
      "Epoch [3/10], Step [542/938], Loss: 0.1902\n",
      "Epoch [3/10], Step [544/938], Loss: 0.0102\n",
      "Epoch [3/10], Step [546/938], Loss: 0.0362\n",
      "Epoch [3/10], Step [548/938], Loss: 0.0227\n",
      "Epoch [3/10], Step [550/938], Loss: 0.0123\n",
      "Epoch [3/10], Step [552/938], Loss: 0.0103\n",
      "Epoch [3/10], Step [554/938], Loss: 0.0022\n",
      "Epoch [3/10], Step [556/938], Loss: 0.0713\n",
      "Epoch [3/10], Step [558/938], Loss: 0.0261\n",
      "Epoch [3/10], Step [560/938], Loss: 0.0446\n",
      "Epoch [3/10], Step [562/938], Loss: 0.0422\n",
      "Epoch [3/10], Step [564/938], Loss: 0.0434\n",
      "Epoch [3/10], Step [566/938], Loss: 0.0076\n",
      "Epoch [3/10], Step [568/938], Loss: 0.0171\n",
      "Epoch [3/10], Step [570/938], Loss: 0.0445\n",
      "Epoch [3/10], Step [572/938], Loss: 0.0382\n",
      "Epoch [3/10], Step [574/938], Loss: 0.0106\n",
      "Epoch [3/10], Step [576/938], Loss: 0.0315\n",
      "Epoch [3/10], Step [578/938], Loss: 0.0347\n",
      "Epoch [3/10], Step [580/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [582/938], Loss: 0.0020\n",
      "Epoch [3/10], Step [584/938], Loss: 0.1074\n",
      "Epoch [3/10], Step [586/938], Loss: 0.0921\n",
      "Epoch [3/10], Step [588/938], Loss: 0.0274\n",
      "Epoch [3/10], Step [590/938], Loss: 0.0102\n",
      "Epoch [3/10], Step [592/938], Loss: 0.0046\n",
      "Epoch [3/10], Step [594/938], Loss: 0.0064\n",
      "Epoch [3/10], Step [596/938], Loss: 0.0280\n",
      "Epoch [3/10], Step [598/938], Loss: 0.0299\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0598\n",
      "Epoch [3/10], Step [602/938], Loss: 0.0066\n",
      "Epoch [3/10], Step [604/938], Loss: 0.0912\n",
      "Epoch [3/10], Step [606/938], Loss: 0.0281\n",
      "Epoch [3/10], Step [608/938], Loss: 0.0179\n",
      "Epoch [3/10], Step [610/938], Loss: 0.0049\n",
      "Epoch [3/10], Step [612/938], Loss: 0.0142\n",
      "Epoch [3/10], Step [614/938], Loss: 0.0124\n",
      "Epoch [3/10], Step [616/938], Loss: 0.0144\n",
      "Epoch [3/10], Step [618/938], Loss: 0.0471\n",
      "Epoch [3/10], Step [620/938], Loss: 0.0232\n",
      "Epoch [3/10], Step [622/938], Loss: 0.0188\n",
      "Epoch [3/10], Step [624/938], Loss: 0.0941\n",
      "Epoch [3/10], Step [626/938], Loss: 0.1750\n",
      "Epoch [3/10], Step [628/938], Loss: 0.0244\n",
      "Epoch [3/10], Step [630/938], Loss: 0.0477\n",
      "Epoch [3/10], Step [632/938], Loss: 0.0406\n",
      "Epoch [3/10], Step [634/938], Loss: 0.0552\n",
      "Epoch [3/10], Step [636/938], Loss: 0.0115\n",
      "Epoch [3/10], Step [638/938], Loss: 0.0138\n",
      "Epoch [3/10], Step [640/938], Loss: 0.0104\n",
      "Epoch [3/10], Step [642/938], Loss: 0.0793\n",
      "Epoch [3/10], Step [644/938], Loss: 0.0783\n",
      "Epoch [3/10], Step [646/938], Loss: 0.0252\n",
      "Epoch [3/10], Step [648/938], Loss: 0.0547\n",
      "Epoch [3/10], Step [650/938], Loss: 0.0127\n",
      "Epoch [3/10], Step [652/938], Loss: 0.0132\n",
      "Epoch [3/10], Step [654/938], Loss: 0.0752\n",
      "Epoch [3/10], Step [656/938], Loss: 0.0144\n",
      "Epoch [3/10], Step [658/938], Loss: 0.0076\n",
      "Epoch [3/10], Step [660/938], Loss: 0.0522\n",
      "Epoch [3/10], Step [662/938], Loss: 0.1310\n",
      "Epoch [3/10], Step [664/938], Loss: 0.0086\n",
      "Epoch [3/10], Step [666/938], Loss: 0.0108\n",
      "Epoch [3/10], Step [668/938], Loss: 0.0086\n",
      "Epoch [3/10], Step [670/938], Loss: 0.0169\n",
      "Epoch [3/10], Step [672/938], Loss: 0.0702\n",
      "Epoch [3/10], Step [674/938], Loss: 0.0097\n",
      "Epoch [3/10], Step [676/938], Loss: 0.0078\n",
      "Epoch [3/10], Step [678/938], Loss: 0.0123\n",
      "Epoch [3/10], Step [680/938], Loss: 0.0512\n",
      "Epoch [3/10], Step [682/938], Loss: 0.0576\n",
      "Epoch [3/10], Step [684/938], Loss: 0.0056\n",
      "Epoch [3/10], Step [686/938], Loss: 0.0111\n",
      "Epoch [3/10], Step [688/938], Loss: 0.0063\n",
      "Epoch [3/10], Step [690/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [692/938], Loss: 0.0082\n",
      "Epoch [3/10], Step [694/938], Loss: 0.0115\n",
      "Epoch [3/10], Step [696/938], Loss: 0.0184\n",
      "Epoch [3/10], Step [698/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0768\n",
      "Epoch [3/10], Step [702/938], Loss: 0.0910\n",
      "Epoch [3/10], Step [704/938], Loss: 0.0011\n",
      "Epoch [3/10], Step [706/938], Loss: 0.0391\n",
      "Epoch [3/10], Step [708/938], Loss: 0.0743\n",
      "Epoch [3/10], Step [710/938], Loss: 0.0074\n",
      "Epoch [3/10], Step [712/938], Loss: 0.0397\n",
      "Epoch [3/10], Step [714/938], Loss: 0.0417\n",
      "Epoch [3/10], Step [716/938], Loss: 0.0413\n",
      "Epoch [3/10], Step [718/938], Loss: 0.0243\n",
      "Epoch [3/10], Step [720/938], Loss: 0.0599\n",
      "Epoch [3/10], Step [722/938], Loss: 0.0790\n",
      "Epoch [3/10], Step [724/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [726/938], Loss: 0.0136\n",
      "Epoch [3/10], Step [728/938], Loss: 0.0069\n",
      "Epoch [3/10], Step [730/938], Loss: 0.0542\n",
      "Epoch [3/10], Step [732/938], Loss: 0.0135\n",
      "Epoch [3/10], Step [734/938], Loss: 0.0014\n",
      "Epoch [3/10], Step [736/938], Loss: 0.0107\n",
      "Epoch [3/10], Step [738/938], Loss: 0.0976\n",
      "Epoch [3/10], Step [740/938], Loss: 0.0010\n",
      "Epoch [3/10], Step [742/938], Loss: 0.0021\n",
      "Epoch [3/10], Step [744/938], Loss: 0.0320\n",
      "Epoch [3/10], Step [746/938], Loss: 0.0136\n",
      "Epoch [3/10], Step [748/938], Loss: 0.0127\n",
      "Epoch [3/10], Step [750/938], Loss: 0.0829\n",
      "Epoch [3/10], Step [752/938], Loss: 0.0659\n",
      "Epoch [3/10], Step [754/938], Loss: 0.0096\n",
      "Epoch [3/10], Step [756/938], Loss: 0.0364\n",
      "Epoch [3/10], Step [758/938], Loss: 0.0186\n",
      "Epoch [3/10], Step [760/938], Loss: 0.0026\n",
      "Epoch [3/10], Step [762/938], Loss: 0.0134\n",
      "Epoch [3/10], Step [764/938], Loss: 0.0547\n",
      "Epoch [3/10], Step [766/938], Loss: 0.0089\n",
      "Epoch [3/10], Step [768/938], Loss: 0.0448\n",
      "Epoch [3/10], Step [770/938], Loss: 0.0065\n",
      "Epoch [3/10], Step [772/938], Loss: 0.0062\n",
      "Epoch [3/10], Step [774/938], Loss: 0.0223\n",
      "Epoch [3/10], Step [776/938], Loss: 0.0012\n",
      "Epoch [3/10], Step [778/938], Loss: 0.0035\n",
      "Epoch [3/10], Step [780/938], Loss: 0.0066\n",
      "Epoch [3/10], Step [782/938], Loss: 0.0529\n",
      "Epoch [3/10], Step [784/938], Loss: 0.0039\n",
      "Epoch [3/10], Step [786/938], Loss: 0.1011\n",
      "Epoch [3/10], Step [788/938], Loss: 0.0189\n",
      "Epoch [3/10], Step [790/938], Loss: 0.0552\n",
      "Epoch [3/10], Step [792/938], Loss: 0.0077\n",
      "Epoch [3/10], Step [794/938], Loss: 0.1258\n",
      "Epoch [3/10], Step [796/938], Loss: 0.0978\n",
      "Epoch [3/10], Step [798/938], Loss: 0.0093\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0241\n",
      "Epoch [3/10], Step [802/938], Loss: 0.0102\n",
      "Epoch [3/10], Step [804/938], Loss: 0.1409\n",
      "Epoch [3/10], Step [806/938], Loss: 0.0099\n",
      "Epoch [3/10], Step [808/938], Loss: 0.1021\n",
      "Epoch [3/10], Step [810/938], Loss: 0.0362\n",
      "Epoch [3/10], Step [812/938], Loss: 0.0041\n",
      "Epoch [3/10], Step [814/938], Loss: 0.0281\n",
      "Epoch [3/10], Step [816/938], Loss: 0.0405\n",
      "Epoch [3/10], Step [818/938], Loss: 0.0050\n",
      "Epoch [3/10], Step [820/938], Loss: 0.0218\n",
      "Epoch [3/10], Step [822/938], Loss: 0.0557\n",
      "Epoch [3/10], Step [824/938], Loss: 0.0014\n",
      "Epoch [3/10], Step [826/938], Loss: 0.0193\n",
      "Epoch [3/10], Step [828/938], Loss: 0.0263\n",
      "Epoch [3/10], Step [830/938], Loss: 0.0219\n",
      "Epoch [3/10], Step [832/938], Loss: 0.0304\n",
      "Epoch [3/10], Step [834/938], Loss: 0.0063\n",
      "Epoch [3/10], Step [836/938], Loss: 0.0721\n",
      "Epoch [3/10], Step [838/938], Loss: 0.0033\n",
      "Epoch [3/10], Step [840/938], Loss: 0.1135\n",
      "Epoch [3/10], Step [842/938], Loss: 0.0054\n",
      "Epoch [3/10], Step [844/938], Loss: 0.0222\n",
      "Epoch [3/10], Step [846/938], Loss: 0.0007\n",
      "Epoch [3/10], Step [848/938], Loss: 0.0081\n",
      "Epoch [3/10], Step [850/938], Loss: 0.0197\n",
      "Epoch [3/10], Step [852/938], Loss: 0.0431\n",
      "Epoch [3/10], Step [854/938], Loss: 0.0207\n",
      "Epoch [3/10], Step [856/938], Loss: 0.0276\n",
      "Epoch [3/10], Step [858/938], Loss: 0.0218\n",
      "Epoch [3/10], Step [860/938], Loss: 0.0567\n",
      "Epoch [3/10], Step [862/938], Loss: 0.0023\n",
      "Epoch [3/10], Step [864/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [866/938], Loss: 0.0280\n",
      "Epoch [3/10], Step [868/938], Loss: 0.0288\n",
      "Epoch [3/10], Step [870/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [872/938], Loss: 0.0028\n",
      "Epoch [3/10], Step [874/938], Loss: 0.0120\n",
      "Epoch [3/10], Step [876/938], Loss: 0.0012\n",
      "Epoch [3/10], Step [878/938], Loss: 0.0119\n",
      "Epoch [3/10], Step [880/938], Loss: 0.0304\n",
      "Epoch [3/10], Step [882/938], Loss: 0.0111\n",
      "Epoch [3/10], Step [884/938], Loss: 0.0630\n",
      "Epoch [3/10], Step [886/938], Loss: 0.0068\n",
      "Epoch [3/10], Step [888/938], Loss: 0.0097\n",
      "Epoch [3/10], Step [890/938], Loss: 0.0024\n",
      "Epoch [3/10], Step [892/938], Loss: 0.0981\n",
      "Epoch [3/10], Step [894/938], Loss: 0.0060\n",
      "Epoch [3/10], Step [896/938], Loss: 0.0017\n",
      "Epoch [3/10], Step [898/938], Loss: 0.0551\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0217\n",
      "Epoch [3/10], Step [902/938], Loss: 0.0116\n",
      "Epoch [3/10], Step [904/938], Loss: 0.0335\n",
      "Epoch [3/10], Step [906/938], Loss: 0.0435\n",
      "Epoch [3/10], Step [908/938], Loss: 0.0361\n",
      "Epoch [3/10], Step [910/938], Loss: 0.1941\n",
      "Epoch [3/10], Step [912/938], Loss: 0.0415\n",
      "Epoch [3/10], Step [914/938], Loss: 0.0067\n",
      "Epoch [3/10], Step [916/938], Loss: 0.0957\n",
      "Epoch [3/10], Step [918/938], Loss: 0.0095\n",
      "Epoch [3/10], Step [920/938], Loss: 0.0057\n",
      "Epoch [3/10], Step [922/938], Loss: 0.0401\n",
      "Epoch [3/10], Step [924/938], Loss: 0.1223\n",
      "Epoch [3/10], Step [926/938], Loss: 0.0064\n",
      "Epoch [3/10], Step [928/938], Loss: 0.0043\n",
      "Epoch [3/10], Step [930/938], Loss: 0.0070\n",
      "Epoch [3/10], Step [932/938], Loss: 0.0028\n",
      "Epoch [3/10], Step [934/938], Loss: 0.0378\n",
      "Epoch [3/10], Step [936/938], Loss: 0.0205\n",
      "Epoch [3/10], Step [938/938], Loss: 0.0267\n",
      "Epoch [3/10], Loss: 0.0306\n",
      "Epoch [4/10], Step [2/938], Loss: 0.0490\n",
      "Epoch [4/10], Step [4/938], Loss: 0.0040\n",
      "Epoch [4/10], Step [6/938], Loss: 0.0061\n",
      "Epoch [4/10], Step [8/938], Loss: 0.0410\n",
      "Epoch [4/10], Step [10/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [12/938], Loss: 0.0119\n",
      "Epoch [4/10], Step [14/938], Loss: 0.0091\n",
      "Epoch [4/10], Step [16/938], Loss: 0.0195\n",
      "Epoch [4/10], Step [18/938], Loss: 0.0076\n",
      "Epoch [4/10], Step [20/938], Loss: 0.0186\n",
      "Epoch [4/10], Step [22/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [24/938], Loss: 0.0560\n",
      "Epoch [4/10], Step [26/938], Loss: 0.0033\n",
      "Epoch [4/10], Step [28/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [30/938], Loss: 0.0808\n",
      "Epoch [4/10], Step [32/938], Loss: 0.0168\n",
      "Epoch [4/10], Step [34/938], Loss: 0.0019\n",
      "Epoch [4/10], Step [36/938], Loss: 0.0539\n",
      "Epoch [4/10], Step [38/938], Loss: 0.0222\n",
      "Epoch [4/10], Step [40/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [42/938], Loss: 0.0142\n",
      "Epoch [4/10], Step [44/938], Loss: 0.0887\n",
      "Epoch [4/10], Step [46/938], Loss: 0.0093\n",
      "Epoch [4/10], Step [48/938], Loss: 0.0072\n",
      "Epoch [4/10], Step [50/938], Loss: 0.0240\n",
      "Epoch [4/10], Step [52/938], Loss: 0.0550\n",
      "Epoch [4/10], Step [54/938], Loss: 0.0134\n",
      "Epoch [4/10], Step [56/938], Loss: 0.0056\n",
      "Epoch [4/10], Step [58/938], Loss: 0.0115\n",
      "Epoch [4/10], Step [60/938], Loss: 0.0133\n",
      "Epoch [4/10], Step [62/938], Loss: 0.0624\n",
      "Epoch [4/10], Step [64/938], Loss: 0.0024\n",
      "Epoch [4/10], Step [66/938], Loss: 0.0733\n",
      "Epoch [4/10], Step [68/938], Loss: 0.0122\n",
      "Epoch [4/10], Step [70/938], Loss: 0.0113\n",
      "Epoch [4/10], Step [72/938], Loss: 0.0378\n",
      "Epoch [4/10], Step [74/938], Loss: 0.0386\n",
      "Epoch [4/10], Step [76/938], Loss: 0.0376\n",
      "Epoch [4/10], Step [78/938], Loss: 0.0125\n",
      "Epoch [4/10], Step [80/938], Loss: 0.0226\n",
      "Epoch [4/10], Step [82/938], Loss: 0.0476\n",
      "Epoch [4/10], Step [84/938], Loss: 0.0374\n",
      "Epoch [4/10], Step [86/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [88/938], Loss: 0.0128\n",
      "Epoch [4/10], Step [90/938], Loss: 0.0537\n",
      "Epoch [4/10], Step [92/938], Loss: 0.0044\n",
      "Epoch [4/10], Step [94/938], Loss: 0.0200\n",
      "Epoch [4/10], Step [96/938], Loss: 0.0081\n",
      "Epoch [4/10], Step [98/938], Loss: 0.0325\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0090\n",
      "Epoch [4/10], Step [102/938], Loss: 0.0208\n",
      "Epoch [4/10], Step [104/938], Loss: 0.0215\n",
      "Epoch [4/10], Step [106/938], Loss: 0.0292\n",
      "Epoch [4/10], Step [108/938], Loss: 0.0085\n",
      "Epoch [4/10], Step [110/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [112/938], Loss: 0.0014\n",
      "Epoch [4/10], Step [114/938], Loss: 0.0767\n",
      "Epoch [4/10], Step [116/938], Loss: 0.0623\n",
      "Epoch [4/10], Step [118/938], Loss: 0.0035\n",
      "Epoch [4/10], Step [120/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [122/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [124/938], Loss: 0.0094\n",
      "Epoch [4/10], Step [126/938], Loss: 0.0103\n",
      "Epoch [4/10], Step [128/938], Loss: 0.0032\n",
      "Epoch [4/10], Step [130/938], Loss: 0.0124\n",
      "Epoch [4/10], Step [132/938], Loss: 0.0066\n",
      "Epoch [4/10], Step [134/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [136/938], Loss: 0.0099\n",
      "Epoch [4/10], Step [138/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [140/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [142/938], Loss: 0.0155\n",
      "Epoch [4/10], Step [144/938], Loss: 0.0201\n",
      "Epoch [4/10], Step [146/938], Loss: 0.1495\n",
      "Epoch [4/10], Step [148/938], Loss: 0.0371\n",
      "Epoch [4/10], Step [150/938], Loss: 0.0192\n",
      "Epoch [4/10], Step [152/938], Loss: 0.0040\n",
      "Epoch [4/10], Step [154/938], Loss: 0.0221\n",
      "Epoch [4/10], Step [156/938], Loss: 0.0598\n",
      "Epoch [4/10], Step [158/938], Loss: 0.0119\n",
      "Epoch [4/10], Step [160/938], Loss: 0.0058\n",
      "Epoch [4/10], Step [162/938], Loss: 0.0309\n",
      "Epoch [4/10], Step [164/938], Loss: 0.0152\n",
      "Epoch [4/10], Step [166/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [168/938], Loss: 0.0443\n",
      "Epoch [4/10], Step [170/938], Loss: 0.0414\n",
      "Epoch [4/10], Step [172/938], Loss: 0.0064\n",
      "Epoch [4/10], Step [174/938], Loss: 0.0024\n",
      "Epoch [4/10], Step [176/938], Loss: 0.0096\n",
      "Epoch [4/10], Step [178/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [180/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [182/938], Loss: 0.0009\n",
      "Epoch [4/10], Step [184/938], Loss: 0.0078\n",
      "Epoch [4/10], Step [186/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [188/938], Loss: 0.0450\n",
      "Epoch [4/10], Step [190/938], Loss: 0.0297\n",
      "Epoch [4/10], Step [192/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [194/938], Loss: 0.0065\n",
      "Epoch [4/10], Step [196/938], Loss: 0.0058\n",
      "Epoch [4/10], Step [198/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0127\n",
      "Epoch [4/10], Step [202/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [204/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [206/938], Loss: 0.0085\n",
      "Epoch [4/10], Step [208/938], Loss: 0.0909\n",
      "Epoch [4/10], Step [210/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [212/938], Loss: 0.0082\n",
      "Epoch [4/10], Step [214/938], Loss: 0.0353\n",
      "Epoch [4/10], Step [216/938], Loss: 0.0074\n",
      "Epoch [4/10], Step [218/938], Loss: 0.0959\n",
      "Epoch [4/10], Step [220/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [222/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [224/938], Loss: 0.0043\n",
      "Epoch [4/10], Step [226/938], Loss: 0.0223\n",
      "Epoch [4/10], Step [228/938], Loss: 0.0021\n",
      "Epoch [4/10], Step [230/938], Loss: 0.0245\n",
      "Epoch [4/10], Step [232/938], Loss: 0.0442\n",
      "Epoch [4/10], Step [234/938], Loss: 0.0460\n",
      "Epoch [4/10], Step [236/938], Loss: 0.0285\n",
      "Epoch [4/10], Step [238/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [240/938], Loss: 0.0274\n",
      "Epoch [4/10], Step [242/938], Loss: 0.0048\n",
      "Epoch [4/10], Step [244/938], Loss: 0.0115\n",
      "Epoch [4/10], Step [246/938], Loss: 0.0003\n",
      "Epoch [4/10], Step [248/938], Loss: 0.0240\n",
      "Epoch [4/10], Step [250/938], Loss: 0.0024\n",
      "Epoch [4/10], Step [252/938], Loss: 0.0008\n",
      "Epoch [4/10], Step [254/938], Loss: 0.0113\n",
      "Epoch [4/10], Step [256/938], Loss: 0.0041\n",
      "Epoch [4/10], Step [258/938], Loss: 0.0531\n",
      "Epoch [4/10], Step [260/938], Loss: 0.0133\n",
      "Epoch [4/10], Step [262/938], Loss: 0.0033\n",
      "Epoch [4/10], Step [264/938], Loss: 0.0397\n",
      "Epoch [4/10], Step [266/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [268/938], Loss: 0.0635\n",
      "Epoch [4/10], Step [270/938], Loss: 0.0085\n",
      "Epoch [4/10], Step [272/938], Loss: 0.0271\n",
      "Epoch [4/10], Step [274/938], Loss: 0.0342\n",
      "Epoch [4/10], Step [276/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [278/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [280/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [282/938], Loss: 0.0569\n",
      "Epoch [4/10], Step [284/938], Loss: 0.0087\n",
      "Epoch [4/10], Step [286/938], Loss: 0.0101\n",
      "Epoch [4/10], Step [288/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [290/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [292/938], Loss: 0.0774\n",
      "Epoch [4/10], Step [294/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [296/938], Loss: 0.0267\n",
      "Epoch [4/10], Step [298/938], Loss: 0.0118\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [302/938], Loss: 0.0103\n",
      "Epoch [4/10], Step [304/938], Loss: 0.0305\n",
      "Epoch [4/10], Step [306/938], Loss: 0.0002\n",
      "Epoch [4/10], Step [308/938], Loss: 0.0268\n",
      "Epoch [4/10], Step [310/938], Loss: 0.0212\n",
      "Epoch [4/10], Step [312/938], Loss: 0.0045\n",
      "Epoch [4/10], Step [314/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [316/938], Loss: 0.0032\n",
      "Epoch [4/10], Step [318/938], Loss: 0.0014\n",
      "Epoch [4/10], Step [320/938], Loss: 0.0301\n",
      "Epoch [4/10], Step [322/938], Loss: 0.0099\n",
      "Epoch [4/10], Step [324/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [326/938], Loss: 0.0019\n",
      "Epoch [4/10], Step [328/938], Loss: 0.0341\n",
      "Epoch [4/10], Step [330/938], Loss: 0.0034\n",
      "Epoch [4/10], Step [332/938], Loss: 0.0180\n",
      "Epoch [4/10], Step [334/938], Loss: 0.0059\n",
      "Epoch [4/10], Step [336/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [338/938], Loss: 0.0038\n",
      "Epoch [4/10], Step [340/938], Loss: 0.0071\n",
      "Epoch [4/10], Step [342/938], Loss: 0.0168\n",
      "Epoch [4/10], Step [344/938], Loss: 0.0243\n",
      "Epoch [4/10], Step [346/938], Loss: 0.0055\n",
      "Epoch [4/10], Step [348/938], Loss: 0.0580\n",
      "Epoch [4/10], Step [350/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [352/938], Loss: 0.0137\n",
      "Epoch [4/10], Step [354/938], Loss: 0.0166\n",
      "Epoch [4/10], Step [356/938], Loss: 0.0031\n",
      "Epoch [4/10], Step [358/938], Loss: 0.0138\n",
      "Epoch [4/10], Step [360/938], Loss: 0.0115\n",
      "Epoch [4/10], Step [362/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [364/938], Loss: 0.0285\n",
      "Epoch [4/10], Step [366/938], Loss: 0.0156\n",
      "Epoch [4/10], Step [368/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [370/938], Loss: 0.0171\n",
      "Epoch [4/10], Step [372/938], Loss: 0.0055\n",
      "Epoch [4/10], Step [374/938], Loss: 0.0372\n",
      "Epoch [4/10], Step [376/938], Loss: 0.0012\n",
      "Epoch [4/10], Step [378/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [380/938], Loss: 0.0847\n",
      "Epoch [4/10], Step [382/938], Loss: 0.0259\n",
      "Epoch [4/10], Step [384/938], Loss: 0.0194\n",
      "Epoch [4/10], Step [386/938], Loss: 0.0112\n",
      "Epoch [4/10], Step [388/938], Loss: 0.0523\n",
      "Epoch [4/10], Step [390/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [392/938], Loss: 0.0437\n",
      "Epoch [4/10], Step [394/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [396/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [398/938], Loss: 0.0022\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0021\n",
      "Epoch [4/10], Step [402/938], Loss: 0.0862\n",
      "Epoch [4/10], Step [404/938], Loss: 0.0054\n",
      "Epoch [4/10], Step [406/938], Loss: 0.0368\n",
      "Epoch [4/10], Step [408/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [410/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [412/938], Loss: 0.0109\n",
      "Epoch [4/10], Step [414/938], Loss: 0.0419\n",
      "Epoch [4/10], Step [416/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [418/938], Loss: 0.0092\n",
      "Epoch [4/10], Step [420/938], Loss: 0.0134\n",
      "Epoch [4/10], Step [422/938], Loss: 0.0240\n",
      "Epoch [4/10], Step [424/938], Loss: 0.0035\n",
      "Epoch [4/10], Step [426/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [428/938], Loss: 0.0513\n",
      "Epoch [4/10], Step [430/938], Loss: 0.0135\n",
      "Epoch [4/10], Step [432/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [434/938], Loss: 0.0305\n",
      "Epoch [4/10], Step [436/938], Loss: 0.0864\n",
      "Epoch [4/10], Step [438/938], Loss: 0.0046\n",
      "Epoch [4/10], Step [440/938], Loss: 0.0075\n",
      "Epoch [4/10], Step [442/938], Loss: 0.0130\n",
      "Epoch [4/10], Step [444/938], Loss: 0.0045\n",
      "Epoch [4/10], Step [446/938], Loss: 0.0159\n",
      "Epoch [4/10], Step [448/938], Loss: 0.0051\n",
      "Epoch [4/10], Step [450/938], Loss: 0.0548\n",
      "Epoch [4/10], Step [452/938], Loss: 0.0323\n",
      "Epoch [4/10], Step [454/938], Loss: 0.0074\n",
      "Epoch [4/10], Step [456/938], Loss: 0.0129\n",
      "Epoch [4/10], Step [458/938], Loss: 0.0174\n",
      "Epoch [4/10], Step [460/938], Loss: 0.0005\n",
      "Epoch [4/10], Step [462/938], Loss: 0.0427\n",
      "Epoch [4/10], Step [464/938], Loss: 0.0013\n",
      "Epoch [4/10], Step [466/938], Loss: 0.0087\n",
      "Epoch [4/10], Step [468/938], Loss: 0.0118\n",
      "Epoch [4/10], Step [470/938], Loss: 0.0013\n",
      "Epoch [4/10], Step [472/938], Loss: 0.0328\n",
      "Epoch [4/10], Step [474/938], Loss: 0.0560\n",
      "Epoch [4/10], Step [476/938], Loss: 0.0056\n",
      "Epoch [4/10], Step [478/938], Loss: 0.0194\n",
      "Epoch [4/10], Step [480/938], Loss: 0.0232\n",
      "Epoch [4/10], Step [482/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [484/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [486/938], Loss: 0.1182\n",
      "Epoch [4/10], Step [488/938], Loss: 0.0844\n",
      "Epoch [4/10], Step [490/938], Loss: 0.0486\n",
      "Epoch [4/10], Step [492/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [494/938], Loss: 0.0011\n",
      "Epoch [4/10], Step [496/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [498/938], Loss: 0.0105\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0162\n",
      "Epoch [4/10], Step [502/938], Loss: 0.0225\n",
      "Epoch [4/10], Step [504/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [506/938], Loss: 0.0440\n",
      "Epoch [4/10], Step [508/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [510/938], Loss: 0.0675\n",
      "Epoch [4/10], Step [512/938], Loss: 0.0134\n",
      "Epoch [4/10], Step [514/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [516/938], Loss: 0.0022\n",
      "Epoch [4/10], Step [518/938], Loss: 0.0007\n",
      "Epoch [4/10], Step [520/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [522/938], Loss: 0.0912\n",
      "Epoch [4/10], Step [524/938], Loss: 0.0089\n",
      "Epoch [4/10], Step [526/938], Loss: 0.0184\n",
      "Epoch [4/10], Step [528/938], Loss: 0.0281\n",
      "Epoch [4/10], Step [530/938], Loss: 0.0593\n",
      "Epoch [4/10], Step [532/938], Loss: 0.1522\n",
      "Epoch [4/10], Step [534/938], Loss: 0.0368\n",
      "Epoch [4/10], Step [536/938], Loss: 0.0016\n",
      "Epoch [4/10], Step [538/938], Loss: 0.0513\n",
      "Epoch [4/10], Step [540/938], Loss: 0.0258\n",
      "Epoch [4/10], Step [542/938], Loss: 0.0139\n",
      "Epoch [4/10], Step [544/938], Loss: 0.0160\n",
      "Epoch [4/10], Step [546/938], Loss: 0.0121\n",
      "Epoch [4/10], Step [548/938], Loss: 0.0238\n",
      "Epoch [4/10], Step [550/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [552/938], Loss: 0.0028\n",
      "Epoch [4/10], Step [554/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [556/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [558/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [560/938], Loss: 0.0041\n",
      "Epoch [4/10], Step [562/938], Loss: 0.0690\n",
      "Epoch [4/10], Step [564/938], Loss: 0.0008\n",
      "Epoch [4/10], Step [566/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [568/938], Loss: 0.0565\n",
      "Epoch [4/10], Step [570/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [572/938], Loss: 0.0053\n",
      "Epoch [4/10], Step [574/938], Loss: 0.0436\n",
      "Epoch [4/10], Step [576/938], Loss: 0.0259\n",
      "Epoch [4/10], Step [578/938], Loss: 0.0076\n",
      "Epoch [4/10], Step [580/938], Loss: 0.2588\n",
      "Epoch [4/10], Step [582/938], Loss: 0.0321\n",
      "Epoch [4/10], Step [584/938], Loss: 0.1355\n",
      "Epoch [4/10], Step [586/938], Loss: 0.0058\n",
      "Epoch [4/10], Step [588/938], Loss: 0.1539\n",
      "Epoch [4/10], Step [590/938], Loss: 0.1983\n",
      "Epoch [4/10], Step [592/938], Loss: 0.0087\n",
      "Epoch [4/10], Step [594/938], Loss: 0.0446\n",
      "Epoch [4/10], Step [596/938], Loss: 0.0652\n",
      "Epoch [4/10], Step [598/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0011\n",
      "Epoch [4/10], Step [602/938], Loss: 0.0026\n",
      "Epoch [4/10], Step [604/938], Loss: 0.0136\n",
      "Epoch [4/10], Step [606/938], Loss: 0.0412\n",
      "Epoch [4/10], Step [608/938], Loss: 0.1049\n",
      "Epoch [4/10], Step [610/938], Loss: 0.0379\n",
      "Epoch [4/10], Step [612/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [614/938], Loss: 0.0715\n",
      "Epoch [4/10], Step [616/938], Loss: 0.0091\n",
      "Epoch [4/10], Step [618/938], Loss: 0.0400\n",
      "Epoch [4/10], Step [620/938], Loss: 0.0158\n",
      "Epoch [4/10], Step [622/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [624/938], Loss: 0.0024\n",
      "Epoch [4/10], Step [626/938], Loss: 0.0828\n",
      "Epoch [4/10], Step [628/938], Loss: 0.0042\n",
      "Epoch [4/10], Step [630/938], Loss: 0.0277\n",
      "Epoch [4/10], Step [632/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [634/938], Loss: 0.0620\n",
      "Epoch [4/10], Step [636/938], Loss: 0.0093\n",
      "Epoch [4/10], Step [638/938], Loss: 0.0038\n",
      "Epoch [4/10], Step [640/938], Loss: 0.0167\n",
      "Epoch [4/10], Step [642/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [644/938], Loss: 0.0080\n",
      "Epoch [4/10], Step [646/938], Loss: 0.0045\n",
      "Epoch [4/10], Step [648/938], Loss: 0.0297\n",
      "Epoch [4/10], Step [650/938], Loss: 0.0063\n",
      "Epoch [4/10], Step [652/938], Loss: 0.0065\n",
      "Epoch [4/10], Step [654/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [656/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [658/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [660/938], Loss: 0.0154\n",
      "Epoch [4/10], Step [662/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [664/938], Loss: 0.0285\n",
      "Epoch [4/10], Step [666/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [668/938], Loss: 0.0484\n",
      "Epoch [4/10], Step [670/938], Loss: 0.0064\n",
      "Epoch [4/10], Step [672/938], Loss: 0.0372\n",
      "Epoch [4/10], Step [674/938], Loss: 0.0054\n",
      "Epoch [4/10], Step [676/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [678/938], Loss: 0.0045\n",
      "Epoch [4/10], Step [680/938], Loss: 0.0493\n",
      "Epoch [4/10], Step [682/938], Loss: 0.0306\n",
      "Epoch [4/10], Step [684/938], Loss: 0.0031\n",
      "Epoch [4/10], Step [686/938], Loss: 0.0220\n",
      "Epoch [4/10], Step [688/938], Loss: 0.0352\n",
      "Epoch [4/10], Step [690/938], Loss: 0.0107\n",
      "Epoch [4/10], Step [692/938], Loss: 0.0435\n",
      "Epoch [4/10], Step [694/938], Loss: 0.0049\n",
      "Epoch [4/10], Step [696/938], Loss: 0.0270\n",
      "Epoch [4/10], Step [698/938], Loss: 0.0933\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0004\n",
      "Epoch [4/10], Step [702/938], Loss: 0.0526\n",
      "Epoch [4/10], Step [704/938], Loss: 0.0122\n",
      "Epoch [4/10], Step [706/938], Loss: 0.0010\n",
      "Epoch [4/10], Step [708/938], Loss: 0.0297\n",
      "Epoch [4/10], Step [710/938], Loss: 0.0009\n",
      "Epoch [4/10], Step [712/938], Loss: 0.0931\n",
      "Epoch [4/10], Step [714/938], Loss: 0.0011\n",
      "Epoch [4/10], Step [716/938], Loss: 0.0435\n",
      "Epoch [4/10], Step [718/938], Loss: 0.0291\n",
      "Epoch [4/10], Step [720/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [722/938], Loss: 0.0215\n",
      "Epoch [4/10], Step [724/938], Loss: 0.0075\n",
      "Epoch [4/10], Step [726/938], Loss: 0.0090\n",
      "Epoch [4/10], Step [728/938], Loss: 0.0689\n",
      "Epoch [4/10], Step [730/938], Loss: 0.0722\n",
      "Epoch [4/10], Step [732/938], Loss: 0.0100\n",
      "Epoch [4/10], Step [734/938], Loss: 0.0078\n",
      "Epoch [4/10], Step [736/938], Loss: 0.0180\n",
      "Epoch [4/10], Step [738/938], Loss: 0.0384\n",
      "Epoch [4/10], Step [740/938], Loss: 0.0319\n",
      "Epoch [4/10], Step [742/938], Loss: 0.0234\n",
      "Epoch [4/10], Step [744/938], Loss: 0.0045\n",
      "Epoch [4/10], Step [746/938], Loss: 0.0487\n",
      "Epoch [4/10], Step [748/938], Loss: 0.0156\n",
      "Epoch [4/10], Step [750/938], Loss: 0.0187\n",
      "Epoch [4/10], Step [752/938], Loss: 0.0282\n",
      "Epoch [4/10], Step [754/938], Loss: 0.0352\n",
      "Epoch [4/10], Step [756/938], Loss: 0.0046\n",
      "Epoch [4/10], Step [758/938], Loss: 0.0251\n",
      "Epoch [4/10], Step [760/938], Loss: 0.0020\n",
      "Epoch [4/10], Step [762/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [764/938], Loss: 0.0053\n",
      "Epoch [4/10], Step [766/938], Loss: 0.0030\n",
      "Epoch [4/10], Step [768/938], Loss: 0.0095\n",
      "Epoch [4/10], Step [770/938], Loss: 0.0066\n",
      "Epoch [4/10], Step [772/938], Loss: 0.0376\n",
      "Epoch [4/10], Step [774/938], Loss: 0.0173\n",
      "Epoch [4/10], Step [776/938], Loss: 0.0026\n",
      "Epoch [4/10], Step [778/938], Loss: 0.0015\n",
      "Epoch [4/10], Step [780/938], Loss: 0.0059\n",
      "Epoch [4/10], Step [782/938], Loss: 0.0589\n",
      "Epoch [4/10], Step [784/938], Loss: 0.1224\n",
      "Epoch [4/10], Step [786/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [788/938], Loss: 0.0173\n",
      "Epoch [4/10], Step [790/938], Loss: 0.0067\n",
      "Epoch [4/10], Step [792/938], Loss: 0.0471\n",
      "Epoch [4/10], Step [794/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [796/938], Loss: 0.0059\n",
      "Epoch [4/10], Step [798/938], Loss: 0.0008\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0247\n",
      "Epoch [4/10], Step [802/938], Loss: 0.0135\n",
      "Epoch [4/10], Step [804/938], Loss: 0.0037\n",
      "Epoch [4/10], Step [806/938], Loss: 0.0716\n",
      "Epoch [4/10], Step [808/938], Loss: 0.0014\n",
      "Epoch [4/10], Step [810/938], Loss: 0.0082\n",
      "Epoch [4/10], Step [812/938], Loss: 0.0051\n",
      "Epoch [4/10], Step [814/938], Loss: 0.1029\n",
      "Epoch [4/10], Step [816/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [818/938], Loss: 0.0145\n",
      "Epoch [4/10], Step [820/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [822/938], Loss: 0.0143\n",
      "Epoch [4/10], Step [824/938], Loss: 0.0075\n",
      "Epoch [4/10], Step [826/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [828/938], Loss: 0.0713\n",
      "Epoch [4/10], Step [830/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [832/938], Loss: 0.0047\n",
      "Epoch [4/10], Step [834/938], Loss: 0.0059\n",
      "Epoch [4/10], Step [836/938], Loss: 0.0033\n",
      "Epoch [4/10], Step [838/938], Loss: 0.0063\n",
      "Epoch [4/10], Step [840/938], Loss: 0.0034\n",
      "Epoch [4/10], Step [842/938], Loss: 0.0036\n",
      "Epoch [4/10], Step [844/938], Loss: 0.0165\n",
      "Epoch [4/10], Step [846/938], Loss: 0.0092\n",
      "Epoch [4/10], Step [848/938], Loss: 0.0047\n",
      "Epoch [4/10], Step [850/938], Loss: 0.0605\n",
      "Epoch [4/10], Step [852/938], Loss: 0.0173\n",
      "Epoch [4/10], Step [854/938], Loss: 0.0104\n",
      "Epoch [4/10], Step [856/938], Loss: 0.0011\n",
      "Epoch [4/10], Step [858/938], Loss: 0.0099\n",
      "Epoch [4/10], Step [860/938], Loss: 0.0112\n",
      "Epoch [4/10], Step [862/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [864/938], Loss: 0.0094\n",
      "Epoch [4/10], Step [866/938], Loss: 0.0025\n",
      "Epoch [4/10], Step [868/938], Loss: 0.0152\n",
      "Epoch [4/10], Step [870/938], Loss: 0.0023\n",
      "Epoch [4/10], Step [872/938], Loss: 0.0159\n",
      "Epoch [4/10], Step [874/938], Loss: 0.0006\n",
      "Epoch [4/10], Step [876/938], Loss: 0.0129\n",
      "Epoch [4/10], Step [878/938], Loss: 0.0287\n",
      "Epoch [4/10], Step [880/938], Loss: 0.0355\n",
      "Epoch [4/10], Step [882/938], Loss: 0.0008\n",
      "Epoch [4/10], Step [884/938], Loss: 0.0453\n",
      "Epoch [4/10], Step [886/938], Loss: 0.0225\n",
      "Epoch [4/10], Step [888/938], Loss: 0.0123\n",
      "Epoch [4/10], Step [890/938], Loss: 0.0017\n",
      "Epoch [4/10], Step [892/938], Loss: 0.1150\n",
      "Epoch [4/10], Step [894/938], Loss: 0.0180\n",
      "Epoch [4/10], Step [896/938], Loss: 0.0317\n",
      "Epoch [4/10], Step [898/938], Loss: 0.0153\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0045\n",
      "Epoch [4/10], Step [902/938], Loss: 0.0069\n",
      "Epoch [4/10], Step [904/938], Loss: 0.0046\n",
      "Epoch [4/10], Step [906/938], Loss: 0.0216\n",
      "Epoch [4/10], Step [908/938], Loss: 0.0955\n",
      "Epoch [4/10], Step [910/938], Loss: 0.0018\n",
      "Epoch [4/10], Step [912/938], Loss: 0.0085\n",
      "Epoch [4/10], Step [914/938], Loss: 0.0022\n",
      "Epoch [4/10], Step [916/938], Loss: 0.0031\n",
      "Epoch [4/10], Step [918/938], Loss: 0.1017\n",
      "Epoch [4/10], Step [920/938], Loss: 0.0160\n",
      "Epoch [4/10], Step [922/938], Loss: 0.0089\n",
      "Epoch [4/10], Step [924/938], Loss: 0.0027\n",
      "Epoch [4/10], Step [926/938], Loss: 0.0224\n",
      "Epoch [4/10], Step [928/938], Loss: 0.0026\n",
      "Epoch [4/10], Step [930/938], Loss: 0.0310\n",
      "Epoch [4/10], Step [932/938], Loss: 0.0179\n",
      "Epoch [4/10], Step [934/938], Loss: 0.0065\n",
      "Epoch [4/10], Step [936/938], Loss: 0.0005\n",
      "Epoch [4/10], Step [938/938], Loss: 0.0020\n",
      "Epoch [4/10], Loss: 0.0215\n",
      "Epoch [5/10], Step [2/938], Loss: 0.0121\n",
      "Epoch [5/10], Step [4/938], Loss: 0.0195\n",
      "Epoch [5/10], Step [6/938], Loss: 0.0130\n",
      "Epoch [5/10], Step [8/938], Loss: 0.0034\n",
      "Epoch [5/10], Step [10/938], Loss: 0.0443\n",
      "Epoch [5/10], Step [12/938], Loss: 0.0035\n",
      "Epoch [5/10], Step [14/938], Loss: 0.0275\n",
      "Epoch [5/10], Step [16/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [18/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [20/938], Loss: 0.0366\n",
      "Epoch [5/10], Step [22/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [24/938], Loss: 0.0180\n",
      "Epoch [5/10], Step [26/938], Loss: 0.0094\n",
      "Epoch [5/10], Step [28/938], Loss: 0.0286\n",
      "Epoch [5/10], Step [30/938], Loss: 0.0118\n",
      "Epoch [5/10], Step [32/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [34/938], Loss: 0.0176\n",
      "Epoch [5/10], Step [36/938], Loss: 0.0181\n",
      "Epoch [5/10], Step [38/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [40/938], Loss: 0.0119\n",
      "Epoch [5/10], Step [42/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [44/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [46/938], Loss: 0.0255\n",
      "Epoch [5/10], Step [48/938], Loss: 0.0054\n",
      "Epoch [5/10], Step [50/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [52/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [54/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [56/938], Loss: 0.0730\n",
      "Epoch [5/10], Step [58/938], Loss: 0.0073\n",
      "Epoch [5/10], Step [60/938], Loss: 0.0339\n",
      "Epoch [5/10], Step [62/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [64/938], Loss: 0.0101\n",
      "Epoch [5/10], Step [66/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [68/938], Loss: 0.0005\n",
      "Epoch [5/10], Step [70/938], Loss: 0.0299\n",
      "Epoch [5/10], Step [72/938], Loss: 0.0041\n",
      "Epoch [5/10], Step [74/938], Loss: 0.0069\n",
      "Epoch [5/10], Step [76/938], Loss: 0.0342\n",
      "Epoch [5/10], Step [78/938], Loss: 0.0125\n",
      "Epoch [5/10], Step [80/938], Loss: 0.0048\n",
      "Epoch [5/10], Step [82/938], Loss: 0.0064\n",
      "Epoch [5/10], Step [84/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [86/938], Loss: 0.0876\n",
      "Epoch [5/10], Step [88/938], Loss: 0.0588\n",
      "Epoch [5/10], Step [90/938], Loss: 0.0085\n",
      "Epoch [5/10], Step [92/938], Loss: 0.0055\n",
      "Epoch [5/10], Step [94/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [96/938], Loss: 0.0426\n",
      "Epoch [5/10], Step [98/938], Loss: 0.0069\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [102/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [104/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [106/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [108/938], Loss: 0.0032\n",
      "Epoch [5/10], Step [110/938], Loss: 0.0091\n",
      "Epoch [5/10], Step [112/938], Loss: 0.0350\n",
      "Epoch [5/10], Step [114/938], Loss: 0.0296\n",
      "Epoch [5/10], Step [116/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [118/938], Loss: 0.0047\n",
      "Epoch [5/10], Step [120/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [122/938], Loss: 0.0001\n",
      "Epoch [5/10], Step [124/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [126/938], Loss: 0.0050\n",
      "Epoch [5/10], Step [128/938], Loss: 0.0074\n",
      "Epoch [5/10], Step [130/938], Loss: 0.0109\n",
      "Epoch [5/10], Step [132/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [134/938], Loss: 0.0089\n",
      "Epoch [5/10], Step [136/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [138/938], Loss: 0.0052\n",
      "Epoch [5/10], Step [140/938], Loss: 0.0617\n",
      "Epoch [5/10], Step [142/938], Loss: 0.0310\n",
      "Epoch [5/10], Step [144/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [146/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [148/938], Loss: 0.0366\n",
      "Epoch [5/10], Step [150/938], Loss: 0.0801\n",
      "Epoch [5/10], Step [152/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [154/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [156/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [158/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [160/938], Loss: 0.0863\n",
      "Epoch [5/10], Step [162/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [164/938], Loss: 0.0140\n",
      "Epoch [5/10], Step [166/938], Loss: 0.0162\n",
      "Epoch [5/10], Step [168/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [170/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [172/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [174/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [176/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [178/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [180/938], Loss: 0.0090\n",
      "Epoch [5/10], Step [182/938], Loss: 0.0071\n",
      "Epoch [5/10], Step [184/938], Loss: 0.0113\n",
      "Epoch [5/10], Step [186/938], Loss: 0.0115\n",
      "Epoch [5/10], Step [188/938], Loss: 0.0124\n",
      "Epoch [5/10], Step [190/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [192/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [194/938], Loss: 0.0748\n",
      "Epoch [5/10], Step [196/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [198/938], Loss: 0.0080\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0087\n",
      "Epoch [5/10], Step [202/938], Loss: 0.0256\n",
      "Epoch [5/10], Step [204/938], Loss: 0.0117\n",
      "Epoch [5/10], Step [206/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [208/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [210/938], Loss: 0.0249\n",
      "Epoch [5/10], Step [212/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [214/938], Loss: 0.0849\n",
      "Epoch [5/10], Step [216/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [218/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [220/938], Loss: 0.0166\n",
      "Epoch [5/10], Step [222/938], Loss: 0.0063\n",
      "Epoch [5/10], Step [224/938], Loss: 0.0058\n",
      "Epoch [5/10], Step [226/938], Loss: 0.0342\n",
      "Epoch [5/10], Step [228/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [230/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [232/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [234/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [236/938], Loss: 0.0485\n",
      "Epoch [5/10], Step [238/938], Loss: 0.0220\n",
      "Epoch [5/10], Step [240/938], Loss: 0.0069\n",
      "Epoch [5/10], Step [242/938], Loss: 0.0124\n",
      "Epoch [5/10], Step [244/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [246/938], Loss: 0.0240\n",
      "Epoch [5/10], Step [248/938], Loss: 0.0573\n",
      "Epoch [5/10], Step [250/938], Loss: 0.0109\n",
      "Epoch [5/10], Step [252/938], Loss: 0.0456\n",
      "Epoch [5/10], Step [254/938], Loss: 0.0182\n",
      "Epoch [5/10], Step [256/938], Loss: 0.0305\n",
      "Epoch [5/10], Step [258/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [260/938], Loss: 0.0050\n",
      "Epoch [5/10], Step [262/938], Loss: 0.0116\n",
      "Epoch [5/10], Step [264/938], Loss: 0.0039\n",
      "Epoch [5/10], Step [266/938], Loss: 0.0130\n",
      "Epoch [5/10], Step [268/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [270/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [272/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [274/938], Loss: 0.0057\n",
      "Epoch [5/10], Step [276/938], Loss: 0.0131\n",
      "Epoch [5/10], Step [278/938], Loss: 0.0785\n",
      "Epoch [5/10], Step [280/938], Loss: 0.0249\n",
      "Epoch [5/10], Step [282/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [284/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [286/938], Loss: 0.0146\n",
      "Epoch [5/10], Step [288/938], Loss: 0.0330\n",
      "Epoch [5/10], Step [290/938], Loss: 0.0153\n",
      "Epoch [5/10], Step [292/938], Loss: 0.0228\n",
      "Epoch [5/10], Step [294/938], Loss: 0.0428\n",
      "Epoch [5/10], Step [296/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [298/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0495\n",
      "Epoch [5/10], Step [302/938], Loss: 0.0087\n",
      "Epoch [5/10], Step [304/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [306/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [308/938], Loss: 0.0159\n",
      "Epoch [5/10], Step [310/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [312/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [314/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [316/938], Loss: 0.0066\n",
      "Epoch [5/10], Step [318/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [320/938], Loss: 0.0291\n",
      "Epoch [5/10], Step [322/938], Loss: 0.0112\n",
      "Epoch [5/10], Step [324/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [326/938], Loss: 0.0161\n",
      "Epoch [5/10], Step [328/938], Loss: 0.0043\n",
      "Epoch [5/10], Step [330/938], Loss: 0.0237\n",
      "Epoch [5/10], Step [332/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [334/938], Loss: 0.0080\n",
      "Epoch [5/10], Step [336/938], Loss: 0.0065\n",
      "Epoch [5/10], Step [338/938], Loss: 0.0083\n",
      "Epoch [5/10], Step [340/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [342/938], Loss: 0.0046\n",
      "Epoch [5/10], Step [344/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [346/938], Loss: 0.0163\n",
      "Epoch [5/10], Step [348/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [350/938], Loss: 0.0287\n",
      "Epoch [5/10], Step [352/938], Loss: 0.0399\n",
      "Epoch [5/10], Step [354/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [356/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [358/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [360/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [362/938], Loss: 0.0060\n",
      "Epoch [5/10], Step [364/938], Loss: 0.0041\n",
      "Epoch [5/10], Step [366/938], Loss: 0.0078\n",
      "Epoch [5/10], Step [368/938], Loss: 0.0118\n",
      "Epoch [5/10], Step [370/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [372/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [374/938], Loss: 0.0194\n",
      "Epoch [5/10], Step [376/938], Loss: 0.0274\n",
      "Epoch [5/10], Step [378/938], Loss: 0.1037\n",
      "Epoch [5/10], Step [380/938], Loss: 0.0262\n",
      "Epoch [5/10], Step [382/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [384/938], Loss: 0.0126\n",
      "Epoch [5/10], Step [386/938], Loss: 0.0329\n",
      "Epoch [5/10], Step [388/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [390/938], Loss: 0.0887\n",
      "Epoch [5/10], Step [392/938], Loss: 0.0117\n",
      "Epoch [5/10], Step [394/938], Loss: 0.0177\n",
      "Epoch [5/10], Step [396/938], Loss: 0.0092\n",
      "Epoch [5/10], Step [398/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0287\n",
      "Epoch [5/10], Step [402/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [404/938], Loss: 0.0108\n",
      "Epoch [5/10], Step [406/938], Loss: 0.0121\n",
      "Epoch [5/10], Step [408/938], Loss: 0.0053\n",
      "Epoch [5/10], Step [410/938], Loss: 0.0134\n",
      "Epoch [5/10], Step [412/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [414/938], Loss: 0.0021\n",
      "Epoch [5/10], Step [416/938], Loss: 0.0073\n",
      "Epoch [5/10], Step [418/938], Loss: 0.0148\n",
      "Epoch [5/10], Step [420/938], Loss: 0.0057\n",
      "Epoch [5/10], Step [422/938], Loss: 0.0290\n",
      "Epoch [5/10], Step [424/938], Loss: 0.0054\n",
      "Epoch [5/10], Step [426/938], Loss: 0.0546\n",
      "Epoch [5/10], Step [428/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [430/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [432/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [434/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [436/938], Loss: 0.0077\n",
      "Epoch [5/10], Step [438/938], Loss: 0.0052\n",
      "Epoch [5/10], Step [440/938], Loss: 0.0161\n",
      "Epoch [5/10], Step [442/938], Loss: 0.0132\n",
      "Epoch [5/10], Step [444/938], Loss: 0.0253\n",
      "Epoch [5/10], Step [446/938], Loss: 0.0234\n",
      "Epoch [5/10], Step [448/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [450/938], Loss: 0.0141\n",
      "Epoch [5/10], Step [452/938], Loss: 0.0058\n",
      "Epoch [5/10], Step [454/938], Loss: 0.0143\n",
      "Epoch [5/10], Step [456/938], Loss: 0.0200\n",
      "Epoch [5/10], Step [458/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [460/938], Loss: 0.0132\n",
      "Epoch [5/10], Step [462/938], Loss: 0.0696\n",
      "Epoch [5/10], Step [464/938], Loss: 0.0178\n",
      "Epoch [5/10], Step [466/938], Loss: 0.0712\n",
      "Epoch [5/10], Step [468/938], Loss: 0.0156\n",
      "Epoch [5/10], Step [470/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [472/938], Loss: 0.0154\n",
      "Epoch [5/10], Step [474/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [476/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [478/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [480/938], Loss: 0.0283\n",
      "Epoch [5/10], Step [482/938], Loss: 0.0149\n",
      "Epoch [5/10], Step [484/938], Loss: 0.0421\n",
      "Epoch [5/10], Step [486/938], Loss: 0.0005\n",
      "Epoch [5/10], Step [488/938], Loss: 0.0955\n",
      "Epoch [5/10], Step [490/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [492/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [494/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [496/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [498/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0709\n",
      "Epoch [5/10], Step [502/938], Loss: 0.0015\n",
      "Epoch [5/10], Step [504/938], Loss: 0.0094\n",
      "Epoch [5/10], Step [506/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [508/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [510/938], Loss: 0.0300\n",
      "Epoch [5/10], Step [512/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [514/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [516/938], Loss: 0.0046\n",
      "Epoch [5/10], Step [518/938], Loss: 0.0623\n",
      "Epoch [5/10], Step [520/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [522/938], Loss: 0.0445\n",
      "Epoch [5/10], Step [524/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [526/938], Loss: 0.0005\n",
      "Epoch [5/10], Step [528/938], Loss: 0.0058\n",
      "Epoch [5/10], Step [530/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [532/938], Loss: 0.0141\n",
      "Epoch [5/10], Step [534/938], Loss: 0.0257\n",
      "Epoch [5/10], Step [536/938], Loss: 0.0460\n",
      "Epoch [5/10], Step [538/938], Loss: 0.0058\n",
      "Epoch [5/10], Step [540/938], Loss: 0.0209\n",
      "Epoch [5/10], Step [542/938], Loss: 0.0183\n",
      "Epoch [5/10], Step [544/938], Loss: 0.0503\n",
      "Epoch [5/10], Step [546/938], Loss: 0.0082\n",
      "Epoch [5/10], Step [548/938], Loss: 0.0113\n",
      "Epoch [5/10], Step [550/938], Loss: 0.0078\n",
      "Epoch [5/10], Step [552/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [554/938], Loss: 0.0102\n",
      "Epoch [5/10], Step [556/938], Loss: 0.0221\n",
      "Epoch [5/10], Step [558/938], Loss: 0.0124\n",
      "Epoch [5/10], Step [560/938], Loss: 0.0168\n",
      "Epoch [5/10], Step [562/938], Loss: 0.0283\n",
      "Epoch [5/10], Step [564/938], Loss: 0.0253\n",
      "Epoch [5/10], Step [566/938], Loss: 0.0071\n",
      "Epoch [5/10], Step [568/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [570/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [572/938], Loss: 0.0035\n",
      "Epoch [5/10], Step [574/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [576/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [578/938], Loss: 0.0248\n",
      "Epoch [5/10], Step [580/938], Loss: 0.0828\n",
      "Epoch [5/10], Step [582/938], Loss: 0.0005\n",
      "Epoch [5/10], Step [584/938], Loss: 0.0749\n",
      "Epoch [5/10], Step [586/938], Loss: 0.0002\n",
      "Epoch [5/10], Step [588/938], Loss: 0.0277\n",
      "Epoch [5/10], Step [590/938], Loss: 0.0073\n",
      "Epoch [5/10], Step [592/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [594/938], Loss: 0.0533\n",
      "Epoch [5/10], Step [596/938], Loss: 0.0103\n",
      "Epoch [5/10], Step [598/938], Loss: 0.0031\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [602/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [604/938], Loss: 0.0082\n",
      "Epoch [5/10], Step [606/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [608/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [610/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [612/938], Loss: 0.0202\n",
      "Epoch [5/10], Step [614/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [616/938], Loss: 0.0706\n",
      "Epoch [5/10], Step [618/938], Loss: 0.0044\n",
      "Epoch [5/10], Step [620/938], Loss: 0.0131\n",
      "Epoch [5/10], Step [622/938], Loss: 0.0109\n",
      "Epoch [5/10], Step [624/938], Loss: 0.0091\n",
      "Epoch [5/10], Step [626/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [628/938], Loss: 0.0108\n",
      "Epoch [5/10], Step [630/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [632/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [634/938], Loss: 0.0017\n",
      "Epoch [5/10], Step [636/938], Loss: 0.0027\n",
      "Epoch [5/10], Step [638/938], Loss: 0.0145\n",
      "Epoch [5/10], Step [640/938], Loss: 0.0065\n",
      "Epoch [5/10], Step [642/938], Loss: 0.0083\n",
      "Epoch [5/10], Step [644/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [646/938], Loss: 0.0071\n",
      "Epoch [5/10], Step [648/938], Loss: 0.0057\n",
      "Epoch [5/10], Step [650/938], Loss: 0.0643\n",
      "Epoch [5/10], Step [652/938], Loss: 0.0040\n",
      "Epoch [5/10], Step [654/938], Loss: 0.2106\n",
      "Epoch [5/10], Step [656/938], Loss: 0.0097\n",
      "Epoch [5/10], Step [658/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [660/938], Loss: 0.0334\n",
      "Epoch [5/10], Step [662/938], Loss: 0.0028\n",
      "Epoch [5/10], Step [664/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [666/938], Loss: 0.0978\n",
      "Epoch [5/10], Step [668/938], Loss: 0.0292\n",
      "Epoch [5/10], Step [670/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [672/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [674/938], Loss: 0.0329\n",
      "Epoch [5/10], Step [676/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [678/938], Loss: 0.0223\n",
      "Epoch [5/10], Step [680/938], Loss: 0.0070\n",
      "Epoch [5/10], Step [682/938], Loss: 0.0260\n",
      "Epoch [5/10], Step [684/938], Loss: 0.0080\n",
      "Epoch [5/10], Step [686/938], Loss: 0.0413\n",
      "Epoch [5/10], Step [688/938], Loss: 0.0765\n",
      "Epoch [5/10], Step [690/938], Loss: 0.0053\n",
      "Epoch [5/10], Step [692/938], Loss: 0.0329\n",
      "Epoch [5/10], Step [694/938], Loss: 0.0338\n",
      "Epoch [5/10], Step [696/938], Loss: 0.0060\n",
      "Epoch [5/10], Step [698/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0013\n",
      "Epoch [5/10], Step [702/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [704/938], Loss: 0.0106\n",
      "Epoch [5/10], Step [706/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [708/938], Loss: 0.0079\n",
      "Epoch [5/10], Step [710/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [712/938], Loss: 0.0182\n",
      "Epoch [5/10], Step [714/938], Loss: 0.0053\n",
      "Epoch [5/10], Step [716/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [718/938], Loss: 0.0257\n",
      "Epoch [5/10], Step [720/938], Loss: 0.0433\n",
      "Epoch [5/10], Step [722/938], Loss: 0.0048\n",
      "Epoch [5/10], Step [724/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [726/938], Loss: 0.0851\n",
      "Epoch [5/10], Step [728/938], Loss: 0.0237\n",
      "Epoch [5/10], Step [730/938], Loss: 0.0022\n",
      "Epoch [5/10], Step [732/938], Loss: 0.0304\n",
      "Epoch [5/10], Step [734/938], Loss: 0.0047\n",
      "Epoch [5/10], Step [736/938], Loss: 0.0257\n",
      "Epoch [5/10], Step [738/938], Loss: 0.0146\n",
      "Epoch [5/10], Step [740/938], Loss: 0.0074\n",
      "Epoch [5/10], Step [742/938], Loss: 0.0771\n",
      "Epoch [5/10], Step [744/938], Loss: 0.0145\n",
      "Epoch [5/10], Step [746/938], Loss: 0.1003\n",
      "Epoch [5/10], Step [748/938], Loss: 0.0246\n",
      "Epoch [5/10], Step [750/938], Loss: 0.0298\n",
      "Epoch [5/10], Step [752/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [754/938], Loss: 0.0019\n",
      "Epoch [5/10], Step [756/938], Loss: 0.0025\n",
      "Epoch [5/10], Step [758/938], Loss: 0.0691\n",
      "Epoch [5/10], Step [760/938], Loss: 0.0314\n",
      "Epoch [5/10], Step [762/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [764/938], Loss: 0.0116\n",
      "Epoch [5/10], Step [766/938], Loss: 0.0020\n",
      "Epoch [5/10], Step [768/938], Loss: 0.0194\n",
      "Epoch [5/10], Step [770/938], Loss: 0.0089\n",
      "Epoch [5/10], Step [772/938], Loss: 0.0540\n",
      "Epoch [5/10], Step [774/938], Loss: 0.0068\n",
      "Epoch [5/10], Step [776/938], Loss: 0.0038\n",
      "Epoch [5/10], Step [778/938], Loss: 0.0180\n",
      "Epoch [5/10], Step [780/938], Loss: 0.0428\n",
      "Epoch [5/10], Step [782/938], Loss: 0.1779\n",
      "Epoch [5/10], Step [784/938], Loss: 0.0010\n",
      "Epoch [5/10], Step [786/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [788/938], Loss: 0.0004\n",
      "Epoch [5/10], Step [790/938], Loss: 0.0023\n",
      "Epoch [5/10], Step [792/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [794/938], Loss: 0.0152\n",
      "Epoch [5/10], Step [796/938], Loss: 0.0039\n",
      "Epoch [5/10], Step [798/938], Loss: 0.0157\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0200\n",
      "Epoch [5/10], Step [802/938], Loss: 0.0006\n",
      "Epoch [5/10], Step [804/938], Loss: 0.0150\n",
      "Epoch [5/10], Step [806/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [808/938], Loss: 0.0075\n",
      "Epoch [5/10], Step [810/938], Loss: 0.0146\n",
      "Epoch [5/10], Step [812/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [814/938], Loss: 0.0008\n",
      "Epoch [5/10], Step [816/938], Loss: 0.0029\n",
      "Epoch [5/10], Step [818/938], Loss: 0.0108\n",
      "Epoch [5/10], Step [820/938], Loss: 0.0052\n",
      "Epoch [5/10], Step [822/938], Loss: 0.0016\n",
      "Epoch [5/10], Step [824/938], Loss: 0.0341\n",
      "Epoch [5/10], Step [826/938], Loss: 0.0117\n",
      "Epoch [5/10], Step [828/938], Loss: 0.0024\n",
      "Epoch [5/10], Step [830/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [832/938], Loss: 0.0011\n",
      "Epoch [5/10], Step [834/938], Loss: 0.0418\n",
      "Epoch [5/10], Step [836/938], Loss: 0.0406\n",
      "Epoch [5/10], Step [838/938], Loss: 0.0070\n",
      "Epoch [5/10], Step [840/938], Loss: 0.0001\n",
      "Epoch [5/10], Step [842/938], Loss: 0.0318\n",
      "Epoch [5/10], Step [844/938], Loss: 0.0707\n",
      "Epoch [5/10], Step [846/938], Loss: 0.0300\n",
      "Epoch [5/10], Step [848/938], Loss: 0.0061\n",
      "Epoch [5/10], Step [850/938], Loss: 0.0381\n",
      "Epoch [5/10], Step [852/938], Loss: 0.0114\n",
      "Epoch [5/10], Step [854/938], Loss: 0.0320\n",
      "Epoch [5/10], Step [856/938], Loss: 0.0056\n",
      "Epoch [5/10], Step [858/938], Loss: 0.0014\n",
      "Epoch [5/10], Step [860/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [862/938], Loss: 0.0622\n",
      "Epoch [5/10], Step [864/938], Loss: 0.0059\n",
      "Epoch [5/10], Step [866/938], Loss: 0.0092\n",
      "Epoch [5/10], Step [868/938], Loss: 0.0259\n",
      "Epoch [5/10], Step [870/938], Loss: 0.0003\n",
      "Epoch [5/10], Step [872/938], Loss: 0.0041\n",
      "Epoch [5/10], Step [874/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [876/938], Loss: 0.0235\n",
      "Epoch [5/10], Step [878/938], Loss: 0.0051\n",
      "Epoch [5/10], Step [880/938], Loss: 0.0018\n",
      "Epoch [5/10], Step [882/938], Loss: 0.0026\n",
      "Epoch [5/10], Step [884/938], Loss: 0.0030\n",
      "Epoch [5/10], Step [886/938], Loss: 0.0063\n",
      "Epoch [5/10], Step [888/938], Loss: 0.0975\n",
      "Epoch [5/10], Step [890/938], Loss: 0.0395\n",
      "Epoch [5/10], Step [892/938], Loss: 0.0009\n",
      "Epoch [5/10], Step [894/938], Loss: 0.0327\n",
      "Epoch [5/10], Step [896/938], Loss: 0.0096\n",
      "Epoch [5/10], Step [898/938], Loss: 0.0012\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0626\n",
      "Epoch [5/10], Step [902/938], Loss: 0.0007\n",
      "Epoch [5/10], Step [904/938], Loss: 0.0449\n",
      "Epoch [5/10], Step [906/938], Loss: 0.0975\n",
      "Epoch [5/10], Step [908/938], Loss: 0.0384\n",
      "Epoch [5/10], Step [910/938], Loss: 0.0662\n",
      "Epoch [5/10], Step [912/938], Loss: 0.0223\n",
      "Epoch [5/10], Step [914/938], Loss: 0.0060\n",
      "Epoch [5/10], Step [916/938], Loss: 0.0136\n",
      "Epoch [5/10], Step [918/938], Loss: 0.0363\n",
      "Epoch [5/10], Step [920/938], Loss: 0.0280\n",
      "Epoch [5/10], Step [922/938], Loss: 0.0062\n",
      "Epoch [5/10], Step [924/938], Loss: 0.0085\n",
      "Epoch [5/10], Step [926/938], Loss: 0.0060\n",
      "Epoch [5/10], Step [928/938], Loss: 0.0049\n",
      "Epoch [5/10], Step [930/938], Loss: 0.0310\n",
      "Epoch [5/10], Step [932/938], Loss: 0.0587\n",
      "Epoch [5/10], Step [934/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [936/938], Loss: 0.0033\n",
      "Epoch [5/10], Step [938/938], Loss: 0.0170\n",
      "Epoch [5/10], Loss: 0.0175\n",
      "Epoch [6/10], Step [2/938], Loss: 0.0027\n",
      "Epoch [6/10], Step [4/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [6/938], Loss: 0.0039\n",
      "Epoch [6/10], Step [8/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [10/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [12/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [14/938], Loss: 0.0440\n",
      "Epoch [6/10], Step [16/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [18/938], Loss: 0.0043\n",
      "Epoch [6/10], Step [20/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [22/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [24/938], Loss: 0.0020\n",
      "Epoch [6/10], Step [26/938], Loss: 0.0144\n",
      "Epoch [6/10], Step [28/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [30/938], Loss: 0.0164\n",
      "Epoch [6/10], Step [32/938], Loss: 0.0075\n",
      "Epoch [6/10], Step [34/938], Loss: 0.0044\n",
      "Epoch [6/10], Step [36/938], Loss: 0.0047\n",
      "Epoch [6/10], Step [38/938], Loss: 0.0041\n",
      "Epoch [6/10], Step [40/938], Loss: 0.0378\n",
      "Epoch [6/10], Step [42/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [44/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [46/938], Loss: 0.0074\n",
      "Epoch [6/10], Step [48/938], Loss: 0.0444\n",
      "Epoch [6/10], Step [50/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [52/938], Loss: 0.0408\n",
      "Epoch [6/10], Step [54/938], Loss: 0.0061\n",
      "Epoch [6/10], Step [56/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [58/938], Loss: 0.0305\n",
      "Epoch [6/10], Step [60/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [62/938], Loss: 0.0517\n",
      "Epoch [6/10], Step [64/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [66/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [68/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [70/938], Loss: 0.0121\n",
      "Epoch [6/10], Step [72/938], Loss: 0.0066\n",
      "Epoch [6/10], Step [74/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [76/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [78/938], Loss: 0.0347\n",
      "Epoch [6/10], Step [80/938], Loss: 0.0829\n",
      "Epoch [6/10], Step [82/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [84/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [86/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [88/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [90/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [92/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [94/938], Loss: 0.0336\n",
      "Epoch [6/10], Step [96/938], Loss: 0.0062\n",
      "Epoch [6/10], Step [98/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [102/938], Loss: 0.0235\n",
      "Epoch [6/10], Step [104/938], Loss: 0.0809\n",
      "Epoch [6/10], Step [106/938], Loss: 0.0594\n",
      "Epoch [6/10], Step [108/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [110/938], Loss: 0.0105\n",
      "Epoch [6/10], Step [112/938], Loss: 0.0152\n",
      "Epoch [6/10], Step [114/938], Loss: 0.0053\n",
      "Epoch [6/10], Step [116/938], Loss: 0.0128\n",
      "Epoch [6/10], Step [118/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [120/938], Loss: 0.0030\n",
      "Epoch [6/10], Step [122/938], Loss: 0.0022\n",
      "Epoch [6/10], Step [124/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [126/938], Loss: 0.0063\n",
      "Epoch [6/10], Step [128/938], Loss: 0.0034\n",
      "Epoch [6/10], Step [130/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [132/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [134/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [136/938], Loss: 0.0036\n",
      "Epoch [6/10], Step [138/938], Loss: 0.0155\n",
      "Epoch [6/10], Step [140/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [142/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [144/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [146/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [148/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [150/938], Loss: 0.0627\n",
      "Epoch [6/10], Step [152/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [154/938], Loss: 0.0132\n",
      "Epoch [6/10], Step [156/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [158/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [160/938], Loss: 0.0255\n",
      "Epoch [6/10], Step [162/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [164/938], Loss: 0.0122\n",
      "Epoch [6/10], Step [166/938], Loss: 0.0088\n",
      "Epoch [6/10], Step [168/938], Loss: 0.0110\n",
      "Epoch [6/10], Step [170/938], Loss: 0.0031\n",
      "Epoch [6/10], Step [172/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [174/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [176/938], Loss: 0.0083\n",
      "Epoch [6/10], Step [178/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [180/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [182/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [184/938], Loss: 0.0062\n",
      "Epoch [6/10], Step [186/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [188/938], Loss: 0.0061\n",
      "Epoch [6/10], Step [190/938], Loss: 0.0057\n",
      "Epoch [6/10], Step [192/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [194/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [196/938], Loss: 0.0273\n",
      "Epoch [6/10], Step [198/938], Loss: 0.0065\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0102\n",
      "Epoch [6/10], Step [202/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [204/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [206/938], Loss: 0.0028\n",
      "Epoch [6/10], Step [208/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [210/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [212/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [214/938], Loss: 0.0073\n",
      "Epoch [6/10], Step [216/938], Loss: 0.0135\n",
      "Epoch [6/10], Step [218/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [220/938], Loss: 0.0064\n",
      "Epoch [6/10], Step [222/938], Loss: 0.0031\n",
      "Epoch [6/10], Step [224/938], Loss: 0.0308\n",
      "Epoch [6/10], Step [226/938], Loss: 0.0084\n",
      "Epoch [6/10], Step [228/938], Loss: 0.0035\n",
      "Epoch [6/10], Step [230/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [232/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [234/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [236/938], Loss: 0.0505\n",
      "Epoch [6/10], Step [238/938], Loss: 0.0241\n",
      "Epoch [6/10], Step [240/938], Loss: 0.0053\n",
      "Epoch [6/10], Step [242/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [244/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [246/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [248/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [250/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [252/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [254/938], Loss: 0.0181\n",
      "Epoch [6/10], Step [256/938], Loss: 0.0986\n",
      "Epoch [6/10], Step [258/938], Loss: 0.0169\n",
      "Epoch [6/10], Step [260/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [262/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [264/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [266/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [268/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [270/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [272/938], Loss: 0.0152\n",
      "Epoch [6/10], Step [274/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [276/938], Loss: 0.0094\n",
      "Epoch [6/10], Step [278/938], Loss: 0.0027\n",
      "Epoch [6/10], Step [280/938], Loss: 0.0343\n",
      "Epoch [6/10], Step [282/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [284/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [286/938], Loss: 0.0312\n",
      "Epoch [6/10], Step [288/938], Loss: 0.0033\n",
      "Epoch [6/10], Step [290/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [292/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [294/938], Loss: 0.0076\n",
      "Epoch [6/10], Step [296/938], Loss: 0.0110\n",
      "Epoch [6/10], Step [298/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0410\n",
      "Epoch [6/10], Step [302/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [304/938], Loss: 0.0062\n",
      "Epoch [6/10], Step [306/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [308/938], Loss: 0.0086\n",
      "Epoch [6/10], Step [310/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [312/938], Loss: 0.0170\n",
      "Epoch [6/10], Step [314/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [316/938], Loss: 0.0102\n",
      "Epoch [6/10], Step [318/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [320/938], Loss: 0.0355\n",
      "Epoch [6/10], Step [322/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [324/938], Loss: 0.0023\n",
      "Epoch [6/10], Step [326/938], Loss: 0.0538\n",
      "Epoch [6/10], Step [328/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [330/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [332/938], Loss: 0.0051\n",
      "Epoch [6/10], Step [334/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [336/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [338/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [340/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [342/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [344/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [346/938], Loss: 0.0144\n",
      "Epoch [6/10], Step [348/938], Loss: 0.0051\n",
      "Epoch [6/10], Step [350/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [352/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [354/938], Loss: 0.0230\n",
      "Epoch [6/10], Step [356/938], Loss: 0.0054\n",
      "Epoch [6/10], Step [358/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [360/938], Loss: 0.0086\n",
      "Epoch [6/10], Step [362/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [364/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [366/938], Loss: 0.0074\n",
      "Epoch [6/10], Step [368/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [370/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [372/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [374/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [376/938], Loss: 0.0032\n",
      "Epoch [6/10], Step [378/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [380/938], Loss: 0.0079\n",
      "Epoch [6/10], Step [382/938], Loss: 0.0654\n",
      "Epoch [6/10], Step [384/938], Loss: 0.0000\n",
      "Epoch [6/10], Step [386/938], Loss: 0.0023\n",
      "Epoch [6/10], Step [388/938], Loss: 0.0162\n",
      "Epoch [6/10], Step [390/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [392/938], Loss: 0.0050\n",
      "Epoch [6/10], Step [394/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [396/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [398/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [402/938], Loss: 0.0094\n",
      "Epoch [6/10], Step [404/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [406/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [408/938], Loss: 0.0433\n",
      "Epoch [6/10], Step [410/938], Loss: 0.0039\n",
      "Epoch [6/10], Step [412/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [414/938], Loss: 0.0247\n",
      "Epoch [6/10], Step [416/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [418/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [420/938], Loss: 0.0109\n",
      "Epoch [6/10], Step [422/938], Loss: 0.0017\n",
      "Epoch [6/10], Step [424/938], Loss: 0.0244\n",
      "Epoch [6/10], Step [426/938], Loss: 0.0073\n",
      "Epoch [6/10], Step [428/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [430/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [432/938], Loss: 0.0103\n",
      "Epoch [6/10], Step [434/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [436/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [438/938], Loss: 0.0904\n",
      "Epoch [6/10], Step [440/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [442/938], Loss: 0.0755\n",
      "Epoch [6/10], Step [444/938], Loss: 0.0111\n",
      "Epoch [6/10], Step [446/938], Loss: 0.0209\n",
      "Epoch [6/10], Step [448/938], Loss: 0.0081\n",
      "Epoch [6/10], Step [450/938], Loss: 0.0024\n",
      "Epoch [6/10], Step [452/938], Loss: 0.0127\n",
      "Epoch [6/10], Step [454/938], Loss: 0.0434\n",
      "Epoch [6/10], Step [456/938], Loss: 0.0584\n",
      "Epoch [6/10], Step [458/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [460/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [462/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [464/938], Loss: 0.0052\n",
      "Epoch [6/10], Step [466/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [468/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [470/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [472/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [474/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [476/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [478/938], Loss: 0.0050\n",
      "Epoch [6/10], Step [480/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [482/938], Loss: 0.0273\n",
      "Epoch [6/10], Step [484/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [486/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [488/938], Loss: 0.0265\n",
      "Epoch [6/10], Step [490/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [492/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [494/938], Loss: 0.0038\n",
      "Epoch [6/10], Step [496/938], Loss: 0.0221\n",
      "Epoch [6/10], Step [498/938], Loss: 0.0104\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0361\n",
      "Epoch [6/10], Step [502/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [504/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [506/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [508/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [510/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [512/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [514/938], Loss: 0.0057\n",
      "Epoch [6/10], Step [516/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [518/938], Loss: 0.0055\n",
      "Epoch [6/10], Step [520/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [522/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [524/938], Loss: 0.0124\n",
      "Epoch [6/10], Step [526/938], Loss: 0.0058\n",
      "Epoch [6/10], Step [528/938], Loss: 0.0682\n",
      "Epoch [6/10], Step [530/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [532/938], Loss: 0.0686\n",
      "Epoch [6/10], Step [534/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [536/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [538/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [540/938], Loss: 0.0536\n",
      "Epoch [6/10], Step [542/938], Loss: 0.0298\n",
      "Epoch [6/10], Step [544/938], Loss: 0.0206\n",
      "Epoch [6/10], Step [546/938], Loss: 0.0181\n",
      "Epoch [6/10], Step [548/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [550/938], Loss: 0.0635\n",
      "Epoch [6/10], Step [552/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [554/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [556/938], Loss: 0.1051\n",
      "Epoch [6/10], Step [558/938], Loss: 0.0082\n",
      "Epoch [6/10], Step [560/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [562/938], Loss: 0.0232\n",
      "Epoch [6/10], Step [564/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [566/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [568/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [570/938], Loss: 0.0048\n",
      "Epoch [6/10], Step [572/938], Loss: 0.0071\n",
      "Epoch [6/10], Step [574/938], Loss: 0.0259\n",
      "Epoch [6/10], Step [576/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [578/938], Loss: 0.0052\n",
      "Epoch [6/10], Step [580/938], Loss: 0.0151\n",
      "Epoch [6/10], Step [582/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [584/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [586/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [588/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [590/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [592/938], Loss: 0.0680\n",
      "Epoch [6/10], Step [594/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [596/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [598/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0061\n",
      "Epoch [6/10], Step [602/938], Loss: 0.0075\n",
      "Epoch [6/10], Step [604/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [606/938], Loss: 0.0640\n",
      "Epoch [6/10], Step [608/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [610/938], Loss: 0.0539\n",
      "Epoch [6/10], Step [612/938], Loss: 0.1260\n",
      "Epoch [6/10], Step [614/938], Loss: 0.0172\n",
      "Epoch [6/10], Step [616/938], Loss: 0.0095\n",
      "Epoch [6/10], Step [618/938], Loss: 0.0125\n",
      "Epoch [6/10], Step [620/938], Loss: 0.0589\n",
      "Epoch [6/10], Step [622/938], Loss: 0.0089\n",
      "Epoch [6/10], Step [624/938], Loss: 0.0092\n",
      "Epoch [6/10], Step [626/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [628/938], Loss: 0.0393\n",
      "Epoch [6/10], Step [630/938], Loss: 0.0076\n",
      "Epoch [6/10], Step [632/938], Loss: 0.0120\n",
      "Epoch [6/10], Step [634/938], Loss: 0.0038\n",
      "Epoch [6/10], Step [636/938], Loss: 0.0427\n",
      "Epoch [6/10], Step [638/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [640/938], Loss: 0.0030\n",
      "Epoch [6/10], Step [642/938], Loss: 0.0022\n",
      "Epoch [6/10], Step [644/938], Loss: 0.0147\n",
      "Epoch [6/10], Step [646/938], Loss: 0.0039\n",
      "Epoch [6/10], Step [648/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [650/938], Loss: 0.0064\n",
      "Epoch [6/10], Step [652/938], Loss: 0.0038\n",
      "Epoch [6/10], Step [654/938], Loss: 0.0451\n",
      "Epoch [6/10], Step [656/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [658/938], Loss: 0.0321\n",
      "Epoch [6/10], Step [660/938], Loss: 0.0187\n",
      "Epoch [6/10], Step [662/938], Loss: 0.0171\n",
      "Epoch [6/10], Step [664/938], Loss: 0.0286\n",
      "Epoch [6/10], Step [666/938], Loss: 0.0068\n",
      "Epoch [6/10], Step [668/938], Loss: 0.0132\n",
      "Epoch [6/10], Step [670/938], Loss: 0.0026\n",
      "Epoch [6/10], Step [672/938], Loss: 0.0037\n",
      "Epoch [6/10], Step [674/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [676/938], Loss: 0.0577\n",
      "Epoch [6/10], Step [678/938], Loss: 0.0027\n",
      "Epoch [6/10], Step [680/938], Loss: 0.0092\n",
      "Epoch [6/10], Step [682/938], Loss: 0.0217\n",
      "Epoch [6/10], Step [684/938], Loss: 0.0693\n",
      "Epoch [6/10], Step [686/938], Loss: 0.0197\n",
      "Epoch [6/10], Step [688/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [690/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [692/938], Loss: 0.0103\n",
      "Epoch [6/10], Step [694/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [696/938], Loss: 0.0033\n",
      "Epoch [6/10], Step [698/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0168\n",
      "Epoch [6/10], Step [702/938], Loss: 0.0043\n",
      "Epoch [6/10], Step [704/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [706/938], Loss: 0.0611\n",
      "Epoch [6/10], Step [708/938], Loss: 0.0025\n",
      "Epoch [6/10], Step [710/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [712/938], Loss: 0.0203\n",
      "Epoch [6/10], Step [714/938], Loss: 0.0234\n",
      "Epoch [6/10], Step [716/938], Loss: 0.0102\n",
      "Epoch [6/10], Step [718/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [720/938], Loss: 0.0059\n",
      "Epoch [6/10], Step [722/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [724/938], Loss: 0.0494\n",
      "Epoch [6/10], Step [726/938], Loss: 0.0254\n",
      "Epoch [6/10], Step [728/938], Loss: 0.0020\n",
      "Epoch [6/10], Step [730/938], Loss: 0.0021\n",
      "Epoch [6/10], Step [732/938], Loss: 0.0054\n",
      "Epoch [6/10], Step [734/938], Loss: 0.0055\n",
      "Epoch [6/10], Step [736/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [738/938], Loss: 0.0011\n",
      "Epoch [6/10], Step [740/938], Loss: 0.0231\n",
      "Epoch [6/10], Step [742/938], Loss: 0.0028\n",
      "Epoch [6/10], Step [744/938], Loss: 0.0320\n",
      "Epoch [6/10], Step [746/938], Loss: 0.0801\n",
      "Epoch [6/10], Step [748/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [750/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [752/938], Loss: 0.0150\n",
      "Epoch [6/10], Step [754/938], Loss: 0.0019\n",
      "Epoch [6/10], Step [756/938], Loss: 0.0184\n",
      "Epoch [6/10], Step [758/938], Loss: 0.0804\n",
      "Epoch [6/10], Step [760/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [762/938], Loss: 0.0932\n",
      "Epoch [6/10], Step [764/938], Loss: 0.0164\n",
      "Epoch [6/10], Step [766/938], Loss: 0.0053\n",
      "Epoch [6/10], Step [768/938], Loss: 0.0482\n",
      "Epoch [6/10], Step [770/938], Loss: 0.0614\n",
      "Epoch [6/10], Step [772/938], Loss: 0.0077\n",
      "Epoch [6/10], Step [774/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [776/938], Loss: 0.0118\n",
      "Epoch [6/10], Step [778/938], Loss: 0.0055\n",
      "Epoch [6/10], Step [780/938], Loss: 0.1832\n",
      "Epoch [6/10], Step [782/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [784/938], Loss: 0.0042\n",
      "Epoch [6/10], Step [786/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [788/938], Loss: 0.0044\n",
      "Epoch [6/10], Step [790/938], Loss: 0.0084\n",
      "Epoch [6/10], Step [792/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [794/938], Loss: 0.0124\n",
      "Epoch [6/10], Step [796/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [798/938], Loss: 0.0005\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0198\n",
      "Epoch [6/10], Step [802/938], Loss: 0.0018\n",
      "Epoch [6/10], Step [804/938], Loss: 0.0056\n",
      "Epoch [6/10], Step [806/938], Loss: 0.0038\n",
      "Epoch [6/10], Step [808/938], Loss: 0.0036\n",
      "Epoch [6/10], Step [810/938], Loss: 0.0780\n",
      "Epoch [6/10], Step [812/938], Loss: 0.0055\n",
      "Epoch [6/10], Step [814/938], Loss: 0.0001\n",
      "Epoch [6/10], Step [816/938], Loss: 0.0068\n",
      "Epoch [6/10], Step [818/938], Loss: 0.0141\n",
      "Epoch [6/10], Step [820/938], Loss: 0.0030\n",
      "Epoch [6/10], Step [822/938], Loss: 0.0422\n",
      "Epoch [6/10], Step [824/938], Loss: 0.0149\n",
      "Epoch [6/10], Step [826/938], Loss: 0.0031\n",
      "Epoch [6/10], Step [828/938], Loss: 0.0164\n",
      "Epoch [6/10], Step [830/938], Loss: 0.0280\n",
      "Epoch [6/10], Step [832/938], Loss: 0.0057\n",
      "Epoch [6/10], Step [834/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [836/938], Loss: 0.0217\n",
      "Epoch [6/10], Step [838/938], Loss: 0.0000\n",
      "Epoch [6/10], Step [840/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [842/938], Loss: 0.0096\n",
      "Epoch [6/10], Step [844/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [846/938], Loss: 0.0615\n",
      "Epoch [6/10], Step [848/938], Loss: 0.0031\n",
      "Epoch [6/10], Step [850/938], Loss: 0.0006\n",
      "Epoch [6/10], Step [852/938], Loss: 0.0020\n",
      "Epoch [6/10], Step [854/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [856/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [858/938], Loss: 0.0010\n",
      "Epoch [6/10], Step [860/938], Loss: 0.0202\n",
      "Epoch [6/10], Step [862/938], Loss: 0.0383\n",
      "Epoch [6/10], Step [864/938], Loss: 0.0044\n",
      "Epoch [6/10], Step [866/938], Loss: 0.0262\n",
      "Epoch [6/10], Step [868/938], Loss: 0.0040\n",
      "Epoch [6/10], Step [870/938], Loss: 0.0573\n",
      "Epoch [6/10], Step [872/938], Loss: 0.0029\n",
      "Epoch [6/10], Step [874/938], Loss: 0.0015\n",
      "Epoch [6/10], Step [876/938], Loss: 0.0003\n",
      "Epoch [6/10], Step [878/938], Loss: 0.0069\n",
      "Epoch [6/10], Step [880/938], Loss: 0.0004\n",
      "Epoch [6/10], Step [882/938], Loss: 0.0027\n",
      "Epoch [6/10], Step [884/938], Loss: 0.0298\n",
      "Epoch [6/10], Step [886/938], Loss: 0.0115\n",
      "Epoch [6/10], Step [888/938], Loss: 0.0008\n",
      "Epoch [6/10], Step [890/938], Loss: 0.0156\n",
      "Epoch [6/10], Step [892/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [894/938], Loss: 0.0016\n",
      "Epoch [6/10], Step [896/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [898/938], Loss: 0.0044\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0070\n",
      "Epoch [6/10], Step [902/938], Loss: 0.0100\n",
      "Epoch [6/10], Step [904/938], Loss: 0.0007\n",
      "Epoch [6/10], Step [906/938], Loss: 0.0162\n",
      "Epoch [6/10], Step [908/938], Loss: 0.0009\n",
      "Epoch [6/10], Step [910/938], Loss: 0.0243\n",
      "Epoch [6/10], Step [912/938], Loss: 0.0002\n",
      "Epoch [6/10], Step [914/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [916/938], Loss: 0.0013\n",
      "Epoch [6/10], Step [918/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [920/938], Loss: 0.0045\n",
      "Epoch [6/10], Step [922/938], Loss: 0.0328\n",
      "Epoch [6/10], Step [924/938], Loss: 0.0046\n",
      "Epoch [6/10], Step [926/938], Loss: 0.0254\n",
      "Epoch [6/10], Step [928/938], Loss: 0.0103\n",
      "Epoch [6/10], Step [930/938], Loss: 0.0012\n",
      "Epoch [6/10], Step [932/938], Loss: 0.0014\n",
      "Epoch [6/10], Step [934/938], Loss: 0.0027\n",
      "Epoch [6/10], Step [936/938], Loss: 0.0606\n",
      "Epoch [6/10], Step [938/938], Loss: 0.1111\n",
      "Epoch [6/10], Loss: 0.0138\n",
      "Epoch [7/10], Step [2/938], Loss: 0.0671\n",
      "Epoch [7/10], Step [4/938], Loss: 0.0472\n",
      "Epoch [7/10], Step [6/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [8/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [10/938], Loss: 0.0115\n",
      "Epoch [7/10], Step [12/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [14/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [16/938], Loss: 0.0028\n",
      "Epoch [7/10], Step [18/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [20/938], Loss: 0.0059\n",
      "Epoch [7/10], Step [22/938], Loss: 0.0086\n",
      "Epoch [7/10], Step [24/938], Loss: 0.0128\n",
      "Epoch [7/10], Step [26/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [28/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [30/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [32/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [34/938], Loss: 0.0066\n",
      "Epoch [7/10], Step [36/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [38/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [40/938], Loss: 0.0754\n",
      "Epoch [7/10], Step [42/938], Loss: 0.0037\n",
      "Epoch [7/10], Step [44/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [46/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [48/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [50/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [52/938], Loss: 0.0049\n",
      "Epoch [7/10], Step [54/938], Loss: 0.0102\n",
      "Epoch [7/10], Step [56/938], Loss: 0.0389\n",
      "Epoch [7/10], Step [58/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [60/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [62/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [64/938], Loss: 0.0356\n",
      "Epoch [7/10], Step [66/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [68/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [70/938], Loss: 0.0081\n",
      "Epoch [7/10], Step [72/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [74/938], Loss: 0.0105\n",
      "Epoch [7/10], Step [76/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [78/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [80/938], Loss: 0.0207\n",
      "Epoch [7/10], Step [82/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [84/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [86/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [88/938], Loss: 0.0083\n",
      "Epoch [7/10], Step [90/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [92/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [94/938], Loss: 0.0079\n",
      "Epoch [7/10], Step [96/938], Loss: 0.0513\n",
      "Epoch [7/10], Step [98/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [102/938], Loss: 0.0026\n",
      "Epoch [7/10], Step [104/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [106/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [108/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [110/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [112/938], Loss: 0.0339\n",
      "Epoch [7/10], Step [114/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [116/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [118/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [120/938], Loss: 0.0097\n",
      "Epoch [7/10], Step [122/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [124/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [126/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [128/938], Loss: 0.0098\n",
      "Epoch [7/10], Step [130/938], Loss: 0.0070\n",
      "Epoch [7/10], Step [132/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [134/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [136/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [138/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [140/938], Loss: 0.0062\n",
      "Epoch [7/10], Step [142/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [144/938], Loss: 0.0046\n",
      "Epoch [7/10], Step [146/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [148/938], Loss: 0.0511\n",
      "Epoch [7/10], Step [150/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [152/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [154/938], Loss: 0.0109\n",
      "Epoch [7/10], Step [156/938], Loss: 0.0556\n",
      "Epoch [7/10], Step [158/938], Loss: 0.0948\n",
      "Epoch [7/10], Step [160/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [162/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [164/938], Loss: 0.0052\n",
      "Epoch [7/10], Step [166/938], Loss: 0.0098\n",
      "Epoch [7/10], Step [168/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [170/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [172/938], Loss: 0.0072\n",
      "Epoch [7/10], Step [174/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [176/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [178/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [180/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [182/938], Loss: 0.0045\n",
      "Epoch [7/10], Step [184/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [186/938], Loss: 0.0220\n",
      "Epoch [7/10], Step [188/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [190/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [192/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [194/938], Loss: 0.0372\n",
      "Epoch [7/10], Step [196/938], Loss: 0.0409\n",
      "Epoch [7/10], Step [198/938], Loss: 0.0152\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0119\n",
      "Epoch [7/10], Step [202/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [204/938], Loss: 0.0308\n",
      "Epoch [7/10], Step [206/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [208/938], Loss: 0.0079\n",
      "Epoch [7/10], Step [210/938], Loss: 0.0116\n",
      "Epoch [7/10], Step [212/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [214/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [216/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [218/938], Loss: 0.0061\n",
      "Epoch [7/10], Step [220/938], Loss: 0.0080\n",
      "Epoch [7/10], Step [222/938], Loss: 0.0325\n",
      "Epoch [7/10], Step [224/938], Loss: 0.0028\n",
      "Epoch [7/10], Step [226/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [228/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [230/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [232/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [234/938], Loss: 0.0057\n",
      "Epoch [7/10], Step [236/938], Loss: 0.0263\n",
      "Epoch [7/10], Step [238/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [240/938], Loss: 0.0154\n",
      "Epoch [7/10], Step [242/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [244/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [246/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [248/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [250/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [252/938], Loss: 0.0650\n",
      "Epoch [7/10], Step [254/938], Loss: 0.0134\n",
      "Epoch [7/10], Step [256/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [258/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [260/938], Loss: 0.0144\n",
      "Epoch [7/10], Step [262/938], Loss: 0.0090\n",
      "Epoch [7/10], Step [264/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [266/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [268/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [270/938], Loss: 0.0300\n",
      "Epoch [7/10], Step [272/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [274/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [276/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [278/938], Loss: 0.0367\n",
      "Epoch [7/10], Step [280/938], Loss: 0.0182\n",
      "Epoch [7/10], Step [282/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [284/938], Loss: 0.0288\n",
      "Epoch [7/10], Step [286/938], Loss: 0.0026\n",
      "Epoch [7/10], Step [288/938], Loss: 0.0079\n",
      "Epoch [7/10], Step [290/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [292/938], Loss: 0.0674\n",
      "Epoch [7/10], Step [294/938], Loss: 0.0081\n",
      "Epoch [7/10], Step [296/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [298/938], Loss: 0.0054\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [302/938], Loss: 0.0485\n",
      "Epoch [7/10], Step [304/938], Loss: 0.0084\n",
      "Epoch [7/10], Step [306/938], Loss: 0.0109\n",
      "Epoch [7/10], Step [308/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [310/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [312/938], Loss: 0.0345\n",
      "Epoch [7/10], Step [314/938], Loss: 0.0576\n",
      "Epoch [7/10], Step [316/938], Loss: 0.0153\n",
      "Epoch [7/10], Step [318/938], Loss: 0.0693\n",
      "Epoch [7/10], Step [320/938], Loss: 0.0040\n",
      "Epoch [7/10], Step [322/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [324/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [326/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [328/938], Loss: 0.0045\n",
      "Epoch [7/10], Step [330/938], Loss: 0.0104\n",
      "Epoch [7/10], Step [332/938], Loss: 0.0065\n",
      "Epoch [7/10], Step [334/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [336/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [338/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [340/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [342/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [344/938], Loss: 0.0651\n",
      "Epoch [7/10], Step [346/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [348/938], Loss: 0.0172\n",
      "Epoch [7/10], Step [350/938], Loss: 0.0153\n",
      "Epoch [7/10], Step [352/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [354/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [356/938], Loss: 0.0103\n",
      "Epoch [7/10], Step [358/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [360/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [362/938], Loss: 0.0066\n",
      "Epoch [7/10], Step [364/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [366/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [368/938], Loss: 0.0051\n",
      "Epoch [7/10], Step [370/938], Loss: 0.0066\n",
      "Epoch [7/10], Step [372/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [374/938], Loss: 0.0133\n",
      "Epoch [7/10], Step [376/938], Loss: 0.0428\n",
      "Epoch [7/10], Step [378/938], Loss: 0.0068\n",
      "Epoch [7/10], Step [380/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [382/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [384/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [386/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [388/938], Loss: 0.0153\n",
      "Epoch [7/10], Step [390/938], Loss: 0.0107\n",
      "Epoch [7/10], Step [392/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [394/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [396/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [398/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [402/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [404/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [406/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [408/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [410/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [412/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [414/938], Loss: 0.1015\n",
      "Epoch [7/10], Step [416/938], Loss: 0.0050\n",
      "Epoch [7/10], Step [418/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [420/938], Loss: 0.0134\n",
      "Epoch [7/10], Step [422/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [424/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [426/938], Loss: 0.0122\n",
      "Epoch [7/10], Step [428/938], Loss: 0.0140\n",
      "Epoch [7/10], Step [430/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [432/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [434/938], Loss: 0.0044\n",
      "Epoch [7/10], Step [436/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [438/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [440/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [442/938], Loss: 0.0267\n",
      "Epoch [7/10], Step [444/938], Loss: 0.0117\n",
      "Epoch [7/10], Step [446/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [448/938], Loss: 0.0128\n",
      "Epoch [7/10], Step [450/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [452/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [454/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [456/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [458/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [460/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [462/938], Loss: 0.0048\n",
      "Epoch [7/10], Step [464/938], Loss: 0.0146\n",
      "Epoch [7/10], Step [466/938], Loss: 0.0577\n",
      "Epoch [7/10], Step [468/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [470/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [472/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [474/938], Loss: 0.0146\n",
      "Epoch [7/10], Step [476/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [478/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [480/938], Loss: 0.0099\n",
      "Epoch [7/10], Step [482/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [484/938], Loss: 0.0190\n",
      "Epoch [7/10], Step [486/938], Loss: 0.0045\n",
      "Epoch [7/10], Step [488/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [490/938], Loss: 0.0101\n",
      "Epoch [7/10], Step [492/938], Loss: 0.0102\n",
      "Epoch [7/10], Step [494/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [496/938], Loss: 0.1076\n",
      "Epoch [7/10], Step [498/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [502/938], Loss: 0.0053\n",
      "Epoch [7/10], Step [504/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [506/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [508/938], Loss: 0.0058\n",
      "Epoch [7/10], Step [510/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [512/938], Loss: 0.0038\n",
      "Epoch [7/10], Step [514/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [516/938], Loss: 0.0028\n",
      "Epoch [7/10], Step [518/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [520/938], Loss: 0.0240\n",
      "Epoch [7/10], Step [522/938], Loss: 0.0244\n",
      "Epoch [7/10], Step [524/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [526/938], Loss: 0.0144\n",
      "Epoch [7/10], Step [528/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [530/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [532/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [534/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [536/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [538/938], Loss: 0.0197\n",
      "Epoch [7/10], Step [540/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [542/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [544/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [546/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [548/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [550/938], Loss: 0.0862\n",
      "Epoch [7/10], Step [552/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [554/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [556/938], Loss: 0.0047\n",
      "Epoch [7/10], Step [558/938], Loss: 0.0250\n",
      "Epoch [7/10], Step [560/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [562/938], Loss: 0.0214\n",
      "Epoch [7/10], Step [564/938], Loss: 0.0014\n",
      "Epoch [7/10], Step [566/938], Loss: 0.0018\n",
      "Epoch [7/10], Step [568/938], Loss: 0.0192\n",
      "Epoch [7/10], Step [570/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [572/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [574/938], Loss: 0.0029\n",
      "Epoch [7/10], Step [576/938], Loss: 0.0073\n",
      "Epoch [7/10], Step [578/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [580/938], Loss: 0.0146\n",
      "Epoch [7/10], Step [582/938], Loss: 0.0043\n",
      "Epoch [7/10], Step [584/938], Loss: 0.0380\n",
      "Epoch [7/10], Step [586/938], Loss: 0.0347\n",
      "Epoch [7/10], Step [588/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [590/938], Loss: 0.0067\n",
      "Epoch [7/10], Step [592/938], Loss: 0.0052\n",
      "Epoch [7/10], Step [594/938], Loss: 0.0645\n",
      "Epoch [7/10], Step [596/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [598/938], Loss: 0.0032\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [602/938], Loss: 0.0302\n",
      "Epoch [7/10], Step [604/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [606/938], Loss: 0.0029\n",
      "Epoch [7/10], Step [608/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [610/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [612/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [614/938], Loss: 0.0110\n",
      "Epoch [7/10], Step [616/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [618/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [620/938], Loss: 0.0056\n",
      "Epoch [7/10], Step [622/938], Loss: 0.0432\n",
      "Epoch [7/10], Step [624/938], Loss: 0.0209\n",
      "Epoch [7/10], Step [626/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [628/938], Loss: 0.0023\n",
      "Epoch [7/10], Step [630/938], Loss: 0.0052\n",
      "Epoch [7/10], Step [632/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [634/938], Loss: 0.0631\n",
      "Epoch [7/10], Step [636/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [638/938], Loss: 0.0170\n",
      "Epoch [7/10], Step [640/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [642/938], Loss: 0.0336\n",
      "Epoch [7/10], Step [644/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [646/938], Loss: 0.0393\n",
      "Epoch [7/10], Step [648/938], Loss: 0.0096\n",
      "Epoch [7/10], Step [650/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [652/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [654/938], Loss: 0.1107\n",
      "Epoch [7/10], Step [656/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [658/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [660/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [662/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [664/938], Loss: 0.0140\n",
      "Epoch [7/10], Step [666/938], Loss: 0.0043\n",
      "Epoch [7/10], Step [668/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [670/938], Loss: 0.0047\n",
      "Epoch [7/10], Step [672/938], Loss: 0.0193\n",
      "Epoch [7/10], Step [674/938], Loss: 0.0441\n",
      "Epoch [7/10], Step [676/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [678/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [680/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [682/938], Loss: 0.0160\n",
      "Epoch [7/10], Step [684/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [686/938], Loss: 0.0331\n",
      "Epoch [7/10], Step [688/938], Loss: 0.0006\n",
      "Epoch [7/10], Step [690/938], Loss: 0.0025\n",
      "Epoch [7/10], Step [692/938], Loss: 0.0672\n",
      "Epoch [7/10], Step [694/938], Loss: 0.0067\n",
      "Epoch [7/10], Step [696/938], Loss: 0.0021\n",
      "Epoch [7/10], Step [698/938], Loss: 0.0071\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0178\n",
      "Epoch [7/10], Step [702/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [704/938], Loss: 0.0013\n",
      "Epoch [7/10], Step [706/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [708/938], Loss: 0.0211\n",
      "Epoch [7/10], Step [710/938], Loss: 0.0045\n",
      "Epoch [7/10], Step [712/938], Loss: 0.0008\n",
      "Epoch [7/10], Step [714/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [716/938], Loss: 0.0278\n",
      "Epoch [7/10], Step [718/938], Loss: 0.0049\n",
      "Epoch [7/10], Step [720/938], Loss: 0.0131\n",
      "Epoch [7/10], Step [722/938], Loss: 0.0004\n",
      "Epoch [7/10], Step [724/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [726/938], Loss: 0.0017\n",
      "Epoch [7/10], Step [728/938], Loss: 0.0207\n",
      "Epoch [7/10], Step [730/938], Loss: 0.0080\n",
      "Epoch [7/10], Step [732/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [734/938], Loss: 0.0066\n",
      "Epoch [7/10], Step [736/938], Loss: 0.0029\n",
      "Epoch [7/10], Step [738/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [740/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [742/938], Loss: 0.0086\n",
      "Epoch [7/10], Step [744/938], Loss: 0.0118\n",
      "Epoch [7/10], Step [746/938], Loss: 0.0602\n",
      "Epoch [7/10], Step [748/938], Loss: 0.0412\n",
      "Epoch [7/10], Step [750/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [752/938], Loss: 0.0611\n",
      "Epoch [7/10], Step [754/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [756/938], Loss: 0.0179\n",
      "Epoch [7/10], Step [758/938], Loss: 0.0041\n",
      "Epoch [7/10], Step [760/938], Loss: 0.1653\n",
      "Epoch [7/10], Step [762/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [764/938], Loss: 0.0458\n",
      "Epoch [7/10], Step [766/938], Loss: 0.0475\n",
      "Epoch [7/10], Step [768/938], Loss: 0.0034\n",
      "Epoch [7/10], Step [770/938], Loss: 0.1018\n",
      "Epoch [7/10], Step [772/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [774/938], Loss: 0.0466\n",
      "Epoch [7/10], Step [776/938], Loss: 0.0011\n",
      "Epoch [7/10], Step [778/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [780/938], Loss: 0.0022\n",
      "Epoch [7/10], Step [782/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [784/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [786/938], Loss: 0.0183\n",
      "Epoch [7/10], Step [788/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [790/938], Loss: 0.0195\n",
      "Epoch [7/10], Step [792/938], Loss: 0.0049\n",
      "Epoch [7/10], Step [794/938], Loss: 0.0000\n",
      "Epoch [7/10], Step [796/938], Loss: 0.0055\n",
      "Epoch [7/10], Step [798/938], Loss: 0.0035\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [802/938], Loss: 0.0047\n",
      "Epoch [7/10], Step [804/938], Loss: 0.0239\n",
      "Epoch [7/10], Step [806/938], Loss: 0.0250\n",
      "Epoch [7/10], Step [808/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [810/938], Loss: 0.0104\n",
      "Epoch [7/10], Step [812/938], Loss: 0.0820\n",
      "Epoch [7/10], Step [814/938], Loss: 0.0009\n",
      "Epoch [7/10], Step [816/938], Loss: 0.0402\n",
      "Epoch [7/10], Step [818/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [820/938], Loss: 0.0039\n",
      "Epoch [7/10], Step [822/938], Loss: 0.0250\n",
      "Epoch [7/10], Step [824/938], Loss: 0.0080\n",
      "Epoch [7/10], Step [826/938], Loss: 0.0084\n",
      "Epoch [7/10], Step [828/938], Loss: 0.0159\n",
      "Epoch [7/10], Step [830/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [832/938], Loss: 0.0271\n",
      "Epoch [7/10], Step [834/938], Loss: 0.0203\n",
      "Epoch [7/10], Step [836/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [838/938], Loss: 0.0041\n",
      "Epoch [7/10], Step [840/938], Loss: 0.0042\n",
      "Epoch [7/10], Step [842/938], Loss: 0.0028\n",
      "Epoch [7/10], Step [844/938], Loss: 0.0743\n",
      "Epoch [7/10], Step [846/938], Loss: 0.0627\n",
      "Epoch [7/10], Step [848/938], Loss: 0.0140\n",
      "Epoch [7/10], Step [850/938], Loss: 0.0212\n",
      "Epoch [7/10], Step [852/938], Loss: 0.0068\n",
      "Epoch [7/10], Step [854/938], Loss: 0.0010\n",
      "Epoch [7/10], Step [856/938], Loss: 0.0819\n",
      "Epoch [7/10], Step [858/938], Loss: 0.0478\n",
      "Epoch [7/10], Step [860/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [862/938], Loss: 0.0012\n",
      "Epoch [7/10], Step [864/938], Loss: 0.0036\n",
      "Epoch [7/10], Step [866/938], Loss: 0.0091\n",
      "Epoch [7/10], Step [868/938], Loss: 0.0019\n",
      "Epoch [7/10], Step [870/938], Loss: 0.0071\n",
      "Epoch [7/10], Step [872/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [874/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [876/938], Loss: 0.0033\n",
      "Epoch [7/10], Step [878/938], Loss: 0.0072\n",
      "Epoch [7/10], Step [880/938], Loss: 0.0353\n",
      "Epoch [7/10], Step [882/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [884/938], Loss: 0.0292\n",
      "Epoch [7/10], Step [886/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [888/938], Loss: 0.0002\n",
      "Epoch [7/10], Step [890/938], Loss: 0.0182\n",
      "Epoch [7/10], Step [892/938], Loss: 0.0033\n",
      "Epoch [7/10], Step [894/938], Loss: 0.0007\n",
      "Epoch [7/10], Step [896/938], Loss: 0.0046\n",
      "Epoch [7/10], Step [898/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0573\n",
      "Epoch [7/10], Step [902/938], Loss: 0.0015\n",
      "Epoch [7/10], Step [904/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [906/938], Loss: 0.0016\n",
      "Epoch [7/10], Step [908/938], Loss: 0.0236\n",
      "Epoch [7/10], Step [910/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [912/938], Loss: 0.0074\n",
      "Epoch [7/10], Step [914/938], Loss: 0.0136\n",
      "Epoch [7/10], Step [916/938], Loss: 0.0301\n",
      "Epoch [7/10], Step [918/938], Loss: 0.0003\n",
      "Epoch [7/10], Step [920/938], Loss: 0.0102\n",
      "Epoch [7/10], Step [922/938], Loss: 0.0243\n",
      "Epoch [7/10], Step [924/938], Loss: 0.0495\n",
      "Epoch [7/10], Step [926/938], Loss: 0.0001\n",
      "Epoch [7/10], Step [928/938], Loss: 0.0005\n",
      "Epoch [7/10], Step [930/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [932/938], Loss: 0.0111\n",
      "Epoch [7/10], Step [934/938], Loss: 0.0036\n",
      "Epoch [7/10], Step [936/938], Loss: 0.0020\n",
      "Epoch [7/10], Step [938/938], Loss: 0.0005\n",
      "Epoch [7/10], Loss: 0.0108\n",
      "Epoch [8/10], Step [2/938], Loss: 0.0427\n",
      "Epoch [8/10], Step [4/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [6/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [8/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [10/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [12/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [14/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [16/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [18/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [20/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [22/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [24/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [26/938], Loss: 0.0102\n",
      "Epoch [8/10], Step [28/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [30/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [32/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [34/938], Loss: 0.0055\n",
      "Epoch [8/10], Step [36/938], Loss: 0.0025\n",
      "Epoch [8/10], Step [38/938], Loss: 0.0180\n",
      "Epoch [8/10], Step [40/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [42/938], Loss: 0.0069\n",
      "Epoch [8/10], Step [44/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [46/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [48/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [50/938], Loss: 0.0054\n",
      "Epoch [8/10], Step [52/938], Loss: 0.0055\n",
      "Epoch [8/10], Step [54/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [56/938], Loss: 0.0120\n",
      "Epoch [8/10], Step [58/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [60/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [62/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [64/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [66/938], Loss: 0.0026\n",
      "Epoch [8/10], Step [68/938], Loss: 0.0137\n",
      "Epoch [8/10], Step [70/938], Loss: 0.0026\n",
      "Epoch [8/10], Step [72/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [74/938], Loss: 0.0367\n",
      "Epoch [8/10], Step [76/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [78/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [80/938], Loss: 0.0523\n",
      "Epoch [8/10], Step [82/938], Loss: 0.0062\n",
      "Epoch [8/10], Step [84/938], Loss: 0.0351\n",
      "Epoch [8/10], Step [86/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [88/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [90/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [92/938], Loss: 0.0016\n",
      "Epoch [8/10], Step [94/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [96/938], Loss: 0.0026\n",
      "Epoch [8/10], Step [98/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [102/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [104/938], Loss: 0.0083\n",
      "Epoch [8/10], Step [106/938], Loss: 0.0272\n",
      "Epoch [8/10], Step [108/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [110/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [112/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [114/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [116/938], Loss: 0.0667\n",
      "Epoch [8/10], Step [118/938], Loss: 0.0088\n",
      "Epoch [8/10], Step [120/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [122/938], Loss: 0.0217\n",
      "Epoch [8/10], Step [124/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [126/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [128/938], Loss: 0.0439\n",
      "Epoch [8/10], Step [130/938], Loss: 0.0163\n",
      "Epoch [8/10], Step [132/938], Loss: 0.0117\n",
      "Epoch [8/10], Step [134/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [136/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [138/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [140/938], Loss: 0.0168\n",
      "Epoch [8/10], Step [142/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [144/938], Loss: 0.0502\n",
      "Epoch [8/10], Step [146/938], Loss: 0.0200\n",
      "Epoch [8/10], Step [148/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [150/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [152/938], Loss: 0.0108\n",
      "Epoch [8/10], Step [154/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [156/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [158/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [160/938], Loss: 0.0056\n",
      "Epoch [8/10], Step [162/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [164/938], Loss: 0.0189\n",
      "Epoch [8/10], Step [166/938], Loss: 0.0808\n",
      "Epoch [8/10], Step [168/938], Loss: 0.0046\n",
      "Epoch [8/10], Step [170/938], Loss: 0.0016\n",
      "Epoch [8/10], Step [172/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [174/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [176/938], Loss: 0.0065\n",
      "Epoch [8/10], Step [178/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [180/938], Loss: 0.0051\n",
      "Epoch [8/10], Step [182/938], Loss: 0.0487\n",
      "Epoch [8/10], Step [184/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [186/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [188/938], Loss: 0.0071\n",
      "Epoch [8/10], Step [190/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [192/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [194/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [196/938], Loss: 0.0253\n",
      "Epoch [8/10], Step [198/938], Loss: 0.0150\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [202/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [204/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [206/938], Loss: 0.0028\n",
      "Epoch [8/10], Step [208/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [210/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [212/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [214/938], Loss: 0.0058\n",
      "Epoch [8/10], Step [216/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [218/938], Loss: 0.0059\n",
      "Epoch [8/10], Step [220/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [222/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [224/938], Loss: 0.0142\n",
      "Epoch [8/10], Step [226/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [228/938], Loss: 0.0056\n",
      "Epoch [8/10], Step [230/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [232/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [234/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [236/938], Loss: 0.0160\n",
      "Epoch [8/10], Step [238/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [240/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [242/938], Loss: 0.0231\n",
      "Epoch [8/10], Step [244/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [246/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [248/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [250/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [252/938], Loss: 0.0063\n",
      "Epoch [8/10], Step [254/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [256/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [258/938], Loss: 0.0395\n",
      "Epoch [8/10], Step [260/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [262/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [264/938], Loss: 0.0363\n",
      "Epoch [8/10], Step [266/938], Loss: 0.0084\n",
      "Epoch [8/10], Step [268/938], Loss: 0.0084\n",
      "Epoch [8/10], Step [270/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [272/938], Loss: 0.0016\n",
      "Epoch [8/10], Step [274/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [276/938], Loss: 0.0064\n",
      "Epoch [8/10], Step [278/938], Loss: 0.0530\n",
      "Epoch [8/10], Step [280/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [282/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [284/938], Loss: 0.1599\n",
      "Epoch [8/10], Step [286/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [288/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [290/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [292/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [294/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [296/938], Loss: 0.0223\n",
      "Epoch [8/10], Step [298/938], Loss: 0.0141\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [302/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [304/938], Loss: 0.0437\n",
      "Epoch [8/10], Step [306/938], Loss: 0.0045\n",
      "Epoch [8/10], Step [308/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [310/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [312/938], Loss: 0.0074\n",
      "Epoch [8/10], Step [314/938], Loss: 0.0032\n",
      "Epoch [8/10], Step [316/938], Loss: 0.0279\n",
      "Epoch [8/10], Step [318/938], Loss: 0.0102\n",
      "Epoch [8/10], Step [320/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [322/938], Loss: 0.0068\n",
      "Epoch [8/10], Step [324/938], Loss: 0.0347\n",
      "Epoch [8/10], Step [326/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [328/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [330/938], Loss: 0.0109\n",
      "Epoch [8/10], Step [332/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [334/938], Loss: 0.0269\n",
      "Epoch [8/10], Step [336/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [338/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [340/938], Loss: 0.0248\n",
      "Epoch [8/10], Step [342/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [344/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [346/938], Loss: 0.0100\n",
      "Epoch [8/10], Step [348/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [350/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [352/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [354/938], Loss: 0.0475\n",
      "Epoch [8/10], Step [356/938], Loss: 0.0106\n",
      "Epoch [8/10], Step [358/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [360/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [362/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [364/938], Loss: 0.0033\n",
      "Epoch [8/10], Step [366/938], Loss: 0.0026\n",
      "Epoch [8/10], Step [368/938], Loss: 0.0092\n",
      "Epoch [8/10], Step [370/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [372/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [374/938], Loss: 0.0034\n",
      "Epoch [8/10], Step [376/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [378/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [380/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [382/938], Loss: 0.0436\n",
      "Epoch [8/10], Step [384/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [386/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [388/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [390/938], Loss: 0.0694\n",
      "Epoch [8/10], Step [392/938], Loss: 0.0037\n",
      "Epoch [8/10], Step [394/938], Loss: 0.0028\n",
      "Epoch [8/10], Step [396/938], Loss: 0.0226\n",
      "Epoch [8/10], Step [398/938], Loss: 0.0086\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [402/938], Loss: 0.0173\n",
      "Epoch [8/10], Step [404/938], Loss: 0.0040\n",
      "Epoch [8/10], Step [406/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [408/938], Loss: 0.0050\n",
      "Epoch [8/10], Step [410/938], Loss: 0.0364\n",
      "Epoch [8/10], Step [412/938], Loss: 0.0534\n",
      "Epoch [8/10], Step [414/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [416/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [418/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [420/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [422/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [424/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [426/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [428/938], Loss: 0.0028\n",
      "Epoch [8/10], Step [430/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [432/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [434/938], Loss: 0.0157\n",
      "Epoch [8/10], Step [436/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [438/938], Loss: 0.0565\n",
      "Epoch [8/10], Step [440/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [442/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [444/938], Loss: 0.0032\n",
      "Epoch [8/10], Step [446/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [448/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [450/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [452/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [454/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [456/938], Loss: 0.0043\n",
      "Epoch [8/10], Step [458/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [460/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [462/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [464/938], Loss: 0.0250\n",
      "Epoch [8/10], Step [466/938], Loss: 0.0074\n",
      "Epoch [8/10], Step [468/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [470/938], Loss: 0.0582\n",
      "Epoch [8/10], Step [472/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [474/938], Loss: 0.0066\n",
      "Epoch [8/10], Step [476/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [478/938], Loss: 0.0237\n",
      "Epoch [8/10], Step [480/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [482/938], Loss: 0.0047\n",
      "Epoch [8/10], Step [484/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [486/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [488/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [490/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [492/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [494/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [496/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [498/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [502/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [504/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [506/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [508/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [510/938], Loss: 0.0312\n",
      "Epoch [8/10], Step [512/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [514/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [516/938], Loss: 0.0306\n",
      "Epoch [8/10], Step [518/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [520/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [522/938], Loss: 0.0243\n",
      "Epoch [8/10], Step [524/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [526/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [528/938], Loss: 0.0432\n",
      "Epoch [8/10], Step [530/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [532/938], Loss: 0.0133\n",
      "Epoch [8/10], Step [534/938], Loss: 0.0159\n",
      "Epoch [8/10], Step [536/938], Loss: 0.0241\n",
      "Epoch [8/10], Step [538/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [540/938], Loss: 0.0143\n",
      "Epoch [8/10], Step [542/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [544/938], Loss: 0.0136\n",
      "Epoch [8/10], Step [546/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [548/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [550/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [552/938], Loss: 0.0611\n",
      "Epoch [8/10], Step [554/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [556/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [558/938], Loss: 0.0041\n",
      "Epoch [8/10], Step [560/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [562/938], Loss: 0.0551\n",
      "Epoch [8/10], Step [564/938], Loss: 0.0052\n",
      "Epoch [8/10], Step [566/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [568/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [570/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [572/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [574/938], Loss: 0.0215\n",
      "Epoch [8/10], Step [576/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [578/938], Loss: 0.0255\n",
      "Epoch [8/10], Step [580/938], Loss: 0.0683\n",
      "Epoch [8/10], Step [582/938], Loss: 0.0662\n",
      "Epoch [8/10], Step [584/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [586/938], Loss: 0.0226\n",
      "Epoch [8/10], Step [588/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [590/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [592/938], Loss: 0.0097\n",
      "Epoch [8/10], Step [594/938], Loss: 0.0045\n",
      "Epoch [8/10], Step [596/938], Loss: 0.0099\n",
      "Epoch [8/10], Step [598/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [602/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [604/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [606/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [608/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [610/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [612/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [614/938], Loss: 0.0097\n",
      "Epoch [8/10], Step [616/938], Loss: 0.0024\n",
      "Epoch [8/10], Step [618/938], Loss: 0.0043\n",
      "Epoch [8/10], Step [620/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [622/938], Loss: 0.0078\n",
      "Epoch [8/10], Step [624/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [626/938], Loss: 0.0076\n",
      "Epoch [8/10], Step [628/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [630/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [632/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [634/938], Loss: 0.0150\n",
      "Epoch [8/10], Step [636/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [638/938], Loss: 0.0142\n",
      "Epoch [8/10], Step [640/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [642/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [644/938], Loss: 0.0432\n",
      "Epoch [8/10], Step [646/938], Loss: 0.0015\n",
      "Epoch [8/10], Step [648/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [650/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [652/938], Loss: 0.0380\n",
      "Epoch [8/10], Step [654/938], Loss: 0.0047\n",
      "Epoch [8/10], Step [656/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [658/938], Loss: 0.0278\n",
      "Epoch [8/10], Step [660/938], Loss: 0.0173\n",
      "Epoch [8/10], Step [662/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [664/938], Loss: 0.0080\n",
      "Epoch [8/10], Step [666/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [668/938], Loss: 0.0076\n",
      "Epoch [8/10], Step [670/938], Loss: 0.1224\n",
      "Epoch [8/10], Step [672/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [674/938], Loss: 0.0009\n",
      "Epoch [8/10], Step [676/938], Loss: 0.0068\n",
      "Epoch [8/10], Step [678/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [680/938], Loss: 0.0073\n",
      "Epoch [8/10], Step [682/938], Loss: 0.0330\n",
      "Epoch [8/10], Step [684/938], Loss: 0.0019\n",
      "Epoch [8/10], Step [686/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [688/938], Loss: 0.0061\n",
      "Epoch [8/10], Step [690/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [692/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [694/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [696/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [698/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [702/938], Loss: 0.0026\n",
      "Epoch [8/10], Step [704/938], Loss: 0.0144\n",
      "Epoch [8/10], Step [706/938], Loss: 0.0320\n",
      "Epoch [8/10], Step [708/938], Loss: 0.0039\n",
      "Epoch [8/10], Step [710/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [712/938], Loss: 0.0040\n",
      "Epoch [8/10], Step [714/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [716/938], Loss: 0.0256\n",
      "Epoch [8/10], Step [718/938], Loss: 0.0574\n",
      "Epoch [8/10], Step [720/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [722/938], Loss: 0.0018\n",
      "Epoch [8/10], Step [724/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [726/938], Loss: 0.0034\n",
      "Epoch [8/10], Step [728/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [730/938], Loss: 0.0017\n",
      "Epoch [8/10], Step [732/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [734/938], Loss: 0.0586\n",
      "Epoch [8/10], Step [736/938], Loss: 0.0094\n",
      "Epoch [8/10], Step [738/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [740/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [742/938], Loss: 0.0005\n",
      "Epoch [8/10], Step [744/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [746/938], Loss: 0.0008\n",
      "Epoch [8/10], Step [748/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [750/938], Loss: 0.0060\n",
      "Epoch [8/10], Step [752/938], Loss: 0.0085\n",
      "Epoch [8/10], Step [754/938], Loss: 0.0948\n",
      "Epoch [8/10], Step [756/938], Loss: 0.0086\n",
      "Epoch [8/10], Step [758/938], Loss: 0.0102\n",
      "Epoch [8/10], Step [760/938], Loss: 0.0035\n",
      "Epoch [8/10], Step [762/938], Loss: 0.0053\n",
      "Epoch [8/10], Step [764/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [766/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [768/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [770/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [772/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [774/938], Loss: 0.0011\n",
      "Epoch [8/10], Step [776/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [778/938], Loss: 0.0047\n",
      "Epoch [8/10], Step [780/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [782/938], Loss: 0.0014\n",
      "Epoch [8/10], Step [784/938], Loss: 0.0342\n",
      "Epoch [8/10], Step [786/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [788/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [790/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [792/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [794/938], Loss: 0.0365\n",
      "Epoch [8/10], Step [796/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [798/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [802/938], Loss: 0.0021\n",
      "Epoch [8/10], Step [804/938], Loss: 0.0168\n",
      "Epoch [8/10], Step [806/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [808/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [810/938], Loss: 0.0012\n",
      "Epoch [8/10], Step [812/938], Loss: 0.0027\n",
      "Epoch [8/10], Step [814/938], Loss: 0.0086\n",
      "Epoch [8/10], Step [816/938], Loss: 0.0010\n",
      "Epoch [8/10], Step [818/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [820/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [822/938], Loss: 0.1831\n",
      "Epoch [8/10], Step [824/938], Loss: 0.0023\n",
      "Epoch [8/10], Step [826/938], Loss: 0.0040\n",
      "Epoch [8/10], Step [828/938], Loss: 0.0181\n",
      "Epoch [8/10], Step [830/938], Loss: 0.0102\n",
      "Epoch [8/10], Step [832/938], Loss: 0.1594\n",
      "Epoch [8/10], Step [834/938], Loss: 0.1175\n",
      "Epoch [8/10], Step [836/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [838/938], Loss: 0.0077\n",
      "Epoch [8/10], Step [840/938], Loss: 0.0111\n",
      "Epoch [8/10], Step [842/938], Loss: 0.0115\n",
      "Epoch [8/10], Step [844/938], Loss: 0.0052\n",
      "Epoch [8/10], Step [846/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [848/938], Loss: 0.0543\n",
      "Epoch [8/10], Step [850/938], Loss: 0.0054\n",
      "Epoch [8/10], Step [852/938], Loss: 0.0050\n",
      "Epoch [8/10], Step [854/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [856/938], Loss: 0.0056\n",
      "Epoch [8/10], Step [858/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [860/938], Loss: 0.0042\n",
      "Epoch [8/10], Step [862/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [864/938], Loss: 0.0110\n",
      "Epoch [8/10], Step [866/938], Loss: 0.0142\n",
      "Epoch [8/10], Step [868/938], Loss: 0.0048\n",
      "Epoch [8/10], Step [870/938], Loss: 0.0036\n",
      "Epoch [8/10], Step [872/938], Loss: 0.0242\n",
      "Epoch [8/10], Step [874/938], Loss: 0.0762\n",
      "Epoch [8/10], Step [876/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [878/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [880/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [882/938], Loss: 0.0031\n",
      "Epoch [8/10], Step [884/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [886/938], Loss: 0.0777\n",
      "Epoch [8/10], Step [888/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [890/938], Loss: 0.0001\n",
      "Epoch [8/10], Step [892/938], Loss: 0.0004\n",
      "Epoch [8/10], Step [894/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [896/938], Loss: 0.0599\n",
      "Epoch [8/10], Step [898/938], Loss: 0.0342\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0013\n",
      "Epoch [8/10], Step [902/938], Loss: 0.0034\n",
      "Epoch [8/10], Step [904/938], Loss: 0.0032\n",
      "Epoch [8/10], Step [906/938], Loss: 0.0006\n",
      "Epoch [8/10], Step [908/938], Loss: 0.0251\n",
      "Epoch [8/10], Step [910/938], Loss: 0.0020\n",
      "Epoch [8/10], Step [912/938], Loss: 0.0002\n",
      "Epoch [8/10], Step [914/938], Loss: 0.0219\n",
      "Epoch [8/10], Step [916/938], Loss: 0.0212\n",
      "Epoch [8/10], Step [918/938], Loss: 0.0070\n",
      "Epoch [8/10], Step [920/938], Loss: 0.0073\n",
      "Epoch [8/10], Step [922/938], Loss: 0.0007\n",
      "Epoch [8/10], Step [924/938], Loss: 0.0029\n",
      "Epoch [8/10], Step [926/938], Loss: 0.0158\n",
      "Epoch [8/10], Step [928/938], Loss: 0.0000\n",
      "Epoch [8/10], Step [930/938], Loss: 0.0069\n",
      "Epoch [8/10], Step [932/938], Loss: 0.0093\n",
      "Epoch [8/10], Step [934/938], Loss: 0.0003\n",
      "Epoch [8/10], Step [936/938], Loss: 0.0141\n",
      "Epoch [8/10], Step [938/938], Loss: 0.0001\n",
      "Epoch [8/10], Loss: 0.0097\n",
      "Epoch [9/10], Step [2/938], Loss: 0.0275\n",
      "Epoch [9/10], Step [4/938], Loss: 0.0050\n",
      "Epoch [9/10], Step [6/938], Loss: 0.0069\n",
      "Epoch [9/10], Step [8/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [10/938], Loss: 0.0047\n",
      "Epoch [9/10], Step [12/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [14/938], Loss: 0.0019\n",
      "Epoch [9/10], Step [16/938], Loss: 0.0219\n",
      "Epoch [9/10], Step [18/938], Loss: 0.0170\n",
      "Epoch [9/10], Step [20/938], Loss: 0.0053\n",
      "Epoch [9/10], Step [22/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [24/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [26/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [28/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [30/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [32/938], Loss: 0.0049\n",
      "Epoch [9/10], Step [34/938], Loss: 0.0062\n",
      "Epoch [9/10], Step [36/938], Loss: 0.0590\n",
      "Epoch [9/10], Step [38/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [40/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [42/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [44/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [46/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [48/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [50/938], Loss: 0.0142\n",
      "Epoch [9/10], Step [52/938], Loss: 0.0186\n",
      "Epoch [9/10], Step [54/938], Loss: 0.0190\n",
      "Epoch [9/10], Step [56/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [58/938], Loss: 0.0140\n",
      "Epoch [9/10], Step [60/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [62/938], Loss: 0.0067\n",
      "Epoch [9/10], Step [64/938], Loss: 0.0097\n",
      "Epoch [9/10], Step [66/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [68/938], Loss: 0.0041\n",
      "Epoch [9/10], Step [70/938], Loss: 0.0049\n",
      "Epoch [9/10], Step [72/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [74/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [76/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [78/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [80/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [82/938], Loss: 0.0136\n",
      "Epoch [9/10], Step [84/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [86/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [88/938], Loss: 0.0213\n",
      "Epoch [9/10], Step [90/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [92/938], Loss: 0.0047\n",
      "Epoch [9/10], Step [94/938], Loss: 0.0050\n",
      "Epoch [9/10], Step [96/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [98/938], Loss: 0.0161\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0083\n",
      "Epoch [9/10], Step [102/938], Loss: 0.0029\n",
      "Epoch [9/10], Step [104/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [106/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [108/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [110/938], Loss: 0.0189\n",
      "Epoch [9/10], Step [112/938], Loss: 0.0105\n",
      "Epoch [9/10], Step [114/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [116/938], Loss: 0.0063\n",
      "Epoch [9/10], Step [118/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [120/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [122/938], Loss: 0.0097\n",
      "Epoch [9/10], Step [124/938], Loss: 0.0365\n",
      "Epoch [9/10], Step [126/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [128/938], Loss: 0.0058\n",
      "Epoch [9/10], Step [130/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [132/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [134/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [136/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [138/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [140/938], Loss: 0.0116\n",
      "Epoch [9/10], Step [142/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [144/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [146/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [148/938], Loss: 0.0376\n",
      "Epoch [9/10], Step [150/938], Loss: 0.0573\n",
      "Epoch [9/10], Step [152/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [154/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [156/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [158/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [160/938], Loss: 0.0116\n",
      "Epoch [9/10], Step [162/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [164/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [166/938], Loss: 0.0489\n",
      "Epoch [9/10], Step [168/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [170/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [172/938], Loss: 0.1132\n",
      "Epoch [9/10], Step [174/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [176/938], Loss: 0.0055\n",
      "Epoch [9/10], Step [178/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [180/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [182/938], Loss: 0.0034\n",
      "Epoch [9/10], Step [184/938], Loss: 0.0346\n",
      "Epoch [9/10], Step [186/938], Loss: 0.0077\n",
      "Epoch [9/10], Step [188/938], Loss: 0.0180\n",
      "Epoch [9/10], Step [190/938], Loss: 0.0577\n",
      "Epoch [9/10], Step [192/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [194/938], Loss: 0.0088\n",
      "Epoch [9/10], Step [196/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [198/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [202/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [204/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [206/938], Loss: 0.0120\n",
      "Epoch [9/10], Step [208/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [210/938], Loss: 0.0043\n",
      "Epoch [9/10], Step [212/938], Loss: 0.0030\n",
      "Epoch [9/10], Step [214/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [216/938], Loss: 0.0073\n",
      "Epoch [9/10], Step [218/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [220/938], Loss: 0.0056\n",
      "Epoch [9/10], Step [222/938], Loss: 0.0078\n",
      "Epoch [9/10], Step [224/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [226/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [228/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [230/938], Loss: 0.0041\n",
      "Epoch [9/10], Step [232/938], Loss: 0.0276\n",
      "Epoch [9/10], Step [234/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [236/938], Loss: 0.0117\n",
      "Epoch [9/10], Step [238/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [240/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [242/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [244/938], Loss: 0.0483\n",
      "Epoch [9/10], Step [246/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [248/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [250/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [252/938], Loss: 0.0126\n",
      "Epoch [9/10], Step [254/938], Loss: 0.0250\n",
      "Epoch [9/10], Step [256/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [258/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [260/938], Loss: 0.0054\n",
      "Epoch [9/10], Step [262/938], Loss: 0.0032\n",
      "Epoch [9/10], Step [264/938], Loss: 0.0159\n",
      "Epoch [9/10], Step [266/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [268/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [270/938], Loss: 0.0048\n",
      "Epoch [9/10], Step [272/938], Loss: 0.0057\n",
      "Epoch [9/10], Step [274/938], Loss: 0.0215\n",
      "Epoch [9/10], Step [276/938], Loss: 0.0037\n",
      "Epoch [9/10], Step [278/938], Loss: 0.0287\n",
      "Epoch [9/10], Step [280/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [282/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [284/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [286/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [288/938], Loss: 0.0072\n",
      "Epoch [9/10], Step [290/938], Loss: 0.0145\n",
      "Epoch [9/10], Step [292/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [294/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [296/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [298/938], Loss: 0.0036\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [302/938], Loss: 0.0550\n",
      "Epoch [9/10], Step [304/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [306/938], Loss: 0.0494\n",
      "Epoch [9/10], Step [308/938], Loss: 0.0560\n",
      "Epoch [9/10], Step [310/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [312/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [314/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [316/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [318/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [320/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [322/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [324/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [326/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [328/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [330/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [332/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [334/938], Loss: 0.0081\n",
      "Epoch [9/10], Step [336/938], Loss: 0.0163\n",
      "Epoch [9/10], Step [338/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [340/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [342/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [344/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [346/938], Loss: 0.0246\n",
      "Epoch [9/10], Step [348/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [350/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [352/938], Loss: 0.0745\n",
      "Epoch [9/10], Step [354/938], Loss: 0.0158\n",
      "Epoch [9/10], Step [356/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [358/938], Loss: 0.0210\n",
      "Epoch [9/10], Step [360/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [362/938], Loss: 0.0041\n",
      "Epoch [9/10], Step [364/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [366/938], Loss: 0.0131\n",
      "Epoch [9/10], Step [368/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [370/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [372/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [374/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [376/938], Loss: 0.0741\n",
      "Epoch [9/10], Step [378/938], Loss: 0.0020\n",
      "Epoch [9/10], Step [380/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [382/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [384/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [386/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [388/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [390/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [392/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [394/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [396/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [398/938], Loss: 0.0082\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [402/938], Loss: 0.0030\n",
      "Epoch [9/10], Step [404/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [406/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [408/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [410/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [412/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [414/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [416/938], Loss: 0.0153\n",
      "Epoch [9/10], Step [418/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [420/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [422/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [424/938], Loss: 0.0224\n",
      "Epoch [9/10], Step [426/938], Loss: 0.0319\n",
      "Epoch [9/10], Step [428/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [430/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [432/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [434/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [436/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [438/938], Loss: 0.0063\n",
      "Epoch [9/10], Step [440/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [442/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [444/938], Loss: 0.0089\n",
      "Epoch [9/10], Step [446/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [448/938], Loss: 0.0749\n",
      "Epoch [9/10], Step [450/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [452/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [454/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [456/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [458/938], Loss: 0.0111\n",
      "Epoch [9/10], Step [460/938], Loss: 0.0080\n",
      "Epoch [9/10], Step [462/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [464/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [466/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [468/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [470/938], Loss: 0.0351\n",
      "Epoch [9/10], Step [472/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [474/938], Loss: 0.0403\n",
      "Epoch [9/10], Step [476/938], Loss: 0.0092\n",
      "Epoch [9/10], Step [478/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [480/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [482/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [484/938], Loss: 0.0094\n",
      "Epoch [9/10], Step [486/938], Loss: 0.0159\n",
      "Epoch [9/10], Step [488/938], Loss: 0.1560\n",
      "Epoch [9/10], Step [490/938], Loss: 0.0044\n",
      "Epoch [9/10], Step [492/938], Loss: 0.0897\n",
      "Epoch [9/10], Step [494/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [496/938], Loss: 0.0651\n",
      "Epoch [9/10], Step [498/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [502/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [504/938], Loss: 0.0311\n",
      "Epoch [9/10], Step [506/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [508/938], Loss: 0.0146\n",
      "Epoch [9/10], Step [510/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [512/938], Loss: 0.0037\n",
      "Epoch [9/10], Step [514/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [516/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [518/938], Loss: 0.0027\n",
      "Epoch [9/10], Step [520/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [522/938], Loss: 0.0536\n",
      "Epoch [9/10], Step [524/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [526/938], Loss: 0.0079\n",
      "Epoch [9/10], Step [528/938], Loss: 0.0035\n",
      "Epoch [9/10], Step [530/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [532/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [534/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [536/938], Loss: 0.0063\n",
      "Epoch [9/10], Step [538/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [540/938], Loss: 0.0042\n",
      "Epoch [9/10], Step [542/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [544/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [546/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [548/938], Loss: 0.0073\n",
      "Epoch [9/10], Step [550/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [552/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [554/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [556/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [558/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [560/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [562/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [564/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [566/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [568/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [570/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [572/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [574/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [576/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [578/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [580/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [582/938], Loss: 0.0084\n",
      "Epoch [9/10], Step [584/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [586/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [588/938], Loss: 0.0129\n",
      "Epoch [9/10], Step [590/938], Loss: 0.0017\n",
      "Epoch [9/10], Step [592/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [594/938], Loss: 0.0959\n",
      "Epoch [9/10], Step [596/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [598/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [602/938], Loss: 0.0343\n",
      "Epoch [9/10], Step [604/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [606/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [608/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [610/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [612/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [614/938], Loss: 0.0151\n",
      "Epoch [9/10], Step [616/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [618/938], Loss: 0.0124\n",
      "Epoch [9/10], Step [620/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [622/938], Loss: 0.0342\n",
      "Epoch [9/10], Step [624/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [626/938], Loss: 0.0042\n",
      "Epoch [9/10], Step [628/938], Loss: 0.0122\n",
      "Epoch [9/10], Step [630/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [632/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [634/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [636/938], Loss: 0.0104\n",
      "Epoch [9/10], Step [638/938], Loss: 0.0020\n",
      "Epoch [9/10], Step [640/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [642/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [644/938], Loss: 0.0525\n",
      "Epoch [9/10], Step [646/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [648/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [650/938], Loss: 0.0038\n",
      "Epoch [9/10], Step [652/938], Loss: 0.0036\n",
      "Epoch [9/10], Step [654/938], Loss: 0.0065\n",
      "Epoch [9/10], Step [656/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [658/938], Loss: 0.0226\n",
      "Epoch [9/10], Step [660/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [662/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [664/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [666/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [668/938], Loss: 0.0067\n",
      "Epoch [9/10], Step [670/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [672/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [674/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [676/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [678/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [680/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [682/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [684/938], Loss: 0.0321\n",
      "Epoch [9/10], Step [686/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [688/938], Loss: 0.0835\n",
      "Epoch [9/10], Step [690/938], Loss: 0.0052\n",
      "Epoch [9/10], Step [692/938], Loss: 0.0015\n",
      "Epoch [9/10], Step [694/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [696/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [698/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0035\n",
      "Epoch [9/10], Step [702/938], Loss: 0.0617\n",
      "Epoch [9/10], Step [704/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [706/938], Loss: 0.0020\n",
      "Epoch [9/10], Step [708/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [710/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [712/938], Loss: 0.0030\n",
      "Epoch [9/10], Step [714/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [716/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [718/938], Loss: 0.0118\n",
      "Epoch [9/10], Step [720/938], Loss: 0.0079\n",
      "Epoch [9/10], Step [722/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [724/938], Loss: 0.0037\n",
      "Epoch [9/10], Step [726/938], Loss: 0.0602\n",
      "Epoch [9/10], Step [728/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [730/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [732/938], Loss: 0.0483\n",
      "Epoch [9/10], Step [734/938], Loss: 0.0213\n",
      "Epoch [9/10], Step [736/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [738/938], Loss: 0.0293\n",
      "Epoch [9/10], Step [740/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [742/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [744/938], Loss: 0.0194\n",
      "Epoch [9/10], Step [746/938], Loss: 0.0945\n",
      "Epoch [9/10], Step [748/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [750/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [752/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [754/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [756/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [758/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [760/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [762/938], Loss: 0.0146\n",
      "Epoch [9/10], Step [764/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [766/938], Loss: 0.1438\n",
      "Epoch [9/10], Step [768/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [770/938], Loss: 0.0509\n",
      "Epoch [9/10], Step [772/938], Loss: 0.0025\n",
      "Epoch [9/10], Step [774/938], Loss: 0.0524\n",
      "Epoch [9/10], Step [776/938], Loss: 0.0031\n",
      "Epoch [9/10], Step [778/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [780/938], Loss: 0.0013\n",
      "Epoch [9/10], Step [782/938], Loss: 0.0055\n",
      "Epoch [9/10], Step [784/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [786/938], Loss: 0.0027\n",
      "Epoch [9/10], Step [788/938], Loss: 0.0036\n",
      "Epoch [9/10], Step [790/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [792/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [794/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [796/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [798/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [802/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [804/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [806/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [808/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [810/938], Loss: 0.0150\n",
      "Epoch [9/10], Step [812/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [814/938], Loss: 0.0028\n",
      "Epoch [9/10], Step [816/938], Loss: 0.0007\n",
      "Epoch [9/10], Step [818/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [820/938], Loss: 0.0258\n",
      "Epoch [9/10], Step [822/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [824/938], Loss: 0.0209\n",
      "Epoch [9/10], Step [826/938], Loss: 0.0442\n",
      "Epoch [9/10], Step [828/938], Loss: 0.0132\n",
      "Epoch [9/10], Step [830/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [832/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [834/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [836/938], Loss: 0.0029\n",
      "Epoch [9/10], Step [838/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [840/938], Loss: 0.0054\n",
      "Epoch [9/10], Step [842/938], Loss: 0.0150\n",
      "Epoch [9/10], Step [844/938], Loss: 0.0014\n",
      "Epoch [9/10], Step [846/938], Loss: 0.0060\n",
      "Epoch [9/10], Step [848/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [850/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [852/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [854/938], Loss: 0.1212\n",
      "Epoch [9/10], Step [856/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [858/938], Loss: 0.0088\n",
      "Epoch [9/10], Step [860/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [862/938], Loss: 0.0106\n",
      "Epoch [9/10], Step [864/938], Loss: 0.0019\n",
      "Epoch [9/10], Step [866/938], Loss: 0.0231\n",
      "Epoch [9/10], Step [868/938], Loss: 0.0000\n",
      "Epoch [9/10], Step [870/938], Loss: 0.0010\n",
      "Epoch [9/10], Step [872/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [874/938], Loss: 0.0082\n",
      "Epoch [9/10], Step [876/938], Loss: 0.0022\n",
      "Epoch [9/10], Step [878/938], Loss: 0.0020\n",
      "Epoch [9/10], Step [880/938], Loss: 0.0096\n",
      "Epoch [9/10], Step [882/938], Loss: 0.0011\n",
      "Epoch [9/10], Step [884/938], Loss: 0.0159\n",
      "Epoch [9/10], Step [886/938], Loss: 0.0009\n",
      "Epoch [9/10], Step [888/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [890/938], Loss: 0.0008\n",
      "Epoch [9/10], Step [892/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [894/938], Loss: 0.0018\n",
      "Epoch [9/10], Step [896/938], Loss: 0.0721\n",
      "Epoch [9/10], Step [898/938], Loss: 0.0004\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0001\n",
      "Epoch [9/10], Step [902/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [904/938], Loss: 0.0266\n",
      "Epoch [9/10], Step [906/938], Loss: 0.0016\n",
      "Epoch [9/10], Step [908/938], Loss: 0.0005\n",
      "Epoch [9/10], Step [910/938], Loss: 0.0021\n",
      "Epoch [9/10], Step [912/938], Loss: 0.0026\n",
      "Epoch [9/10], Step [914/938], Loss: 0.0012\n",
      "Epoch [9/10], Step [916/938], Loss: 0.0392\n",
      "Epoch [9/10], Step [918/938], Loss: 0.0364\n",
      "Epoch [9/10], Step [920/938], Loss: 0.0019\n",
      "Epoch [9/10], Step [922/938], Loss: 0.0023\n",
      "Epoch [9/10], Step [924/938], Loss: 0.0243\n",
      "Epoch [9/10], Step [926/938], Loss: 0.0051\n",
      "Epoch [9/10], Step [928/938], Loss: 0.0003\n",
      "Epoch [9/10], Step [930/938], Loss: 0.0250\n",
      "Epoch [9/10], Step [932/938], Loss: 0.0238\n",
      "Epoch [9/10], Step [934/938], Loss: 0.0002\n",
      "Epoch [9/10], Step [936/938], Loss: 0.0006\n",
      "Epoch [9/10], Step [938/938], Loss: 0.0010\n",
      "Epoch [9/10], Loss: 0.0085\n",
      "Epoch [10/10], Step [2/938], Loss: 0.0447\n",
      "Epoch [10/10], Step [4/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [6/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [8/938], Loss: 0.0032\n",
      "Epoch [10/10], Step [10/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [12/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [14/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [16/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [18/938], Loss: 0.0167\n",
      "Epoch [10/10], Step [20/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [22/938], Loss: 0.0062\n",
      "Epoch [10/10], Step [24/938], Loss: 0.0283\n",
      "Epoch [10/10], Step [26/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [28/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [30/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [32/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [34/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [36/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [38/938], Loss: 0.0040\n",
      "Epoch [10/10], Step [40/938], Loss: 0.0037\n",
      "Epoch [10/10], Step [42/938], Loss: 0.0201\n",
      "Epoch [10/10], Step [44/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [46/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [48/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [50/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [52/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [54/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [56/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [58/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [60/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [62/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [64/938], Loss: 0.0147\n",
      "Epoch [10/10], Step [66/938], Loss: 0.0045\n",
      "Epoch [10/10], Step [68/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [70/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [72/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [74/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [76/938], Loss: 0.0358\n",
      "Epoch [10/10], Step [78/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [80/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [82/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [84/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [86/938], Loss: 0.0021\n",
      "Epoch [10/10], Step [88/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [90/938], Loss: 0.0094\n",
      "Epoch [10/10], Step [92/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [94/938], Loss: 0.0031\n",
      "Epoch [10/10], Step [96/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [98/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0020\n",
      "Epoch [10/10], Step [102/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [104/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [106/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [108/938], Loss: 0.0050\n",
      "Epoch [10/10], Step [110/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [112/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [114/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [116/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [118/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [120/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [122/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [124/938], Loss: 0.0027\n",
      "Epoch [10/10], Step [126/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [128/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [130/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [132/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [134/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [136/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [138/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [140/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [142/938], Loss: 0.0031\n",
      "Epoch [10/10], Step [144/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [146/938], Loss: 0.0017\n",
      "Epoch [10/10], Step [148/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [150/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [152/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [154/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [156/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [158/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [160/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [162/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [164/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [166/938], Loss: 0.0065\n",
      "Epoch [10/10], Step [168/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [170/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [172/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [174/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [176/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [178/938], Loss: 0.0117\n",
      "Epoch [10/10], Step [180/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [182/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [184/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [186/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [188/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [190/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [192/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [194/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [196/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [198/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [202/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [204/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [206/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [208/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [210/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [212/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [214/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [216/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [218/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [220/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [222/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [224/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [226/938], Loss: 0.0257\n",
      "Epoch [10/10], Step [228/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [230/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [232/938], Loss: 0.0796\n",
      "Epoch [10/10], Step [234/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [236/938], Loss: 0.0621\n",
      "Epoch [10/10], Step [238/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [240/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [242/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [244/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [246/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [248/938], Loss: 0.0129\n",
      "Epoch [10/10], Step [250/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [252/938], Loss: 0.0045\n",
      "Epoch [10/10], Step [254/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [256/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [258/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [260/938], Loss: 0.0575\n",
      "Epoch [10/10], Step [262/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [264/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [266/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [268/938], Loss: 0.0022\n",
      "Epoch [10/10], Step [270/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [272/938], Loss: 0.0106\n",
      "Epoch [10/10], Step [274/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [276/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [278/938], Loss: 0.0180\n",
      "Epoch [10/10], Step [280/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [282/938], Loss: 0.0271\n",
      "Epoch [10/10], Step [284/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [286/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [288/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [290/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [292/938], Loss: 0.0027\n",
      "Epoch [10/10], Step [294/938], Loss: 0.0687\n",
      "Epoch [10/10], Step [296/938], Loss: 0.0038\n",
      "Epoch [10/10], Step [298/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0036\n",
      "Epoch [10/10], Step [302/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [304/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [306/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [308/938], Loss: 0.0552\n",
      "Epoch [10/10], Step [310/938], Loss: 0.0279\n",
      "Epoch [10/10], Step [312/938], Loss: 0.0261\n",
      "Epoch [10/10], Step [314/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [316/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [318/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [320/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [322/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [324/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [326/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [328/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [330/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [332/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [334/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [336/938], Loss: 0.0127\n",
      "Epoch [10/10], Step [338/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [340/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [342/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [344/938], Loss: 0.0050\n",
      "Epoch [10/10], Step [346/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [348/938], Loss: 0.0077\n",
      "Epoch [10/10], Step [350/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [352/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [354/938], Loss: 0.0408\n",
      "Epoch [10/10], Step [356/938], Loss: 0.0117\n",
      "Epoch [10/10], Step [358/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [360/938], Loss: 0.0101\n",
      "Epoch [10/10], Step [362/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [364/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [366/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [368/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [370/938], Loss: 0.0022\n",
      "Epoch [10/10], Step [372/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [374/938], Loss: 0.0064\n",
      "Epoch [10/10], Step [376/938], Loss: 0.1356\n",
      "Epoch [10/10], Step [378/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [380/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [382/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [384/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [386/938], Loss: 0.0372\n",
      "Epoch [10/10], Step [388/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [390/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [392/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [394/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [396/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [398/938], Loss: 0.0175\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [402/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [404/938], Loss: 0.0539\n",
      "Epoch [10/10], Step [406/938], Loss: 0.1133\n",
      "Epoch [10/10], Step [408/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [410/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [412/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [414/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [416/938], Loss: 0.0294\n",
      "Epoch [10/10], Step [418/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [420/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [422/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [424/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [426/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [428/938], Loss: 0.0351\n",
      "Epoch [10/10], Step [430/938], Loss: 0.0046\n",
      "Epoch [10/10], Step [432/938], Loss: 0.0306\n",
      "Epoch [10/10], Step [434/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [436/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [438/938], Loss: 0.0052\n",
      "Epoch [10/10], Step [440/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [442/938], Loss: 0.0034\n",
      "Epoch [10/10], Step [444/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [446/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [448/938], Loss: 0.0174\n",
      "Epoch [10/10], Step [450/938], Loss: 0.0797\n",
      "Epoch [10/10], Step [452/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [454/938], Loss: 0.0020\n",
      "Epoch [10/10], Step [456/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [458/938], Loss: 0.0067\n",
      "Epoch [10/10], Step [460/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [462/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [464/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [466/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [468/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [470/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [472/938], Loss: 0.0047\n",
      "Epoch [10/10], Step [474/938], Loss: 0.0120\n",
      "Epoch [10/10], Step [476/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [478/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [480/938], Loss: 0.0109\n",
      "Epoch [10/10], Step [482/938], Loss: 0.0274\n",
      "Epoch [10/10], Step [484/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [486/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [488/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [490/938], Loss: 0.0402\n",
      "Epoch [10/10], Step [492/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [494/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [496/938], Loss: 0.0021\n",
      "Epoch [10/10], Step [498/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0497\n",
      "Epoch [10/10], Step [502/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [504/938], Loss: 0.0161\n",
      "Epoch [10/10], Step [506/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [508/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [510/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [512/938], Loss: 0.0031\n",
      "Epoch [10/10], Step [514/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [516/938], Loss: 0.0102\n",
      "Epoch [10/10], Step [518/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [520/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [522/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [524/938], Loss: 0.0270\n",
      "Epoch [10/10], Step [526/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [528/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [530/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [532/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [534/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [536/938], Loss: 0.0373\n",
      "Epoch [10/10], Step [538/938], Loss: 0.0034\n",
      "Epoch [10/10], Step [540/938], Loss: 0.0129\n",
      "Epoch [10/10], Step [542/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [544/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [546/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [548/938], Loss: 0.0060\n",
      "Epoch [10/10], Step [550/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [552/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [554/938], Loss: 0.0639\n",
      "Epoch [10/10], Step [556/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [558/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [560/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [562/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [564/938], Loss: 0.0047\n",
      "Epoch [10/10], Step [566/938], Loss: 0.0334\n",
      "Epoch [10/10], Step [568/938], Loss: 0.0063\n",
      "Epoch [10/10], Step [570/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [572/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [574/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [576/938], Loss: 0.0187\n",
      "Epoch [10/10], Step [578/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [580/938], Loss: 0.0050\n",
      "Epoch [10/10], Step [582/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [584/938], Loss: 0.0483\n",
      "Epoch [10/10], Step [586/938], Loss: 0.0136\n",
      "Epoch [10/10], Step [588/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [590/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [592/938], Loss: 0.0035\n",
      "Epoch [10/10], Step [594/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [596/938], Loss: 0.0021\n",
      "Epoch [10/10], Step [598/938], Loss: 0.0036\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [602/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [604/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [606/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [608/938], Loss: 0.0034\n",
      "Epoch [10/10], Step [610/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [612/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [614/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [616/938], Loss: 0.0057\n",
      "Epoch [10/10], Step [618/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [620/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [622/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [624/938], Loss: 0.0048\n",
      "Epoch [10/10], Step [626/938], Loss: 0.0057\n",
      "Epoch [10/10], Step [628/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [630/938], Loss: 0.0044\n",
      "Epoch [10/10], Step [632/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [634/938], Loss: 0.0177\n",
      "Epoch [10/10], Step [636/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [638/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [640/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [642/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [644/938], Loss: 0.0076\n",
      "Epoch [10/10], Step [646/938], Loss: 0.0474\n",
      "Epoch [10/10], Step [648/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [650/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [652/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [654/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [656/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [658/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [660/938], Loss: 0.0027\n",
      "Epoch [10/10], Step [662/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [664/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [666/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [668/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [670/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [672/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [674/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [676/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [678/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [680/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [682/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [684/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [686/938], Loss: 0.0052\n",
      "Epoch [10/10], Step [688/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [690/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [692/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [694/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [696/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [698/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0020\n",
      "Epoch [10/10], Step [702/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [704/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [706/938], Loss: 0.0028\n",
      "Epoch [10/10], Step [708/938], Loss: 0.0019\n",
      "Epoch [10/10], Step [710/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [712/938], Loss: 0.0026\n",
      "Epoch [10/10], Step [714/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [716/938], Loss: 0.0086\n",
      "Epoch [10/10], Step [718/938], Loss: 0.0042\n",
      "Epoch [10/10], Step [720/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [722/938], Loss: 0.0337\n",
      "Epoch [10/10], Step [724/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [726/938], Loss: 0.0086\n",
      "Epoch [10/10], Step [728/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [730/938], Loss: 0.0046\n",
      "Epoch [10/10], Step [732/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [734/938], Loss: 0.0357\n",
      "Epoch [10/10], Step [736/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [738/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [740/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [742/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [744/938], Loss: 0.1024\n",
      "Epoch [10/10], Step [746/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [748/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [750/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [752/938], Loss: 0.0030\n",
      "Epoch [10/10], Step [754/938], Loss: 0.0086\n",
      "Epoch [10/10], Step [756/938], Loss: 0.0239\n",
      "Epoch [10/10], Step [758/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [760/938], Loss: 0.0071\n",
      "Epoch [10/10], Step [762/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [764/938], Loss: 0.0029\n",
      "Epoch [10/10], Step [766/938], Loss: 0.0114\n",
      "Epoch [10/10], Step [768/938], Loss: 0.0304\n",
      "Epoch [10/10], Step [770/938], Loss: 0.0052\n",
      "Epoch [10/10], Step [772/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [774/938], Loss: 0.0126\n",
      "Epoch [10/10], Step [776/938], Loss: 0.0081\n",
      "Epoch [10/10], Step [778/938], Loss: 0.0012\n",
      "Epoch [10/10], Step [780/938], Loss: 0.0089\n",
      "Epoch [10/10], Step [782/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [784/938], Loss: 0.0305\n",
      "Epoch [10/10], Step [786/938], Loss: 0.0082\n",
      "Epoch [10/10], Step [788/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [790/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [792/938], Loss: 0.0096\n",
      "Epoch [10/10], Step [794/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [796/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [798/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0005\n",
      "Epoch [10/10], Step [802/938], Loss: 0.0075\n",
      "Epoch [10/10], Step [804/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [806/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [808/938], Loss: 0.0006\n",
      "Epoch [10/10], Step [810/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [812/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [814/938], Loss: 0.0053\n",
      "Epoch [10/10], Step [816/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [818/938], Loss: 0.0210\n",
      "Epoch [10/10], Step [820/938], Loss: 0.0025\n",
      "Epoch [10/10], Step [822/938], Loss: 0.0013\n",
      "Epoch [10/10], Step [824/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [826/938], Loss: 0.0048\n",
      "Epoch [10/10], Step [828/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [830/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [832/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [834/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [836/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [838/938], Loss: 0.0011\n",
      "Epoch [10/10], Step [840/938], Loss: 0.0192\n",
      "Epoch [10/10], Step [842/938], Loss: 0.0357\n",
      "Epoch [10/10], Step [844/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [846/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [848/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [850/938], Loss: 0.0221\n",
      "Epoch [10/10], Step [852/938], Loss: 0.0009\n",
      "Epoch [10/10], Step [854/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [856/938], Loss: 0.0016\n",
      "Epoch [10/10], Step [858/938], Loss: 0.0372\n",
      "Epoch [10/10], Step [860/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [862/938], Loss: 0.0053\n",
      "Epoch [10/10], Step [864/938], Loss: 0.0024\n",
      "Epoch [10/10], Step [866/938], Loss: 0.0023\n",
      "Epoch [10/10], Step [868/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [870/938], Loss: 0.0203\n",
      "Epoch [10/10], Step [872/938], Loss: 0.0007\n",
      "Epoch [10/10], Step [874/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [876/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [878/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [880/938], Loss: 0.0002\n",
      "Epoch [10/10], Step [882/938], Loss: 0.0073\n",
      "Epoch [10/10], Step [884/938], Loss: 0.0018\n",
      "Epoch [10/10], Step [886/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [888/938], Loss: 0.0008\n",
      "Epoch [10/10], Step [890/938], Loss: 0.0001\n",
      "Epoch [10/10], Step [892/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [894/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [896/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [898/938], Loss: 0.0174\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0045\n",
      "Epoch [10/10], Step [902/938], Loss: 0.0115\n",
      "Epoch [10/10], Step [904/938], Loss: 0.0031\n",
      "Epoch [10/10], Step [906/938], Loss: 0.0220\n",
      "Epoch [10/10], Step [908/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [910/938], Loss: 0.0028\n",
      "Epoch [10/10], Step [912/938], Loss: 0.0003\n",
      "Epoch [10/10], Step [914/938], Loss: 0.0074\n",
      "Epoch [10/10], Step [916/938], Loss: 0.0040\n",
      "Epoch [10/10], Step [918/938], Loss: 0.0069\n",
      "Epoch [10/10], Step [920/938], Loss: 0.0762\n",
      "Epoch [10/10], Step [922/938], Loss: 0.0010\n",
      "Epoch [10/10], Step [924/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [926/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [928/938], Loss: 0.0000\n",
      "Epoch [10/10], Step [930/938], Loss: 0.0121\n",
      "Epoch [10/10], Step [932/938], Loss: 0.0014\n",
      "Epoch [10/10], Step [934/938], Loss: 0.0112\n",
      "Epoch [10/10], Step [936/938], Loss: 0.0004\n",
      "Epoch [10/10], Step [938/938], Loss: 0.0002\n",
      "Epoch [10/10], Loss: 0.0064\n",
      "Training is finished!\n",
      "Accuracy: 99.12%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model with RMSprop\")\n",
    "rmsprop_model = SimpleCNN().to(device)\n",
    "rms_train_loss = train_model('Adam', rmsprop_model, train_loader, criterion, num_epochs)\n",
    "accuracy_sgd = evaluate_model(rmsprop_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eFHGYkqVVELf",
   "metadata": {
    "id": "eFHGYkqVVELf"
   },
   "source": [
    "##### Plotting the training loss for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "iJPlZGRYYe3T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "iJPlZGRYYe3T",
    "outputId": "329f575a-ad7a-4435-bb47-e73246b52ba2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss for comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sgd_train_loss, label='SGD')\n",
    "plt.plot(adam_train_loss, label='Adam')\n",
    "plt.plot(rms_train_loss, label='RMSprop')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RSKNVFVwY1bl",
   "metadata": {
    "id": "RSKNVFVwY1bl"
   },
   "source": [
    "##### Compare the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tr0hvj4HY-n8",
   "metadata": {
    "id": "Tr0hvj4HY-n8"
   },
   "source": [
    "After running the training for each optimizer, compare:\n",
    "\n",
    "Training Loss: How fast does each optimizer reduce the loss over the epochs?\n",
    "\n",
    "Accuracy: What accuracy does each optimizer achieve on the test set?\n",
    "\n",
    "Convergence Speed: How quickly does each optimizer converge?\n",
    "\n",
    "\n",
    "##### Expected Results:\n",
    "\n",
    "|Optimizer|Final Accuracy(%)|Convergence|Remarks|\n",
    "|---|---|---|---|\n",
    "|**SGD**|Moderate|Slow|SGD tends to converge slower, especially without momentum.|\n",
    "|**Adam**|High|Fast|Adam generally converges quickly due to its adaptive learning rates and momentum.|\n",
    "|**RMSprop**|High|Moderate|RMSprop can perform similarly to Adam in terms of accuracy but may converge slightly slower.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rzCxrvqUZQxk",
   "metadata": {
    "id": "rzCxrvqUZQxk"
   },
   "source": [
    "### 9. Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural network architecture and task. Consider factors such as convergence speed, stability, and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hw5XfLEZahax",
   "metadata": {
    "id": "hw5XfLEZahax"
   },
   "source": [
    "|Optimizer|Convergence|Stability|Generalization Performance|\n",
    "|---|---|---|---|\n",
    "|**GD**|Traditional gradient descent calculates the gradient over the entire dataset, which leads to slow convergence. It is not practical for large datasets because it updates the weights only after processing the entire dataset.|It is a bit unstable as it can get blocked in the valleys or flat surfaces while reducing the loss.|GD can generalize but without momentum it fails to escape the local minima.|\n",
    "|**SGD**|Unlike GD, SGD updates the model parameters for every training example, which speeds up learning but can introduce high variance in the updates, leading to oscillations around the optimal value.|While it can be unstable due to its high variance, introducing momentum (SGD with Momentum) reduces oscillations and improves stability, especially when navigating flat areas or valleys in the loss surface.|Due to its high variance, SGD can often escape local minima, which sometimes leads to better generalization compared to more aggressive optimizers. With proper tuning of learning rates and decaying schedules, SGD can generalize well, though it typically converges slower.|\n",
    "|**Adam**|Adam combines the benefits of both momentum (faster convergence) and adaptive learning rates. It tends to converge faster than both SGD and RMSprop, making it a popular choice for many applications.|Adam typically offers better stability due to its use of both momentum and adaptive learning rates. However, in some cases, especially for very complex tasks or noisy datasets, Adam can lead to poor generalization because it may converge too fast and overfit.|While Adam is good for fast convergence, it sometimes struggles with generalization because it tends to overfit to the training data, especially if used with a high learning rate. Regularization techniques such as dropout or weight decay are often necessary when using Adam.|\n",
    "|**RMSprop**|RMSprop adapts learning rates based on the data and is faster than vanilla SGD in many cases, especially for deep networks. It can perform similarly to Adam but may need more careful tuning of hyperparameters.|RMSprop offers good stability, especially in noisy environments, due to its adaptive learning rate. It works well in practice for recurrent neural networks (RNNs) and deep architectures.|RMSprop has adaptive learning rates, making it robust on tasks with noisy or non-stationary objectives. It can generalize well but may require learning rate scheduling or other regularization methods to avoid overfitting.|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
