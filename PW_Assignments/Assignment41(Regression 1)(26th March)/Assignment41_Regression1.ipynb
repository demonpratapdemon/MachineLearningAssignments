{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53d8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb79b19",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04ac00",
   "metadata": {},
   "source": [
    "|Point|Simple|Multiple|\n",
    "|---|---|---|\n",
    "|**1. Number of Independent Variables**|Only one independent variable tp determine the output variable.|Mulitple independent varibales which determine only one output variable|\n",
    "|**2. Equation**|The equation of a simple linear regression model is of the form: $$y = a + bx$$|The equation of a multiple linear regression model is of the form: $$y = a + b1*x1 + b2*x2 + ... + bn*xn$$|\n",
    "|**3. Example**| Suppose we want to predict a person's weight (y) based on their height (x). In this case, you would use a simple linear regression model.|Suppose you want to predict a person's salary (y) based on their years of experience (x1) and the number of years of education (x2). In this case, you would use a multiple linear regression model.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43224c73",
   "metadata": {},
   "source": [
    "## Example of Simple Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da162327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Weight for a Height of 175 cm: 75.00 kg\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "data = {'Height': [150, 160, 170, 180, 190],\n",
    "        'Weight': [50, 60, 70, 80, 90]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit a simple linear regression model\n",
    "x = df[['Height']]\n",
    "y = df['Weight']\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Predict weight based on height\n",
    "height_to_predict = np.array([[175]])  # Height to predict the weight for\n",
    "predicted_weight = model.predict(height_to_predict)\n",
    "print(f\"Predicted Weight for a Height of 175 cm: {predicted_weight[0]:.2f} kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c371b66",
   "metadata": {},
   "source": [
    "## Example of Multiple Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64250964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Salary for 6 years of experience and 19 years of education: $65000.00\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = {'Experience': [2, 5, 8, 10, 12],\n",
    "        'Education': [16, 18, 20, 22, 24],\n",
    "        'Salary': [50000, 60000, 70000, 80000, 90000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit a multiple linear regression model\n",
    "X = df[['Experience', 'Education']]\n",
    "y = df['Salary']\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict salary based on experience and education\n",
    "experience_to_predict = 6\n",
    "education_to_predict = 19\n",
    "predicted_salary = model.predict([[experience_to_predict, education_to_predict]])\n",
    "print(f\"Predicted Salary for {experience_to_predict} years of experience and {education_to_predict} years of education: ${predicted_salary[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf5bcd",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a92b8a8",
   "metadata": {},
   "source": [
    "Linear regression relies on several assumptions to provide valid and reliable results. Violations of these assumptions can lead to unreliable or incorrect conclusions. Here are the key assumptions of linear regression and ways to check whether these assumptions hold in a given dataset:\n",
    "\n",
    "1. **Linearity:** This assumption states that the relationship between the independent variables and the dependent variable is linear. You can check this assumption by creating scatterplots of the dependent variable against each independent variable. If the data points form a reasonably linear pattern, the assumption may hold. If the scatterplots reveal a nonlinear relationship, a transformation of the data or a different model might be more appropriate.\n",
    "\n",
    "2. **Independence of Errors:** The errors (residuals) in the model should be independent of each other. This assumption can be checked by examining the residuals using a **residuals plot**. There should be no clear patterns, trends, or autocorrelation in the residuals. We can also use statistical tests for autocorrelation, such as the Durbin-Watson test.\n",
    "\n",
    "3. **Homoscedasticity:** Homoscedasticity means that the variance of the residuals is constant across all levels of the independent variables. To check this assumption, we can create a residuals vs. fitted values plot. If the spread of residuals is roughly constant across the range of fitted values, homoscedasticity is likely met. If the spread widens or narrows systematically, you may have heteroscedasticity, which can be addressed through transformations or robust regression techniques.\n",
    "\n",
    "4. **Normality of Residuals:** This assumption assumes that the residuals follow a normal distribution. We can check this by creating a histogram or a quantile-quantile (Q-Q) plot of the residuals. If the histogram is roughly bell-shaped and the Q-Q plot follows a straight line, the assumption is likely met. Deviations from normality may indicate that your data requires transformation or the use of a generalized linear model.\n",
    "\n",
    "5. **No or Little Multicollinearity:** Multicollinearity occurs when independent variables are highly correlated with each other, making it challenging to isolate their individual effects. We can check for multicollinearity by calculating correlation coefficients or variance inflation factors (VIF) for the independent variables. High correlations or VIF values above 5-10 may indicate multicollinearity, requiring variable selection or transformation.\n",
    "\n",
    "6. **No Endogeneity:** Endogeneity occurs when an independent variable is correlated with the error term. While this assumption is often addressed through study design and causal modeling, you can also use instrumental variables or control functions to account for endogeneity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d3428",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadee64",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations that help us understand the relationship between the independent variable(s) and the dependent variable. Here's how to interpret them using a real-world scenario:\n",
    "\n",
    "##### Intercept (Intercept Term or Constant, often denoted as 'a' in the model equation):\n",
    "\n",
    "- The intercept represents the predicted value of the dependent variable (Y) when all independent variables (X) are zero.\n",
    "- In many cases, the intercept might not have a meaningful interpretation, especially when it doesn't make sense for the independent variables to be zero. For example, in a linear regression model predicting house prices based on the number of bedrooms, the intercept might represent the base price of a house with zero bedrooms, which is not a meaningful concept.\n",
    "- In practice, the intercept often serves as a \"baseline\" or starting point, and the main focus is on how the independent variables affect the dependent variable's value.\n",
    "\n",
    "##### Slope (Coefficient of the independent variable, often denoted as 'b' in the model equation):\n",
    "\n",
    "- The slope represents the change in the dependent variable (Y) associated with a one-unit change in the corresponding independent variable (X), while holding all other independent variables constant.\n",
    "- It quantifies the strength and direction of the linear relationship between the independent and dependent variables.\n",
    "- A positive slope means that an increase in the independent variable is associated with an increase in the dependent variable, while a negative slope indicates a decrease in the dependent variable when the independent variable increases.\n",
    "\n",
    "##### Example using a Real-World Scenario:\n",
    "\n",
    "Let's consider a real-world scenario where we want to predict a student's final exam score (dependent variable, Y) based on the number of hours they studied (independent variable, X).\n",
    "\n",
    "- **Intercept Interpretation:** If the linear regression model predicts an intercept (constant) of 50, it suggests that a student who didn't study (X = 0 hours) is expected to score 50 on the final exam. However, this might not be a meaningful interpretation, as a student cannot score 50 without studying.\n",
    "\n",
    "- **Slope Interpretation:** If the slope of the number of hours studied is 5, it means that, on average, for each additional hour a student studies, their final exam score is expected to increase by 5 points when we hold other factors constant. So, if a student studied for 3 hours (X = 3), their predicted score would be 50 (intercept) + (3 hours * 5 points/hour) = 65.\n",
    "\n",
    "In this scenario, the slope of 5 indicates that studying more hours is associated with higher exam scores, and the intercept provides a baseline value for a student who didn't study. It's important to interpret these values in the context of your specific problem and consider any assumptions and limitations of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872becf",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4cea2a",
   "metadata": {},
   "source": [
    "**Gradient descent** is a fundamental optimization algorithm used in machine learning, particularly in the context of training models to minimize a loss function. It is the key method for finding the optimal model parameters that result in the best model performance.\n",
    "\n",
    "##### Concept of Gradient Descent:\n",
    "\n",
    "Gradient descent is an iterative optimization technique used to minimize a cost or loss function. The idea behind gradient descent is to find the minimum of a function by adjusting the model parameters in small steps using a parameter called **learning rate**. It takes its name from the mathematical concept of a gradient, which is a vector that points in the direction of the steepest increase of a function.\n",
    "\n",
    "The basic steps of gradient descent are as follows:\n",
    "\n",
    "1. **Initialize Parameters:** Start with initial values for the model parameters (weights and biases).\n",
    "\n",
    "2. **Compute the Gradient:** Calculate the gradient (or derivative) of the cost function with respect to each parameter. The gradient indicates the direction of the steepest increase in the cost function.\n",
    "\n",
    "3. **Update Parameters:** Adjust the parameters by moving in the opposite direction of the gradient, scaled by a learning rate (step size). The learning rate controls the size of the steps taken during optimization.\n",
    "\n",
    "4. **Repeat:** Repeat steps 2 and 3 iteratively until a stopping criterion is met. Common stopping criteria include a maximum number of iterations, achieving a target cost value, or observing only small changes in the parameters.\n",
    "\n",
    "\n",
    "##### Role in Machine Learning:\n",
    "\n",
    "Gradient descent is a core technique used in various machine learning algorithms, particularly in the training of models that involve cost function optimization. Here are some key applications of gradient descent in machine learning:\n",
    "\n",
    "1. **Linear Regression:** In linear regression, gradient descent is used to find the optimal regression coefficients that minimize the **mean squared error** between the predicted and actual values.\n",
    "\n",
    "2. **Logistic Regression:** Gradient descent is employed in logistic regression to find the optimal parameters that **maximize the likelihood** of the observed outcomes.\n",
    "\n",
    "3. **Neural Networks:** Gradient descent is a fundamental optimization algorithm for training neural networks. It **adjusts the weights and biases of neurons** to minimize the loss function, which is often a complex non-convex function.\n",
    "\n",
    "4. **Support Vector Machines (SVM):** SVMs use gradient descent to **optimize the margin** between different classes while **minimizing the classification error**.\n",
    "\n",
    "5. **Deep Learning:** In deep learning, gradient descent variants like stochastic gradient descent (SGD), Adam, and RMSprop are commonly used to train deep neural networks with many layers and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2003d68",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694566b4",
   "metadata": {},
   "source": [
    "**Multiple Linear Regression** is an extension of the simple linear regression model that allows us to model the relationship between a dependent variable (target) and multiple independent variables (features or predictors). In simple linear regression, we have one dependent variable and one independent variable, whereas in multiple linear regression, you have one dependent variable and two or more independent variables. Here's an overview of multiple linear regression and how it differs from simple linear regression:\n",
    "\n",
    "Multiple Linear Regression Model:\n",
    "In a multiple linear regression model, the relationship between the dependent variable (Y) and multiple independent variables (X1, X2, X3, ..., Xn) is expressed by the following equation:\n",
    "\n",
    "$$ Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + ... + \\beta_{n}X_{n} + \\epsilon$$\n",
    "\n",
    "The difference between simple and multiple linear regression is discussed in details in Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42468b2",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07588e04",
   "metadata": {},
   "source": [
    "**Multicollinearity** is a common issue in multiple linear regression when two or more independent variables in the model are highly correlated with each other. This can lead to several problems and challenges in regression analysis. Here's an explanation of multicollinearity and how to detect and address this issue:\n",
    "\n",
    "#### Concept of Multicollinearity:\n",
    "\n",
    "In multiple linear regression, we aim to estimate the relationships between a dependent variable and multiple independent variables. Multicollinearity occurs when two or more independent variables are highly correlated, meaning that they move together in a predictable way. This correlation can make it challenging to isolate the individual effects of these variables on the dependent variable. It can also lead to problems in estimating the regression coefficients accurately.\n",
    "\n",
    "##### Issues Caused by Multicollinearity:\n",
    "\n",
    "- **Inflated Standard Errors:** Multicollinearity inflates the standard errors of the regression coefficients. As a result, it becomes harder to identify statistically significant predictors, and **confidence intervals become wider**.\n",
    "\n",
    "- **Unstable Coefficients:** Small changes in the data can lead to large changes in the estimated coefficients. This instability makes the model less reliable for prediction and interpretation.\n",
    "\n",
    "- **Ambiguity in Variable Importance:** Multicollinearity makes it difficult to determine the relative importance of correlated variables in explaining the dependent variable's variation.\n",
    "\n",
    "##### Detection of Multicollinearity:\n",
    "\n",
    "To detect multicollinearity in your data and regression model, you can use the following methods:\n",
    "\n",
    "- Correlation Matrix: Calculate the correlation matrix of the independent variables. High correlation coefficients (close to 1 or -1) between pairs of variables indicate potential multicollinearity.\n",
    "\n",
    "- Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much the variance of the estimated regression coefficient is increased due to multicollinearity. VIF values above 5-10 are often considered indicative of multicollinearity.\n",
    "\n",
    "- Scatterplots: Creating **scatterplots** or **pair plots** to visualize the relationships between the independent variables. Strong linear relationships may suggest multicollinearity.\n",
    "\n",
    "##### Addressing Multicollinearity:\n",
    "\n",
    "- **Variable Selection:** Consider removing one or more of the highly correlated variables from the model. This may involve domain knowledge or feature importance analysis to decide which variables are most relevant.\n",
    "\n",
    "- **Combine Variables:** Sometimes, we can combine highly correlated variables into a single composite variable, reducing the multicollinearity issue.\n",
    "\n",
    "- **Regularization Techniques:** **Ridge regression** and **Lasso regression** are regression techniques that can help mitigate multicollinearity. They add penalty terms to the regression objective function, which shrinks the regression coefficients and reduces multicollinearity.\n",
    "\n",
    "- **Collect More Data:** If feasible, collecting more data can help reduce the impact of multicollinearity.\n",
    "\n",
    "- **Principal Component Analysis (PCA):** PCA can be used to transform correlated variables into uncorrelated principal components, which can be included in the model instead of the original variables.\n",
    "\n",
    "- **Partial Least Squares (PLS) Regression:** PLS regression is designed to handle multicollinearity and can be used when addressing this issue is a priority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c0133",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6c071",
   "metadata": {},
   "source": [
    "**Polynomial Regression** is a type of regression analysis that models the relationship between the dependent variable and one or more independent variables as an nth-degree polynomial. In contrast to simple and multiple linear regression, where the relationship is modeled as a straight line, polynomial regression allows for more flexible and nonlinear relationships to be captured. Here's an overview of polynomial regression and how it differs from linear regression:\n",
    "\n",
    "##### Polynomial Regression Model:\n",
    "\n",
    "In polynomial regression, the model equation is expressed as follows:\n",
    "\n",
    "$$ Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^2 + \\beta_{3}X^3 + ... + + \\beta_{n}X^n + \\epsilon$$\n",
    "\n",
    "##### Differences from Linear Regression:\n",
    "\n",
    "1. **Model Equation:** The linear or multiple linear regression models a straight line throught the datapoints where the Polynomial regression draws a model equation of the degree in order of n.\n",
    "\n",
    "2. **Model Flexibility:** Linear regression is simple and has the advantage of ease of interpretation but may not fit data with nonlinear relationships. Polynomial regression is more flexible and can fit data with curvilinear patterns, but it may lead to overfitting if the degree of the polynomial is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf899e",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741db83",
   "metadata": {},
   "source": [
    "##### Advantages of Polynomial Regression Compared to Linear Regression:\n",
    "\n",
    "1. **Modeling Nonlinear Relationships:** Polynomial regression is capable of capturing nonlinear relationships between the independent and dependent variables. Linear regression, on the other hand, assumes a linear relationship, which can be a limitation when dealing with data that has a nonlinear pattern.\n",
    "\n",
    "2. **Increased Flexibility:** Polynomial regression provides a higher degree of flexibility in modeling. It can fit a wider range of data patterns, including curves, bends, and more complex shapes.\n",
    "\n",
    "3. **Improved Fit to Data:** When the relationship between variables is curvilinear or exhibits complex patterns, polynomial regression can provide a better fit to the data compared to linear regression, which may not adequately capture these patterns.\n",
    "\n",
    "\n",
    "##### Disadvantages of Polynomial Regression:\n",
    "\n",
    "1. **Overfitting:** Polynomial regression can be susceptible to overfitting, especially when a high-degree polynomial is used. Overfit models may perform well on the training data but generalize poorly to new data.\n",
    "\n",
    "2. **Increased Complexity:** As the degree of the polynomial increases, the model becomes more complex and difficult to interpret. High-degree polynomials can lead to models that are challenging to understand and explain.\n",
    "\n",
    "3. **Limited Extrapolation:** Polynomial regression is primarily suited for interpolating within the range of the observed data. Extrapolating beyond the range of the data can lead to unreliable predictions.\n",
    "\n",
    "4. **Increased Variability:** High-degree polynomials can produce models with high variance, leading to instability in parameter estimates and prediction intervals.\n",
    "\n",
    "##### Situation in Which to Prefer Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a useful tool when dealing with specific situations or data patterns, including:\n",
    "\n",
    "1. Nonlinear Relationships: When we have strong domain knowledge or reason to believe that the relationship between variables is nonlinear, polynomial regression can be a good choice. It allows us to model and capture these nonlinear relationships effectively.\n",
    "\n",
    "2. Data with Curves or Bends: In cases where the data exhibits curves, bends, or inflection points, polynomial regression can provide a more accurate fit compared to linear regression.\n",
    "\n",
    "3. Exploratory Data Analysis: Polynomial regression can be a valuable tool for exploratory data analysis when we want to examine the data for possible nonlinear patterns. It can help us identify the functional form of the relationship.\n",
    "\n",
    "4. Interpolation within the Data Range: Polynomial regression is well-suited for interpolation within the observed data range, making it useful for modeling data that does not extend beyond the observed range.\n",
    "\n",
    "5. When using polynomial regression, it's important to carefully select the degree of the polynomial to avoid overfitting and to balance model complexity with accuracy. Cross-validation techniques can help assess the performance of different polynomial degrees and choose the most appropriate one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
