{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6885b4fe",
   "metadata": {},
   "source": [
    "## Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695a1a2",
   "metadata": {},
   "source": [
    "Bayes' theorem is the foundation of the Naive Bayes classifier, a popular machine learning algorithm used for classification tasks. The theorem itself is a fundamental concept in probability theory, named after the Reverend Thomas Bayes. It calculates the probability of a hypothesis given some evidence.\n",
    "\n",
    "In the context of the Naive Bayes classifier, Bayes' theorem is used to predict the probability that a given data point belongs to a particular class based on the observed features. The \"naive\" part of Naive Bayes comes from the assumption of independence among features, meaning that each feature makes an independent and equal contribution to the probability of a data point belonging to a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1b267",
   "metadata": {},
   "source": [
    "## Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857ba24a",
   "metadata": {},
   "source": [
    "Here's the formula for of Bayes' theorem:\n",
    "\n",
    "$$ P(C_{k}|X) = \\frac{P(C_{k}) P(X|C_{k})}{P(X)} $$\n",
    "\n",
    "where,\n",
    "\n",
    "- $P(C_{k}|X)$ is the probability of class $C_{k}$ given the event X has already occured\n",
    "- $P(C_{k}$ is the probability of the class $C_{k}$\n",
    "- $P(X)$ is the probability of the event $X$\n",
    "- $P(X|C_{k})$ is the probability of the event $X$ occuring given that the class is $C_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c305e9",
   "metadata": {},
   "source": [
    "## Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ff9a9",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in various fields and applications where **uncertainty** and **probability** play a significant role. Some common practical applications include:\n",
    "\n",
    "1. **Medical Diagnosis:** Bayes' theorem is used in medical diagnosis to calculate the probability of a patient having a particular disease given their symptoms and test results. It helps doctors make informed decisions about patient care and treatment.\n",
    "\n",
    "2. **Spam Filtering:** In email filtering, Bayes' theorem is used in spam detection algorithms to classify emails as spam or non-spam based on the occurrence of certain words or phrases. This approach, often called **\"Bayesian spam filtering\"**, is effective in filtering out unwanted emails.\n",
    "\n",
    "3. **Document Classification:** Bayes' theorem is utilized in text classification tasks, such as categorizing documents into different topics or genres. It helps determine the likelihood of a document belonging to a particular category based on its content.\n",
    "\n",
    "4. **Stock Market Prediction:** Bayes' theorem can be applied in financial modeling and stock market prediction to estimate the probability of certain market events occurring given historical data and market indicators.\n",
    "\n",
    "5. **Fault Diagnosis in Engineering:** In engineering, Bayes' theorem is used for fault diagnosis in systems like manufacturing plants, automotive systems, and aircraft. It helps in identifying the root cause of failures or malfunctions based on observed symptoms or sensor data.\n",
    "\n",
    "6. **Information Retrieval:** Bayes' theorem is applied in search engines and information retrieval systems to rank search results based on relevance to a user's query. It helps calculate the probability of a document being relevant to the query.\n",
    "\n",
    "7. **Machine Learning:** Bayes' theorem serves as the foundation for various machine learning algorithms, including Naive Bayes classifiers, Bayesian networks, and Bayesian optimization. These techniques are used for tasks such as classification, regression, clustering, and parameter optimization.\n",
    "\n",
    "In practice, Bayes' theorem enables decision-making under uncertainty by incorporating prior knowledge and updating beliefs based on new evidence. Its versatility and applicability across diverse domains make it a powerful tool in probabilistic reasoning and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b93f0",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a2ea2",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability, as it provides a way to calculate conditional probabilities using prior probabilities and likelihoods.\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. Mathematically, it's expressed as $P(Aâˆ£B)$, which reads as \"the probability of event A occuring given event B has already occured.\"\n",
    "\n",
    "Bayes' theorem is a formula that relates conditional probability to its reverse counterpart. It states:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A) P(B|A)}{P(B)} $$\n",
    "\n",
    "In essence, Bayes' theorem allows us to update our belief about the probability of an event A occurring based on new evidence provided by the occurrence of event B. It's a fundamental tool in probabilistic reasoning and is widely used in various fields, including statistics, machine learning, and decision theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761f654",
   "metadata": {},
   "source": [
    "## Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10ff71",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier depends on the nature of the data and the assumptions weeee are willing to make about the underlying distribution of the features. Here are the common types of Naive Bayes classifiers and considerations for choosing among them:\n",
    "\n",
    "1. **Gaussian Naive Bayes:** This classifier assumes that the features follow a Gaussian (normal) distribution. It's suitable for **continuous numerical features** where the distribution of each class is assumed to be Gaussian.\n",
    "    - **Use Cases:** When dealing with continuous features that are approximately normally distributed. For example, it can be used in tasks like predicting housing prices based on features like size, number of bedrooms, etc.\n",
    "\n",
    "2. **Multinomial Naive Bayes:** This classifier is appropriate when the **features represent counts or frequencies of events**, typically in text classification tasks where the features are word counts or TF-IDF (Term Frequency-Inverse Document Frequency) values.\n",
    "    - **Use Cases:** Text classification, spam detection, sentiment analysis, and other tasks where the features are counts or frequencies of events.\n",
    "    \n",
    "3. **Bernoulli Naive Bayes:** Similar to Multinomial Naive Bayes, this classifier is suitable for **binary feature vectors** (i.e., presence or absence of a feature), often used in text classification tasks where the features represent whether a word occurs in a document or not.\n",
    "    - **Use Cases:** Binary text classification tasks, such as spam detection or sentiment analysis where the features represent the presence or absence of words in a document.\n",
    "    \n",
    "4. **Complement Naive Bayes:** This variant of Naive Bayes is designed **to address class imbalances by adjusting the probability calculation for each class**. It works well when dealing with imbalanced datasets.\n",
    "    - **Use Cases:** Text classification tasks with imbalanced class distributions, where some classes have significantly more samples than others.\n",
    "    \n",
    "When choosing the type of Naive Bayes classifier, we need to consider the following factors:\n",
    "\n",
    "- **Nature of Data:** Determine whether the features are continuous, binary, or represent counts/frequencies. Choose the classifier that best matches the nature of the data.\n",
    "- **Assumptions:** Be aware of the assumptions made by each type of Naive Bayes classifier and assess whether they hold true for the dataset. For example, Gaussian Naive Bayes assumes Gaussian distribution of features.\n",
    "- **Performance:** Experiment with different types of Naive Bayes classifiers and evaluate their performance using metrics such as accuracy, precision, recall, and F1-score on a validation set.\n",
    "- **Computational Efficiency:** Consider the computational complexity of each classifier, especially for large datasets. Some variants may be more computationally efficient than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd826d27",
   "metadata": {},
   "source": [
    "## Q6. Assignment:\n",
    "##### You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "\n",
    "|Class|X1=1|X1=2|X1=3|X2=1|X2=2|X2=3|X2=4|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|A|3|3|4|4|3|3|3|\n",
    "|B|2|2|1|2|2|2|3|\n",
    " \n",
    "##### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07fe77",
   "metadata": {},
   "source": [
    "To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probabilities for each class, given these feature values. We can do this using Bayes' theorem:\n",
    "\n",
    "$P(A|X1=3,X2=4) = \\frac{P(X1=3,X2=4|A) * P(A)} {P(X1=3,X2=4)}$\n",
    "\n",
    "$P(B|X1=3,X2=4) = \\frac{P(X1=3,X2=4|B) * P(B)} {P(X1=3,X2=4)}$\n",
    "\n",
    "Since the prior probabilities for A and B are assumed to be equal, we can simplify this to:\n",
    "\n",
    "$P(A|X1=3,X2=4) = \\frac{P(X1=3,X2=4|A)} {P(X1=3,X2=4)}$\n",
    "\n",
    "$P(B|X1=3,X2=4) = \\frac{P(X1=3,X2=4|B)} {P(X1=3,X2=4)}$\n",
    "\n",
    "To calculate the probabilities, we need to use the Naive Bayes assumption that the features are conditionally independent, given the class. This allows us to factorize the joint probability distribution as follows:\n",
    "\n",
    "$P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A)$\n",
    "\n",
    "$P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B)$\n",
    "\n",
    "We can estimate these probabilities from the frequency table provided:\n",
    "\n",
    "$P(X1=3|A) = \\frac{4}{10}$\n",
    "\n",
    "$P(X1=3|B) = \\frac{1}{7}$\n",
    "\n",
    "$P(X2=4|A) = \\frac{3}{10}$\n",
    "\n",
    "$P(X2=4|B) = \\frac{1}{7}$\n",
    "\n",
    "To calculate the denominator, we need to use the law of total probability:\n",
    "\n",
    "$P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)$\n",
    "\n",
    "We can estimate these probabilities from the frequency table provided:\n",
    "\n",
    "$P(X1=3,X2=4|A) = P(X1=3|A) * P(X2=4|A) = (4/10) * (3/10) = 12/100$\n",
    "\n",
    "$P(X1=3,X2=4|B) = P(X1=3|B) * P(X2=4|B) = (1/7) * (1/7) = 1/49$\n",
    "\n",
    "$P(A) = P(B) = 0.5$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$P(X1=3,X2=4) = (12/100) * 0.5 + (1/49) * 0.5 = 0.124$\n",
    "\n",
    "Now we can plug these values into the formula for the posterior probabilities:\n",
    "$P(A|X1=3,X2=4) = (4/10) * (3/10) / 0.124 = 0.967$\n",
    "\n",
    "$P(B|X1=3,X2=4) = (1/7) * (1/7) / 0.124 = 0.033$\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance with features X1=3 and X2=4 belongs to class A, since it has a much higher posterior probability than class B."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
