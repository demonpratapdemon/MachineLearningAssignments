{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d21a54",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd47eaa",
   "metadata": {},
   "source": [
    "**Lasso Regression**, or **L1 regularization**, is a linear regression technique that introduces a penalty term based on the absolute values of the coefficients. This penalty encourages sparsity in the model, leading some coefficients to be exactly zero. The key characteristics and differences of Lasso Regression compared to other regression techniques include:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "    - Lasso Regression introduces a regularization term proportional to the sum of the absolute values of the coefficients ($|\\theta|$). This term is added to the ordinary least squares (OLS) objective function.\n",
    "    \n",
    "2. **Sparsity:**\n",
    "    - One distinctive feature of Lasso Regression is its ability to induce sparsity in the model. As the regularization parameter ($\\lambda$) increases, some coefficients are driven to exactly zero, effectively performing feature selection.\n",
    "    \n",
    "3. **Feature Selection:**\n",
    "    - Lasso Regression is often used for feature selection because it tends to eliminate less important variables by setting their coefficients to zero. This is particularly valuable in high-dimensional datasets with many potentially irrelevant features.\n",
    "    \n",
    "    \n",
    "##### Differences with Ridge Regression\n",
    "\n",
    "|Points|Lasso Regression|Ridge Regression|\n",
    "|---|---|---|\n",
    "|**Type**|This is also called L1 regression where it adds a linear penalty to the OLS cost function.|This is also called L2 regression where it adds a polynomial penalty to the OLS cost function.|\n",
    "|**Formula**|$$ J(\\theta) = MSE + \\alpha \\sum_{i=1}^{n} |\\theta_{i}| $$|$$ J(\\theta) = MSE + \\alpha \\sum_{i=1}^{n} \\theta_{i}^2 $$|\n",
    "|**Sparsity**|It introduces sparsity into the coefficients.|This doesn't introduce spartsity so much but leads towards zero.|\n",
    "|**Feature Selection**|Since it leads the coefficients towards 0 and introduces sparsity, it effectively leads toward feature selection.|This doesn't leads the coefficients towards absolute zero but can be used for feature selection if we filter the coefficients of the features based on a threshold.|\n",
    "|**Multicollinearity**|This doesn't effectively handle multicollinearity.|This can effectively handle multicollinearity.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f7295",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b3196",
   "metadata": {},
   "source": [
    "The primary advantage of using Lasso Regression for feature selection is its ability to perform both feature selection and regularization, which helps prevent overfitting in predictive models.\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) works by adding a penalty term (L1 regularization) to the linear regression equation. This penalty encourages the model to minimize the coefficients of less important features by shrinking them toward zero, effectively performing feature selection by eliminating those features entirely.\n",
    "\n",
    "This feature selection property is particularly valuable in scenarios where there are a large number of features, as Lasso Regression can automatically identify and remove irrelevant or redundant features, simplifying the model and potentially improving its predictive performance. It essentially helps in creating a more parsimonious or sparse model by selecting only the most relevant features, leading to better interpretability and generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b0c01",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed5be2",
   "metadata": {},
   "source": [
    "Here is how the coefficients of a Lasso Regression model are interpreted:\n",
    "\n",
    "1. **Non-Zero Coefficients:** The non-zero coefficients directly indicate the importance of the corresponding features. A non-zero coefficient suggests that the feature is influential in predicting the target variable. The magnitude of the coefficient reflects the strength of that influence: larger coefficients indicate a stronger impact on the predictions.\n",
    "\n",
    "2. **Zero Coefficients:** A coefficient that's reduced to zero by the Lasso penalty indicates that the corresponding feature has been excluded from the model. Essentially, the Lasso has performed feature selection by setting these coefficients to zero, implying that these features are considered less important for predicting the target variable.\n",
    "\n",
    "3. **Magnitude Comparison:** Comparing the magnitudes of non-zero coefficients can provide insights into which features have a more significant impact on the model's predictions. Larger coefficients usually imply a more substantial influence on the target variable, while smaller coefficients suggest a weaker impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593cbf8",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557af95",
   "metadata": {},
   "source": [
    "In Lasso Regression, the primary tuning parameter that you can adjust is the regularization strength or the hyperparameter, often denoted as \"alpha\" ($\\alpha$). This hyperparameter controls the degree of regularization applied to the model.\n",
    "\n",
    "1. **Alpha Parameter ($\\alpha$):**\n",
    "\n",
    "    - High $\\alpha$: When $\\alpha$ is set to a high value, the Lasso penalty becomes more pronounced. This results in stronger regularization, causing more coefficients to be pushed towards zero. As a result, it increases the level of feature selection and model simplification. However, excessively high $\\alpha$ values may lead to underfitting, as important features may also be eliminated.\n",
    "\n",
    "    - Low $\\alpha$: Lower $\\alpha$ values reduce the strength of the L1 regularization, allowing more coefficients to remain non-zero. This results in a less sparse model with more features included. Lower $\\alpha$ values can lead to overfitting if there are too many features or if the features are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04688dfd",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1368d8d",
   "metadata": {},
   "source": [
    "Lasso Regression, by itself, is inherently a linear regression technique, as it fits a linear model with L1 regularization to prevent overfitting. It's primarily designed for problems where the relationship between the features and the target variable is linear. However, there are ways to extend Lasso Regression to address non-linear regression problems.\n",
    "\n",
    "Here are some approaches to adapt Lasso Regression for non-linear problems:\n",
    "\n",
    "1. **Feature Engineering:** One way to use Lasso Regression for non-linear problems is through feature engineering. We can create new features that capture non-linear relationships by transforming the existing features. For instance, we can add squared or cubed terms, take logarithms, or use other mathematical functions to represent non-linear relationships in the data. Once these non-linear transformations are included as features, Lasso Regression can be applied to the expanded feature space.\n",
    "\n",
    "2. **Polynomial Regression:** By including polynomial features (e.g., x², x³) in the model, Lasso Regression can capture non-linear relationships. Transforming features into higher-degree polynomials can help model complex, non-linear patterns. Lasso can then select the most relevant polynomial features while shrinking less important ones towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1105954",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2697e203",
   "metadata": {},
   "source": [
    "    \n",
    "##### Differences between Lasso and Ridge Regression are as follows:\n",
    "\n",
    "|Points|Lasso Regression|Ridge Regression|\n",
    "|---|---|---|\n",
    "|**Type**|This is also called L1 regression where it adds a linear penalty to the OLS cost function.|This is also called L2 regression where it adds a polynomial penalty to the OLS cost function.|\n",
    "|**Formula**|$$ J(\\theta) = MSE + \\alpha \\sum_{i=1}^{n} |\\theta_{i}| $$|$$ J(\\theta) = MSE + \\alpha \\sum_{i=1}^{n} \\theta_{i}^2 $$|\n",
    "|**Sparsity**|It introduces sparsity into the coefficients.|This doesn't introduce spartsity so much but leads towards zero.|\n",
    "|**Feature Selection**|Since it leads the coefficients towards 0 and introduces sparsity, it effectively leads toward feature selection.|This doesn't leads the coefficients towards absolute zero but can be used for feature selection if we filter the coefficients of the features based on a threshold.|\n",
    "|**Multicollinearity**|This doesn't effectively handle multicollinearity.|This can effectively handle multicollinearity.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9f64d",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f599f",
   "metadata": {},
   "source": [
    "Lasso Regression has a built-in feature selection mechanism that helps in handling multicollinearity to some extent, but it doesn't explicitly address multicollinearity issues as its primary purpose is feature selection through regularization. However, its inherent property of reducing coefficients or setting some to zero indirectly addresses multicollinearity by effectively choosing one feature over another when they are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaff73",
   "metadata": {},
   "source": [
    "##  Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a7a51",
   "metadata": {},
   "source": [
    "There are multiple ways to find the optimal value of the the regularization paramter $\\lambda$ in Lasso Regressio. Some of the ways are discussed below:\n",
    "\n",
    "1. **Cross-Validation:** Utilize k-fold cross-validation techniques to assess model performance across different values of lambda. This involves dividing the dataset into k subsets, training the model on k-1 subsets, and validating it on the remaining subset. This process is repeated k times, each time with a different subset held out for validation. The lambda value that results in the best average performance across these iterations is chosen.\n",
    "\n",
    "2. **Grid Search:** Implement a grid search where a predefined range of lambda values is tested exhaustively. This method involves training the model with different lambda values and evaluating each model's performance. The lambda value that yields the best performance is selected.\n",
    "\n",
    "3. **Randomized Search:** Similar to grid search, but instead of testing all possible values, a randomized search tests a random subset of possible values within a defined range. This can be more efficient, especially when dealing with a large range of potential lambda values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
